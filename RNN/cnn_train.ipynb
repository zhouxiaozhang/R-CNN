{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import logging\n",
    "import pickle\n",
    "import CNN\n",
    "import math\n",
    "from CNN import malware_RNN\n",
    "from config import get_config\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.contrib import learn\n",
    "from sklearn.metrics import roc_auc_score \n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Data parameters\n",
    "tf.flags.DEFINE_string(\"data_path\", \"/home/zx/gram3_gain1000/data_process_data_original_value_train\",\"data_path\")\n",
    "tf.flags.DEFINE_string(\"save_path\" , \"/home/zx/cuckoo_1000/session_save2/\",\"Model output directory.\")\n",
    "tf.flags.DEFINE_string(\"board_path\", \"/home/zx/cuckoo_1000/tensor_board2/\",\"Tensor board output directory.\")\n",
    "tf.flags.DEFINE_string(\"log_path\", \"/home/zx/cuckoo_1000/log.log2\", \"log output path.\")\n",
    "#Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\",True,\"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\",False,\"Log placement of ops on devices\")\n",
    "#train Parameters\n",
    "tf.flags.DEFINE_string(\"evaluate_every\",2,\"Evaluate model on dev set after this many epochs\")\n",
    "\n",
    "FLAGS =tf.flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#log\n",
    "LOG = None\n",
    "def init_logger():\n",
    "    global LOG\n",
    "\n",
    "    LOG = logging.getLogger('seq')\n",
    "    LOG.setLevel(logging.DEBUG)\n",
    "\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "    file_handler = logging.FileHandler(filename=FLAGS.log_path, encoding=\"utf-8\")\n",
    "    file_handler.setLevel(logging.DEBUG)\n",
    "    file_handler.setFormatter(formatter)\n",
    "    LOG.addHandler(file_handler)\n",
    "\n",
    "    stream_handler = logging.StreamHandler(sys.stdout)\n",
    "    stream_handler.setLevel(logging.INFO)\n",
    "    stream_handler.setFormatter(formatter)\n",
    "    LOG.addHandler(stream_handler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data(file_name):\n",
    "    with open(file_name, \"rb\") as f:\n",
    "        raw_x, raw_y = pickle.load(f)\n",
    "        l = len(raw_x)\n",
    "        train_data = []\n",
    "        train_lable=[]\n",
    "        valid_data = []\n",
    "        valid_lable=[]\n",
    "        pos_1 = int(l * 0.9)\n",
    "        train_data = raw_x[:pos_1]\n",
    "        train_lable=raw_y[:pos_1]\n",
    "        valid_data = raw_x[pos_1: ]\n",
    "        valid_lable = raw_y[pos_1: ]\n",
    "        return  train_data,valid_data,train_lable,valid_lable\n",
    "    \n",
    "def batch_iter(x_data,y_data, batch_size, num_epochs, shuffle=True):\n",
    "    x_data=np.array(x_data)\n",
    "    y_data=np.array(y_data)\n",
    "    data_size =len(x_data)\n",
    "    num_batches_per_epoch = int((data_size-1) / batch_size)+1\n",
    "    #for epoch in range(num_epochs):\n",
    "    if shuffle:\n",
    "        shuffle_indices=np.random.permutation(data_size)\n",
    "        shuffled_data_x=x_data[shuffle_indices]\n",
    "        shuffled_data_y=y_data[shuffle_indices]\n",
    "                \n",
    "    else:\n",
    "        shuffled_data_x=x_data\n",
    "        shuffled_data_y=y_data\n",
    "    for batch_num in range(num_batches_per_epoch):\n",
    "        start_index=batch_num*batch_size\n",
    "        end_index=min((batch_num+1)*batch_size,data_size)\n",
    "        yield shuffled_data_x[start_index:end_index],shuffled_data_y[start_index:end_index]\n",
    "        \n",
    "def real_len(batches):\n",
    "    real=[]\n",
    "    for batch in batches:\n",
    "        if min(batch)>=1:\n",
    "            result=1000\n",
    "        else:\n",
    "            result=np.argmin(batch)\n",
    "        real.append(result) \n",
    "    return real\n",
    "                    \n",
    "def run_epoch(\n",
    "        session,\n",
    "        x_batch,\n",
    "        y_batch,\n",
    "        dropout,\n",
    "        model,\n",
    "        global_step,\n",
    "        summary_op,\n",
    "        eval_op=None,\n",
    "        verbose=False\n",
    "       \n",
    "):\n",
    "    #feed,give\n",
    "    feed_dict={\n",
    "        model.input_x:x_batch,\n",
    "        model.input_y:y_batch,\n",
    "        model.dropout_keep_prob:dropout,\n",
    "        model.batch_size:len(x_batch),\n",
    "        model.real_len:real_len(x_batch)\n",
    "              }\n",
    "    #out\n",
    "    fetches = {\n",
    "        \"loss\": model.loss,\n",
    "        \"global_step\":global_step,\n",
    "        \"summary_op\" :summary_op,\n",
    "        \"y_pre\"      :model.y_pre\n",
    "    }\n",
    "    if eval_op is not None:\n",
    "        fetches[\"eval_op\"] = eval_op\n",
    "    \n",
    "    fetches_ret = session.run(fetches, feed_dict)\n",
    "    loss = fetches_ret[\"loss\"]\n",
    "    y_p=fetches_ret[\"y_pre\"]\n",
    "    auc=roc_auc_score(y_batch, y_p)\n",
    "    for i,y_acc in enumerate(y_p):\n",
    "            if y_acc>0.5:\n",
    "                y_p[i]=1\n",
    "            else:\n",
    "                y_p[i]=0\n",
    "    acc=accuracy_score(y_batch, y_p)\n",
    "    if verbose:\n",
    "        LOG.info( \"step: %d,loss: %.3f auc: %.3f acc:%.3f\" \n",
    "                  % ( fetches_ret[\"global_step\"], fetches_ret[\"loss\"],auc,acc)\n",
    "                 )\n",
    "    return auc, fetches_ret[\"loss\"],fetches_ret[\"summary_op\"]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    \n",
    "    #training \n",
    "    x_train,x_dev,y_train,y_dev=load_data(FLAGS.data_path)\n",
    "    init_logger()\n",
    "    train_config=get_config(\"train\")\n",
    "    valid_config=get_config(\"valid\")\n",
    "    with tf.Graph().as_default():\n",
    "        session_conf=tf.ConfigProto(allow_soft_placement=FLAGS.allow_soft_placement,log_device_placement=FLAGS.log_device_placement)\n",
    "        sess=tf.Session(config=session_conf)\n",
    "        with sess.as_default():\n",
    "            cnn=malware_RNN(config=train_config)\n",
    "            global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "            trainable_vars = tf.trainable_variables()\n",
    "            grads, _ = tf.clip_by_global_norm(tf.gradients( cnn.loss, trainable_vars),train_config.MAX_GRAD)\n",
    "            optimizer = tf.train.AdamOptimizer(train_config.learning_rate)\n",
    "            train_op = optimizer.apply_gradients(zip(grads, trainable_vars),global_step=tf.contrib.framework.get_or_create_global_step())\n",
    "            #optimizer=tf.train.RMSPropOptimizer(train_config.learning_rate,decay=0.9)\n",
    "            #grads_and_vars=optimizer.compute_gradients(cnn.loss)\n",
    "            #train_op=optimizer.apply_gradients(grads_and_vars,global_step)\n",
    "            #keep track of gradient values and sparsity\n",
    "            grad_summaries=[]\n",
    "            for g,v in zip(grads, trainable_vars):\n",
    "                if g is not None:\n",
    "                    grad_hist_summary=tf.summary.histogram(\"{}/grad/hist\".format(v.name),g)\n",
    "                    sparsity_summary=tf.summary.scalar(\"{}/grad/sparsity\".format(v.name),tf.nn.zero_fraction(g))\n",
    "                    grad_summaries.append(grad_hist_summary)\n",
    "                    grad_summaries.append(sparsity_summary)\n",
    "            grad_summaries_merged=tf.summary.merge(grad_summaries)\n",
    "            #Summaries for loss and acc\n",
    "            loss_summary=tf.summary.scalar(\"loss\",cnn.loss)\n",
    "            \n",
    "            #Train summaries\n",
    "            train_summary_op=tf.summary.merge([grad_summaries_merged,loss_summary])\n",
    "            train_summary_dir=os.path.join(FLAGS.board_path,\"summaries\",\"train\")\n",
    "            train_summary_write=tf.summary.FileWriter(train_summary_dir,sess.graph)\n",
    "            #Dev summaries\n",
    "            dev_summary_op=loss_summary\n",
    "            dev_summary_dir=os.path.join(FLAGS.board_path,\"summaries\",\"dev\")\n",
    "            dev_summary_write=tf.summary.FileWriter(dev_summary_dir,sess.graph)\n",
    "            \n",
    "           \n",
    "            #init\n",
    "            checkpoint_dir=os.path.join(FLAGS.save_path,\"checkpoints\")\n",
    "            if not os.path.exists(checkpoint_dir):\n",
    "                os.makedirs(checkpoint_dir)\n",
    "            checkpoint_prefix=os.path.join(checkpoint_dir,\"model\")\n",
    "            \n",
    "            saver=tf.train.Saver(tf.all_variables())\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            #generate batches\n",
    "            \n",
    "            best_acc,best_at_step=0,0\n",
    "            #training loop\n",
    "            accuracy_total=0.0\n",
    "            loss_total=0.0\n",
    "            for epoch_id in range(0, train_config.num_epochs):\n",
    "                #accuracy_total=accuracy_total/ (int((len(x_train)-1) / train_config.batch_size)+1)\n",
    "                #loss_total=loss_total/(int((len(x_train)-1) / train_config.batch_size)+1)\n",
    "                #LOG.info(\"\\ntrain_epoch:\")\n",
    "                #LOG.info( \"loss_total: %.3f accuracy_total: %.3f\" \n",
    "                  #% ( loss_total,accuracy_total)\n",
    "                #)\n",
    "                accuracy_total=0.0\n",
    "                loss_total=0.0\n",
    "                batches=batch_iter(x_train,y_train,train_config.batch_size,train_config.num_epochs)\n",
    "                for batch_x,batch_y in batches:\n",
    "                    acc_train,loss_train,summaries=run_epoch( sess,batch_x,batch_y,dropout=train_config.dropout_keep_prob,model=cnn, eval_op=train_op,verbose=True, global_step= global_step,summary_op=train_summary_op)\n",
    "                    accuracy_total+=acc_train\n",
    "                    loss_total+=loss_train\n",
    "                    current_step=tf.train.global_step(sess,global_step)\n",
    "                    train_summary_write.add_summary(summaries,epoch_id)\n",
    "                accuracy_total=accuracy_total/ (int((len(x_train)-1) / train_config.batch_size)+1)\n",
    "                loss_total=loss_total/(int((len(x_train)-1) / train_config.batch_size)+1)\n",
    "                LOG.info(\"\\ntrain_epoch: %.3f\"%(epoch_id))\n",
    "                LOG.info( \"loss_total: %.3f accuracy_total: %.3f\" % ( loss_total,accuracy_total))\n",
    "                if epoch_id%FLAGS.evaluate_every==0:\n",
    "                    LOG.info(\"\\nEvaluation: %.3f\"%(epoch_id))\n",
    "                    acc,loss,summaries=run_epoch( sess,x_dev,y_dev,dropout=valid_config.dropout_keep_prob,model=cnn,verbose=True, global_step= global_step,summary_op=dev_summary_op) \n",
    "                    dev_summary_write.add_summary(summaries,epoch_id)\n",
    "                    if acc>=best_acc:\n",
    "                        best_acc,best_at_step=acc,epoch_id\n",
    "                        path=saver.save(sess,checkpoint_prefix,global_step=epoch_id)\n",
    "                        LOG.info(\"Saving model to %s at epoch %d.\" % (path,epoch_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(_):\n",
    "    if not FLAGS.data_path:\n",
    "        raise ValueError(\"Must set --data_path to data file\")\n",
    "    train()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zx/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name embedding_1:0/grad/hist is illegal; using embedding_1_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding_1:0/grad/sparsity is illegal; using embedding_1_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name LSTM/RNN/MultiRNNCell/Cell0/LSTMCell/W_0:0/grad/hist is illegal; using LSTM/RNN/MultiRNNCell/Cell0/LSTMCell/W_0_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name LSTM/RNN/MultiRNNCell/Cell0/LSTMCell/W_0:0/grad/sparsity is illegal; using LSTM/RNN/MultiRNNCell/Cell0/LSTMCell/W_0_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name LSTM/RNN/MultiRNNCell/Cell0/LSTMCell/B:0/grad/hist is illegal; using LSTM/RNN/MultiRNNCell/Cell0/LSTMCell/B_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name LSTM/RNN/MultiRNNCell/Cell0/LSTMCell/B:0/grad/sparsity is illegal; using LSTM/RNN/MultiRNNCell/Cell0/LSTMCell/B_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name LSTM/RNN/MultiRNNCell/Cell1/LSTMCell/W_0:0/grad/hist is illegal; using LSTM/RNN/MultiRNNCell/Cell1/LSTMCell/W_0_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name LSTM/RNN/MultiRNNCell/Cell1/LSTMCell/W_0:0/grad/sparsity is illegal; using LSTM/RNN/MultiRNNCell/Cell1/LSTMCell/W_0_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name LSTM/RNN/MultiRNNCell/Cell1/LSTMCell/B:0/grad/hist is illegal; using LSTM/RNN/MultiRNNCell/Cell1/LSTMCell/B_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name LSTM/RNN/MultiRNNCell/Cell1/LSTMCell/B:0/grad/sparsity is illegal; using LSTM/RNN/MultiRNNCell/Cell1/LSTMCell/B_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name classifier/softmax_w:0/grad/hist is illegal; using classifier/softmax_w_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name classifier/softmax_w:0/grad/sparsity is illegal; using classifier/softmax_w_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name classifier/softmax_b:0/grad/hist is illegal; using classifier/softmax_b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name classifier/softmax_b:0/grad/sparsity is illegal; using classifier/softmax_b_0/grad/sparsity instead.\n",
      "WARNING:tensorflow:From <ipython-input-5-a5fe4d10079d>:49 in train.: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n",
      "2017-10-23 21:55:31,540 - INFO - step: 1,loss: 98.166 auc: 0.542 acc:0.430\n",
      "2017-10-23 21:56:32,781 - INFO - step: 2,loss: 172.385 auc: 0.747 acc:0.547\n",
      "2017-10-23 21:57:32,551 - INFO - step: 3,loss: 77.425 auc: 0.731 acc:0.641\n",
      "2017-10-23 21:58:31,283 - INFO - step: 4,loss: 65.438 auc: 0.822 acc:0.719\n",
      "2017-10-23 21:59:29,639 - INFO - step: 5,loss: 75.773 auc: 0.842 acc:0.719\n",
      "2017-10-23 22:00:26,581 - INFO - step: 6,loss: 45.889 auc: 0.931 acc:0.820\n",
      "2017-10-23 22:01:23,292 - INFO - step: 7,loss: 65.628 auc: 0.850 acc:0.758\n",
      "2017-10-23 22:02:19,305 - INFO - step: 8,loss: 48.237 auc: 0.912 acc:0.812\n",
      "2017-10-23 22:03:17,666 - INFO - step: 9,loss: 52.045 auc: 0.888 acc:0.797\n",
      "2017-10-23 22:04:13,478 - INFO - step: 10,loss: 44.437 auc: 0.942 acc:0.898\n",
      "2017-10-23 22:05:11,277 - INFO - step: 11,loss: 46.605 auc: 0.922 acc:0.789\n",
      "2017-10-23 22:06:08,686 - INFO - step: 12,loss: 55.222 auc: 0.904 acc:0.789\n",
      "2017-10-23 22:07:03,181 - INFO - step: 13,loss: 44.843 auc: 0.926 acc:0.852\n",
      "2017-10-23 22:07:58,675 - INFO - step: 14,loss: 47.886 auc: 0.904 acc:0.828\n",
      "2017-10-23 22:08:56,084 - INFO - step: 15,loss: 48.643 auc: 0.919 acc:0.844\n",
      "2017-10-23 22:09:54,325 - INFO - step: 16,loss: 52.498 auc: 0.954 acc:0.836\n",
      "2017-10-23 22:10:52,122 - INFO - step: 17,loss: 80.464 auc: 0.830 acc:0.703\n",
      "2017-10-23 22:11:48,781 - INFO - step: 18,loss: 40.359 auc: 0.943 acc:0.875\n",
      "2017-10-23 22:12:47,750 - INFO - step: 19,loss: 35.723 auc: 0.959 acc:0.891\n",
      "2017-10-23 22:13:25,168 - INFO - step: 20,loss: 32.071 auc: 0.882 acc:0.800\n",
      "2017-10-23 22:13:25,173 - INFO - \n",
      "train_epoch: 0.000\n",
      "2017-10-23 22:13:25,176 - INFO - loss_total: 61.487 accuracy_total: 0.868\n",
      "2017-10-23 22:13:25,178 - INFO - \n",
      "Evaluation: 0.000\n",
      "2017-10-23 22:14:04,730 - INFO - step: 20,loss: 94.017 auc: 0.950 acc:0.838\n",
      "2017-10-23 22:14:05,152 - INFO - Saving model to /home/zx/cuckoo_1000/session_save2/checkpoints/model-0 at epoch 0.\n",
      "2017-10-23 22:15:04,947 - INFO - step: 21,loss: 42.521 auc: 0.927 acc:0.867\n",
      "2017-10-23 22:16:04,159 - INFO - step: 22,loss: 35.102 auc: 0.954 acc:0.906\n",
      "2017-10-23 22:17:05,296 - INFO - step: 23,loss: 49.138 auc: 0.916 acc:0.859\n",
      "2017-10-23 22:18:06,157 - INFO - step: 24,loss: 37.846 auc: 0.955 acc:0.859\n",
      "2017-10-23 22:19:04,088 - INFO - step: 25,loss: 48.840 auc: 0.912 acc:0.852\n",
      "2017-10-23 22:20:02,766 - INFO - step: 26,loss: 47.854 auc: 0.918 acc:0.828\n",
      "2017-10-23 22:21:02,315 - INFO - step: 27,loss: 40.279 auc: 0.941 acc:0.859\n",
      "2017-10-23 22:22:01,356 - INFO - step: 28,loss: 52.947 auc: 0.905 acc:0.805\n",
      "2017-10-23 22:23:01,545 - INFO - step: 29,loss: 32.766 auc: 0.962 acc:0.906\n",
      "2017-10-23 22:23:59,496 - INFO - step: 30,loss: 36.147 auc: 0.951 acc:0.867\n",
      "2017-10-23 22:24:57,908 - INFO - step: 31,loss: 41.812 auc: 0.942 acc:0.867\n",
      "2017-10-23 22:25:56,659 - INFO - step: 32,loss: 36.921 auc: 0.950 acc:0.867\n",
      "2017-10-23 22:26:57,763 - INFO - step: 33,loss: 44.892 auc: 0.931 acc:0.836\n",
      "2017-10-23 22:27:59,916 - INFO - step: 34,loss: 38.634 auc: 0.942 acc:0.898\n",
      "2017-10-23 22:28:57,983 - INFO - step: 35,loss: 28.680 auc: 0.969 acc:0.883\n",
      "2017-10-23 22:29:57,641 - INFO - step: 36,loss: 45.573 auc: 0.945 acc:0.867\n",
      "2017-10-23 22:30:58,477 - INFO - step: 37,loss: 42.497 auc: 0.938 acc:0.875\n",
      "2017-10-23 22:37:37,534 - INFO - step: 44,loss: 43.184 auc: 0.929 acc:0.867\n",
      "2017-10-23 22:38:37,266 - INFO - step: 45,loss: 36.222 auc: 0.954 acc:0.883\n",
      "2017-10-23 22:39:36,602 - INFO - step: 46,loss: 34.402 auc: 0.960 acc:0.859\n",
      "2017-10-23 22:40:38,393 - INFO - step: 47,loss: 39.518 auc: 0.952 acc:0.852\n",
      "2017-10-23 22:41:45,871 - INFO - step: 48,loss: 40.402 auc: 0.965 acc:0.844\n",
      "2017-10-23 22:42:49,411 - INFO - step: 49,loss: 33.482 auc: 0.963 acc:0.891\n",
      "2017-10-23 22:43:49,841 - INFO - step: 50,loss: 34.883 auc: 0.965 acc:0.898\n",
      "2017-10-23 22:44:51,361 - INFO - step: 51,loss: 45.190 auc: 0.948 acc:0.836\n",
      "2017-10-23 22:45:48,844 - INFO - step: 52,loss: 55.368 auc: 0.919 acc:0.836\n",
      "2017-10-23 22:46:48,186 - INFO - step: 53,loss: 30.014 auc: 0.971 acc:0.891\n",
      "2017-10-23 22:47:46,132 - INFO - step: 54,loss: 34.001 auc: 0.962 acc:0.875\n",
      "2017-10-23 22:48:48,002 - INFO - step: 55,loss: 28.889 auc: 0.968 acc:0.938\n",
      "2017-10-23 22:49:51,822 - INFO - step: 56,loss: 30.956 auc: 0.971 acc:0.883\n",
      "2017-10-23 22:50:48,626 - INFO - step: 57,loss: 27.018 auc: 0.978 acc:0.898\n",
      "2017-10-23 22:51:49,965 - INFO - step: 58,loss: 32.375 auc: 0.955 acc:0.914\n",
      "2017-10-23 22:52:46,370 - INFO - step: 59,loss: 19.541 auc: 0.985 acc:0.945\n",
      "2017-10-23 22:53:21,086 - INFO - step: 60,loss: 18.639 auc: 0.951 acc:0.892\n",
      "2017-10-23 22:53:21,090 - INFO - \n",
      "train_epoch: 2.000\n",
      "2017-10-23 22:53:21,093 - INFO - loss_total: 36.160 accuracy_total: 0.955\n",
      "2017-10-23 22:53:21,095 - INFO - \n",
      "Evaluation: 2.000\n",
      "2017-10-23 22:54:00,666 - INFO - step: 60,loss: 81.686 auc: 0.960 acc:0.871\n",
      "2017-10-23 22:54:01,037 - INFO - Saving model to /home/zx/cuckoo_1000/session_save2/checkpoints/model-2 at epoch 2.\n",
      "2017-10-23 22:55:01,530 - INFO - step: 61,loss: 24.364 auc: 0.977 acc:0.898\n",
      "2017-10-23 22:56:01,553 - INFO - step: 62,loss: 39.472 auc: 0.941 acc:0.875\n",
      "2017-10-23 22:57:03,988 - INFO - step: 63,loss: 32.913 auc: 0.962 acc:0.891\n",
      "2017-10-23 22:58:12,968 - INFO - step: 64,loss: 26.622 auc: 0.981 acc:0.906\n",
      "2017-10-23 22:59:14,610 - INFO - step: 65,loss: 31.425 auc: 0.971 acc:0.883\n",
      "2017-10-23 23:00:17,207 - INFO - step: 66,loss: 31.078 auc: 0.970 acc:0.883\n",
      "2017-10-23 23:01:18,207 - INFO - step: 67,loss: 41.401 auc: 0.938 acc:0.867\n",
      "2017-10-23 23:02:19,540 - INFO - step: 68,loss: 26.094 auc: 0.976 acc:0.891\n",
      "2017-10-23 23:03:19,882 - INFO - step: 69,loss: 23.038 auc: 0.983 acc:0.930\n",
      "2017-10-23 23:04:21,634 - INFO - step: 70,loss: 34.243 auc: 0.954 acc:0.906\n",
      "2017-10-23 23:05:20,399 - INFO - step: 71,loss: 26.962 auc: 0.977 acc:0.898\n",
      "2017-10-23 23:06:21,402 - INFO - step: 72,loss: 27.736 auc: 0.974 acc:0.906\n",
      "2017-10-23 23:07:19,752 - INFO - step: 73,loss: 44.736 auc: 0.931 acc:0.859\n",
      "2017-10-23 23:08:18,571 - INFO - step: 74,loss: 32.520 auc: 0.962 acc:0.898\n",
      "2017-10-23 23:09:15,883 - INFO - step: 75,loss: 44.675 auc: 0.956 acc:0.844\n",
      "2017-10-23 23:10:14,626 - INFO - step: 76,loss: 35.751 auc: 0.959 acc:0.891\n",
      "2017-10-23 23:11:12,899 - INFO - step: 77,loss: 32.367 auc: 0.982 acc:0.898\n",
      "2017-10-23 23:12:09,554 - INFO - step: 78,loss: 23.684 auc: 0.979 acc:0.906\n",
      "2017-10-23 23:13:06,133 - INFO - step: 79,loss: 34.810 auc: 0.976 acc:0.883\n",
      "2017-10-23 23:13:45,879 - INFO - step: 80,loss: 17.144 auc: 0.962 acc:0.877\n",
      "2017-10-23 23:13:45,883 - INFO - \n",
      "train_epoch: 3.000\n",
      "2017-10-23 23:13:45,885 - INFO - loss_total: 31.552 accuracy_total: 0.966\n",
      "2017-10-23 23:14:41,295 - INFO - step: 81,loss: 24.422 auc: 0.986 acc:0.922\n",
      "2017-10-23 23:15:37,652 - INFO - step: 82,loss: 54.465 auc: 0.943 acc:0.859\n",
      "2017-10-23 23:16:36,315 - INFO - step: 83,loss: 24.272 auc: 0.992 acc:0.922\n",
      "2017-10-23 23:17:34,285 - INFO - step: 84,loss: 29.376 auc: 0.964 acc:0.906\n",
      "2017-10-23 23:18:32,539 - INFO - step: 85,loss: 34.897 auc: 0.968 acc:0.883\n",
      "2017-10-23 23:19:33,032 - INFO - step: 86,loss: 29.776 auc: 0.975 acc:0.891\n",
      "2017-10-23 23:20:33,660 - INFO - step: 87,loss: 48.892 auc: 0.928 acc:0.836\n",
      "2017-10-23 23:21:33,663 - INFO - step: 88,loss: 35.627 auc: 0.950 acc:0.883\n",
      "2017-10-23 23:22:31,908 - INFO - step: 89,loss: 26.768 auc: 0.974 acc:0.953\n",
      "2017-10-23 23:23:33,066 - INFO - step: 90,loss: 42.255 auc: 0.942 acc:0.867\n",
      "2017-10-23 23:24:32,011 - INFO - step: 91,loss: 28.528 auc: 0.973 acc:0.875\n",
      "2017-10-23 23:25:28,104 - INFO - step: 92,loss: 37.248 auc: 0.960 acc:0.898\n",
      "2017-10-23 23:26:24,167 - INFO - step: 93,loss: 26.667 auc: 0.974 acc:0.922\n",
      "2017-10-23 23:27:17,876 - INFO - step: 94,loss: 28.860 auc: 0.978 acc:0.906\n",
      "2017-10-23 23:28:16,590 - INFO - step: 95,loss: 28.216 auc: 0.974 acc:0.891\n",
      "2017-10-23 23:29:16,636 - INFO - step: 96,loss: 26.574 auc: 0.975 acc:0.922\n",
      "2017-10-23 23:30:17,329 - INFO - step: 97,loss: 29.046 auc: 0.971 acc:0.875\n",
      "2017-10-23 23:31:14,558 - INFO - step: 98,loss: 32.190 auc: 0.966 acc:0.883\n",
      "2017-10-23 23:32:11,819 - INFO - step: 99,loss: 19.538 auc: 0.988 acc:0.945\n",
      "2017-10-23 23:32:46,702 - INFO - step: 100,loss: 19.387 auc: 0.961 acc:0.923\n",
      "2017-10-23 23:32:46,706 - INFO - \n",
      "train_epoch: 4.000\n",
      "2017-10-23 23:32:46,709 - INFO - loss_total: 31.350 accuracy_total: 0.967\n",
      "2017-10-23 23:32:46,710 - INFO - \n",
      "Evaluation: 4.000\n",
      "2017-10-23 23:33:24,604 - INFO - step: 100,loss: 73.871 auc: 0.973 acc:0.888\n",
      "2017-10-23 23:33:25,171 - INFO - Saving model to /home/zx/cuckoo_1000/session_save2/checkpoints/model-4 at epoch 4.\n",
      "2017-10-23 23:34:20,726 - INFO - step: 101,loss: 27.696 auc: 0.983 acc:0.914\n",
      "2017-10-23 23:35:17,371 - INFO - step: 102,loss: 29.572 auc: 0.968 acc:0.906\n",
      "2017-10-23 23:36:16,242 - INFO - step: 103,loss: 48.343 auc: 0.925 acc:0.844\n",
      "2017-10-23 23:37:14,494 - INFO - step: 104,loss: 33.213 auc: 0.957 acc:0.852\n",
      "2017-10-23 23:38:12,704 - INFO - step: 105,loss: 25.316 auc: 0.978 acc:0.898\n",
      "2017-10-23 23:39:10,094 - INFO - step: 106,loss: 20.365 auc: 0.983 acc:0.930\n",
      "2017-10-23 23:40:08,168 - INFO - step: 107,loss: 24.339 auc: 0.980 acc:0.922\n",
      "2017-10-23 23:41:07,028 - INFO - step: 108,loss: 26.032 auc: 0.979 acc:0.930\n",
      "2017-10-23 23:42:06,664 - INFO - step: 109,loss: 33.021 auc: 0.962 acc:0.859\n",
      "2017-10-23 23:43:04,177 - INFO - step: 110,loss: 30.720 auc: 0.965 acc:0.883\n",
      "2017-10-23 23:44:03,755 - INFO - step: 111,loss: 20.043 auc: 0.984 acc:0.953\n",
      "2017-10-23 23:45:02,142 - INFO - step: 112,loss: 23.317 auc: 0.983 acc:0.922\n",
      "2017-10-23 23:45:59,359 - INFO - step: 113,loss: 27.914 auc: 0.971 acc:0.898\n",
      "2017-10-23 23:46:56,773 - INFO - step: 114,loss: 22.107 auc: 0.982 acc:0.922\n",
      "2017-10-23 23:47:53,405 - INFO - step: 115,loss: 23.002 auc: 0.981 acc:0.930\n",
      "2017-10-23 23:48:49,832 - INFO - step: 116,loss: 25.691 auc: 0.977 acc:0.938\n",
      "2017-10-23 23:49:44,876 - INFO - step: 117,loss: 23.073 auc: 0.983 acc:0.906\n",
      "2017-10-23 23:50:41,551 - INFO - step: 118,loss: 23.100 auc: 0.981 acc:0.945\n",
      "2017-10-23 23:51:38,189 - INFO - step: 119,loss: 24.186 auc: 0.985 acc:0.891\n",
      "2017-10-23 23:52:13,865 - INFO - step: 120,loss: 13.289 auc: 0.962 acc:0.923\n",
      "2017-10-23 23:52:13,870 - INFO - \n",
      "train_epoch: 5.000\n",
      "2017-10-23 23:52:13,873 - INFO - loss_total: 26.217 accuracy_total: 0.973\n",
      "2017-10-23 23:53:10,860 - INFO - step: 121,loss: 24.593 auc: 0.976 acc:0.906\n",
      "2017-10-23 23:54:12,848 - INFO - step: 122,loss: 17.139 auc: 0.990 acc:0.945\n",
      "2017-10-23 23:55:10,349 - INFO - step: 123,loss: 20.534 auc: 0.979 acc:0.953\n",
      "2017-10-23 23:56:07,840 - INFO - step: 124,loss: 42.691 auc: 0.944 acc:0.852\n",
      "2017-10-23 23:57:04,565 - INFO - step: 125,loss: 22.757 auc: 0.986 acc:0.906\n",
      "2017-10-23 23:58:03,203 - INFO - step: 126,loss: 24.067 auc: 0.979 acc:0.930\n",
      "2017-10-23 23:59:04,218 - INFO - step: 127,loss: 20.541 auc: 0.989 acc:0.930\n",
      "2017-10-24 00:00:10,524 - INFO - step: 128,loss: 22.827 auc: 0.983 acc:0.914\n",
      "2017-10-24 00:01:09,830 - INFO - step: 129,loss: 25.078 auc: 0.981 acc:0.930\n",
      "2017-10-24 08:26:07,995 - INFO - step: 644,loss: 3.365 auc: 1.000 acc:0.984\n",
      "2017-10-24 08:27:07,436 - INFO - step: 645,loss: 2.949 auc: 1.000 acc:0.992\n",
      "2017-10-24 08:28:04,998 - INFO - step: 646,loss: 1.995 auc: 1.000 acc:1.000\n",
      "2017-10-24 08:29:02,982 - INFO - step: 647,loss: 4.413 auc: 0.999 acc:0.992\n",
      "2017-10-24 08:30:02,404 - INFO - step: 648,loss: 10.460 auc: 0.996 acc:0.961\n",
      "2017-10-24 08:31:02,240 - INFO - step: 649,loss: 6.769 auc: 1.000 acc:0.977\n",
      "2017-10-24 08:32:01,172 - INFO - step: 650,loss: 8.260 auc: 0.999 acc:0.977\n",
      "2017-10-24 08:33:00,338 - INFO - step: 651,loss: 13.738 auc: 0.998 acc:0.953\n",
      "2017-10-24 08:34:02,227 - INFO - step: 652,loss: 6.569 auc: 1.000 acc:0.969\n",
      "2017-10-24 08:35:06,540 - INFO - step: 653,loss: 3.050 auc: 1.000 acc:0.992\n",
      "2017-10-24 08:36:08,661 - INFO - step: 654,loss: 10.490 auc: 0.998 acc:0.961\n",
      "2017-10-24 08:37:10,756 - INFO - step: 655,loss: 3.405 auc: 1.000 acc:0.992\n",
      "2017-10-24 08:38:12,691 - INFO - step: 656,loss: 7.428 auc: 0.999 acc:0.977\n",
      "2017-10-24 08:39:11,645 - INFO - step: 657,loss: 3.321 auc: 1.000 acc:1.000\n",
      "2017-10-24 08:40:13,809 - INFO - step: 658,loss: 5.832 auc: 1.000 acc:0.977\n",
      "2017-10-24 08:41:13,950 - INFO - step: 659,loss: 7.768 auc: 0.997 acc:0.984\n",
      "2017-10-24 08:41:49,934 - INFO - step: 660,loss: 1.290 auc: 1.000 acc:0.985\n",
      "2017-10-24 08:41:49,939 - INFO - \n",
      "train_epoch: 32.000\n",
      "2017-10-24 08:41:49,942 - INFO - loss_total: 6.340 accuracy_total: 0.999\n",
      "2017-10-24 08:41:49,943 - INFO - \n",
      "Evaluation: 32.000\n",
      "2017-10-24 08:42:31,180 - INFO - step: 660,loss: 55.272 auc: 0.980 acc:0.910\n",
      "2017-10-24 08:43:28,469 - INFO - step: 661,loss: 5.489 auc: 1.000 acc:0.984\n",
      "2017-10-24 08:44:28,540 - INFO - step: 662,loss: 6.693 auc: 0.998 acc:0.992\n",
      "2017-10-24 08:45:30,944 - INFO - step: 663,loss: 4.794 auc: 1.000 acc:0.992\n",
      "2017-10-24 08:46:34,801 - INFO - step: 664,loss: 5.255 auc: 1.000 acc:0.984\n",
      "2017-10-24 08:47:34,740 - INFO - step: 665,loss: 7.741 auc: 0.999 acc:0.984\n",
      "2017-10-24 08:48:34,627 - INFO - step: 666,loss: 4.934 auc: 1.000 acc:0.992\n",
      "2017-10-24 08:49:35,046 - INFO - step: 667,loss: 3.732 auc: 0.999 acc:0.992\n",
      "2017-10-24 08:50:33,404 - INFO - step: 668,loss: 7.760 auc: 0.999 acc:0.977\n",
      "2017-10-24 08:51:31,831 - INFO - step: 669,loss: 4.105 auc: 1.000 acc:0.992\n",
      "2017-10-24 08:52:31,252 - INFO - step: 670,loss: 8.396 auc: 0.996 acc:0.992\n",
      "2017-10-24 08:53:32,027 - INFO - step: 671,loss: 8.403 auc: 0.997 acc:0.984\n",
      "2017-10-24 08:54:30,783 - INFO - step: 672,loss: 2.608 auc: 1.000 acc:0.984\n",
      "2017-10-24 08:55:28,489 - INFO - step: 673,loss: 7.198 auc: 0.998 acc:0.984\n",
      "2017-10-24 08:56:26,280 - INFO - step: 674,loss: 8.278 auc: 0.998 acc:0.984\n",
      "2017-10-24 08:57:27,106 - INFO - step: 675,loss: 3.517 auc: 1.000 acc:0.984\n",
      "2017-10-24 08:58:24,628 - INFO - step: 676,loss: 8.682 auc: 0.998 acc:0.977\n",
      "2017-10-24 08:59:23,262 - INFO - step: 677,loss: 8.255 auc: 0.998 acc:0.961\n",
      "2017-10-24 09:00:22,591 - INFO - step: 678,loss: 5.042 auc: 0.999 acc:0.984\n",
      "2017-10-24 09:01:22,594 - INFO - step: 679,loss: 4.005 auc: 1.000 acc:0.984\n",
      "2017-10-24 09:01:59,142 - INFO - step: 680,loss: 1.309 auc: 1.000 acc:1.000\n",
      "2017-10-24 09:01:59,149 - INFO - \n",
      "train_epoch: 33.000\n",
      "2017-10-24 09:01:59,151 - INFO - loss_total: 5.810 accuracy_total: 0.999\n",
      "2017-10-24 09:02:58,629 - INFO - step: 681,loss: 10.425 auc: 0.996 acc:0.984\n",
      "2017-10-24 09:03:56,755 - INFO - step: 682,loss: 5.527 auc: 0.999 acc:0.977\n",
      "2017-10-24 09:04:56,170 - INFO - step: 683,loss: 11.458 auc: 0.997 acc:0.969\n",
      "2017-10-24 09:05:53,521 - INFO - step: 684,loss: 3.621 auc: 1.000 acc:0.992\n",
      "2017-10-24 09:06:48,355 - INFO - step: 685,loss: 6.995 auc: 0.999 acc:0.977\n",
      "2017-10-24 09:07:46,487 - INFO - step: 686,loss: 5.659 auc: 1.000 acc:0.977\n",
      "2017-10-24 09:08:44,472 - INFO - step: 687,loss: 8.731 auc: 0.998 acc:0.969\n",
      "2017-10-24 09:09:40,186 - INFO - step: 688,loss: 9.214 auc: 0.998 acc:0.961\n",
      "2017-10-24 09:10:36,583 - INFO - step: 689,loss: 4.841 auc: 0.999 acc:0.977\n",
      "2017-10-24 09:11:31,808 - INFO - step: 690,loss: 2.558 auc: 1.000 acc:1.000\n",
      "2017-10-24 09:12:28,785 - INFO - step: 691,loss: 5.314 auc: 1.000 acc:0.977\n",
      "2017-10-24 09:13:27,266 - INFO - step: 692,loss: 5.914 auc: 0.999 acc:0.984\n",
      "2017-10-24 09:32:45,333 - INFO - step: 711,loss: 7.246 auc: 1.000 acc:0.977\n",
      "2017-10-24 09:33:45,823 - INFO - step: 712,loss: 5.198 auc: 1.000 acc:0.977\n",
      "2017-10-24 09:34:45,673 - INFO - step: 713,loss: 7.947 auc: 0.998 acc:0.961\n",
      "2017-10-24 09:35:43,904 - INFO - step: 714,loss: 5.716 auc: 0.999 acc:0.977\n",
      "2017-10-24 09:36:41,784 - INFO - step: 715,loss: 4.037 auc: 1.000 acc:0.984\n",
      "2017-10-24 09:37:39,437 - INFO - step: 716,loss: 5.317 auc: 0.999 acc:0.984\n",
      "2017-10-24 09:38:37,575 - INFO - step: 717,loss: 4.372 auc: 0.999 acc:0.992\n",
      "2017-10-24 09:39:34,790 - INFO - step: 718,loss: 9.265 auc: 0.999 acc:0.969\n",
      "2017-10-24 09:40:34,896 - INFO - step: 719,loss: 0.748 auc: 1.000 acc:1.000\n",
      "2017-10-24 09:41:11,655 - INFO - step: 720,loss: 5.354 auc: 0.999 acc:0.985\n",
      "2017-10-24 09:41:11,662 - INFO - \n",
      "train_epoch: 35.000\n",
      "2017-10-24 09:41:11,664 - INFO - loss_total: 6.129 accuracy_total: 0.999\n",
      "2017-10-24 09:42:12,878 - INFO - step: 721,loss: 7.775 auc: 0.998 acc:0.984\n",
      "2017-10-24 09:43:11,784 - INFO - step: 722,loss: 5.081 auc: 1.000 acc:0.984\n",
      "2017-10-24 09:44:10,646 - INFO - step: 723,loss: 4.349 auc: 1.000 acc:0.984\n",
      "2017-10-24 09:45:08,436 - INFO - step: 724,loss: 2.572 auc: 1.000 acc:1.000\n",
      "2017-10-24 09:46:05,335 - INFO - step: 725,loss: 12.539 auc: 0.996 acc:0.953\n",
      "2017-10-24 09:47:07,756 - INFO - step: 726,loss: 6.467 auc: 0.999 acc:0.977\n",
      "2017-10-24 09:48:04,210 - INFO - step: 727,loss: 3.399 auc: 1.000 acc:0.992\n",
      "2017-10-24 09:49:02,550 - INFO - step: 728,loss: 1.729 auc: 1.000 acc:1.000\n",
      "2017-10-24 09:49:58,215 - INFO - step: 729,loss: 5.846 auc: 0.999 acc:0.984\n",
      "2017-10-24 09:50:56,081 - INFO - step: 730,loss: 12.568 auc: 1.000 acc:0.953\n",
      "2017-10-24 09:51:54,392 - INFO - step: 731,loss: 1.583 auc: 1.000 acc:0.992\n",
      "2017-10-24 09:52:50,808 - INFO - step: 732,loss: 6.024 auc: 0.999 acc:0.984\n",
      "2017-10-24 09:53:49,259 - INFO - step: 733,loss: 5.701 auc: 0.999 acc:0.984\n",
      "2017-10-24 09:54:47,113 - INFO - step: 734,loss: 4.524 auc: 1.000 acc:0.992\n",
      "2017-10-24 09:55:44,580 - INFO - step: 735,loss: 3.345 auc: 1.000 acc:0.984\n",
      "2017-10-24 09:56:43,061 - INFO - step: 736,loss: 9.079 auc: 1.000 acc:0.969\n",
      "2017-10-24 09:57:41,944 - INFO - step: 737,loss: 3.777 auc: 1.000 acc:0.977\n",
      "2017-10-24 09:58:42,258 - INFO - step: 738,loss: 2.610 auc: 1.000 acc:0.992\n",
      "2017-10-24 09:59:42,232 - INFO - step: 739,loss: 3.291 auc: 1.000 acc:0.992\n",
      "2017-10-24 10:00:24,079 - INFO - step: 740,loss: 5.055 auc: 0.997 acc:0.969\n",
      "2017-10-24 10:00:24,085 - INFO - \n",
      "train_epoch: 36.000\n",
      "2017-10-24 10:00:24,087 - INFO - loss_total: 5.366 accuracy_total: 0.999\n",
      "2017-10-24 10:00:24,088 - INFO - \n",
      "Evaluation: 36.000\n",
      "2017-10-24 10:00:59,148 - INFO - step: 740,loss: 45.299 auc: 0.987 acc:0.932\n",
      "2017-10-24 10:01:57,044 - INFO - step: 741,loss: 3.500 auc: 1.000 acc:0.984\n",
      "2017-10-24 10:02:56,621 - INFO - step: 742,loss: 11.824 auc: 0.998 acc:0.969\n",
      "2017-10-24 10:03:54,631 - INFO - step: 743,loss: 2.473 auc: 1.000 acc:0.992\n",
      "2017-10-24 10:04:53,495 - INFO - step: 744,loss: 6.022 auc: 1.000 acc:0.969\n",
      "2017-10-24 10:05:52,683 - INFO - step: 745,loss: 5.033 auc: 0.999 acc:0.977\n",
      "2017-10-24 10:06:51,615 - INFO - step: 746,loss: 2.917 auc: 1.000 acc:0.992\n",
      "2017-10-24 10:07:50,644 - INFO - step: 747,loss: 3.148 auc: 1.000 acc:0.992\n",
      "2017-10-24 10:08:49,683 - INFO - step: 748,loss: 4.250 auc: 0.999 acc:0.992\n",
      "2017-10-24 10:09:52,879 - INFO - step: 749,loss: 4.603 auc: 0.999 acc:0.984\n",
      "2017-10-24 10:10:54,339 - INFO - step: 750,loss: 9.301 auc: 0.997 acc:0.969\n",
      "2017-10-24 10:11:56,412 - INFO - step: 751,loss: 5.852 auc: 0.999 acc:0.977\n",
      "2017-10-24 10:12:55,266 - INFO - step: 752,loss: 3.990 auc: 0.999 acc:0.984\n",
      "2017-10-24 10:13:59,168 - INFO - step: 753,loss: 1.400 auc: 1.000 acc:1.000\n",
      "2017-10-24 10:15:00,638 - INFO - step: 754,loss: 1.611 auc: 1.000 acc:1.000\n",
      "2017-10-24 10:16:02,042 - INFO - step: 755,loss: 6.388 auc: 0.998 acc:0.977\n",
      "2017-10-24 10:17:01,347 - INFO - step: 756,loss: 4.409 auc: 0.999 acc:0.984\n",
      "2017-10-24 10:18:01,150 - INFO - step: 757,loss: 3.070 auc: 1.000 acc:1.000\n",
      "2017-10-24 10:19:03,720 - INFO - step: 758,loss: 6.054 auc: 0.999 acc:0.984\n",
      "2017-10-24 10:20:09,006 - INFO - step: 759,loss: 3.615 auc: 1.000 acc:0.992\n",
      "2017-10-24 10:20:48,088 - INFO - step: 760,loss: 2.827 auc: 1.000 acc:1.000\n",
      "2017-10-24 10:20:48,093 - INFO - \n",
      "train_epoch: 37.000\n",
      "2017-10-24 10:20:48,096 - INFO - loss_total: 4.614 accuracy_total: 0.999\n",
      "2017-10-24 10:21:46,687 - INFO - step: 761,loss: 4.897 auc: 0.999 acc:0.984\n",
      "2017-10-24 10:22:46,695 - INFO - step: 762,loss: 2.965 auc: 1.000 acc:1.000\n",
      "2017-10-24 10:23:45,601 - INFO - step: 763,loss: 3.161 auc: 0.999 acc:0.992\n",
      "2017-10-24 10:24:42,948 - INFO - step: 764,loss: 7.906 auc: 0.999 acc:0.969\n",
      "2017-10-24 10:25:41,567 - INFO - step: 765,loss: 3.593 auc: 1.000 acc:0.992\n",
      "2017-10-24 10:26:41,173 - INFO - step: 766,loss: 8.221 auc: 0.999 acc:0.977\n",
      "2017-10-24 10:27:41,319 - INFO - step: 767,loss: 4.134 auc: 1.000 acc:0.992\n",
      "2017-10-24 10:28:44,438 - INFO - step: 768,loss: 5.210 auc: 0.999 acc:0.977\n",
      "2017-10-24 10:29:46,590 - INFO - step: 769,loss: 1.631 auc: 1.000 acc:1.000\n",
      "2017-10-24 10:30:47,340 - INFO - step: 770,loss: 2.661 auc: 1.000 acc:0.992\n",
      "2017-10-24 10:31:46,974 - INFO - step: 771,loss: 1.877 auc: 1.000 acc:0.992\n",
      "2017-10-24 10:32:46,356 - INFO - step: 772,loss: 3.793 auc: 1.000 acc:0.984\n",
      "2017-10-24 10:33:46,927 - INFO - step: 773,loss: 5.021 auc: 1.000 acc:0.977\n",
      "2017-10-24 10:34:45,272 - INFO - step: 774,loss: 4.950 auc: 0.999 acc:0.992\n",
      "2017-10-24 10:35:43,258 - INFO - step: 775,loss: 1.494 auc: 1.000 acc:1.000\n",
      "2017-10-24 10:36:39,341 - INFO - step: 776,loss: 2.546 auc: 1.000 acc:0.992\n",
      "2017-10-24 10:37:38,381 - INFO - step: 777,loss: 16.143 auc: 0.994 acc:0.945\n",
      "2017-10-24 10:38:38,447 - INFO - step: 778,loss: 9.365 auc: 0.998 acc:0.984\n",
      "2017-10-24 10:39:39,200 - INFO - step: 779,loss: 4.398 auc: 0.999 acc:0.992\n",
      "2017-10-24 10:40:18,559 - INFO - step: 780,loss: 1.173 auc: 1.000 acc:1.000\n",
      "2017-10-24 10:40:18,563 - INFO - \n",
      "train_epoch: 38.000\n",
      "2017-10-24 10:40:18,565 - INFO - loss_total: 4.757 accuracy_total: 0.999\n",
      "2017-10-24 10:40:18,567 - INFO - \n",
      "Evaluation: 38.000\n",
      "2017-10-24 10:40:52,893 - INFO - step: 780,loss: 54.204 auc: 0.982 acc:0.928\n",
      "2017-10-24 10:41:50,215 - INFO - step: 781,loss: 9.059 auc: 0.997 acc:0.977\n",
      "2017-10-24 10:42:46,724 - INFO - step: 782,loss: 10.694 auc: 0.997 acc:0.977\n",
      "2017-10-24 10:43:42,971 - INFO - step: 783,loss: 3.313 auc: 1.000 acc:0.992\n",
      "2017-10-24 10:44:40,318 - INFO - step: 784,loss: 8.618 auc: 0.997 acc:0.961\n",
      "2017-10-24 10:45:39,889 - INFO - step: 785,loss: 7.115 auc: 0.999 acc:0.969\n",
      "2017-10-24 10:46:36,037 - INFO - step: 786,loss: 1.025 auc: 1.000 acc:1.000\n",
      "2017-10-24 10:47:33,090 - INFO - step: 787,loss: 2.662 auc: 1.000 acc:0.992\n",
      "2017-10-24 10:48:30,342 - INFO - step: 788,loss: 9.252 auc: 0.998 acc:0.977\n",
      "2017-10-24 10:49:26,953 - INFO - step: 789,loss: 1.660 auc: 1.000 acc:1.000\n",
      "2017-10-24 10:50:25,193 - INFO - step: 790,loss: 5.176 auc: 1.000 acc:0.984\n",
      "2017-10-24 10:51:26,023 - INFO - step: 791,loss: 9.190 auc: 0.996 acc:0.977\n",
      "2017-10-24 10:52:27,185 - INFO - step: 792,loss: 2.357 auc: 1.000 acc:0.992\n",
      "2017-10-24 10:53:27,777 - INFO - step: 793,loss: 6.303 auc: 1.000 acc:0.969\n",
      "2017-10-24 10:54:26,737 - INFO - step: 794,loss: 5.203 auc: 0.999 acc:0.977\n",
      "2017-10-24 10:55:25,849 - INFO - step: 795,loss: 8.684 auc: 0.998 acc:0.984\n",
      "2017-10-24 10:56:26,430 - INFO - step: 796,loss: 2.567 auc: 1.000 acc:0.992\n",
      "2017-10-24 10:57:24,798 - INFO - step: 797,loss: 5.815 auc: 0.999 acc:0.984\n",
      "2017-10-24 10:58:24,215 - INFO - step: 798,loss: 6.185 auc: 0.999 acc:0.977\n",
      "2017-10-24 10:59:25,116 - INFO - step: 799,loss: 4.317 auc: 0.999 acc:0.992\n",
      "2017-10-24 11:00:03,498 - INFO - step: 800,loss: 2.641 auc: 1.000 acc:0.985\n",
      "2017-10-24 11:00:03,503 - INFO - \n",
      "train_epoch: 39.000\n",
      "2017-10-24 11:00:03,506 - INFO - loss_total: 5.592 accuracy_total: 0.999\n",
      "2017-10-24 11:01:01,125 - INFO - step: 801,loss: 2.953 auc: 1.000 acc:0.984\n",
      "2017-10-24 11:01:58,873 - INFO - step: 802,loss: 6.145 auc: 0.999 acc:0.969\n",
      "2017-10-24 11:02:56,044 - INFO - step: 803,loss: 1.619 auc: 1.000 acc:1.000\n",
      "2017-10-24 11:03:53,565 - INFO - step: 804,loss: 3.546 auc: 1.000 acc:0.984\n",
      "2017-10-24 11:04:50,069 - INFO - step: 805,loss: 9.859 auc: 0.998 acc:0.984\n",
      "2017-10-24 11:05:47,910 - INFO - step: 806,loss: 5.979 auc: 0.999 acc:0.977\n",
      "2017-10-24 11:06:44,691 - INFO - step: 807,loss: 2.826 auc: 1.000 acc:0.992\n",
      "2017-10-24 11:07:42,000 - INFO - step: 808,loss: 1.364 auc: 1.000 acc:1.000\n",
      "2017-10-24 11:08:43,280 - INFO - step: 809,loss: 7.064 auc: 0.999 acc:0.977\n",
      "2017-10-24 11:09:44,201 - INFO - step: 810,loss: 5.029 auc: 1.000 acc:0.984\n",
      "2017-10-24 11:10:43,489 - INFO - step: 811,loss: 11.283 auc: 0.992 acc:0.977\n",
      "2017-10-24 11:11:41,818 - INFO - step: 812,loss: 6.206 auc: 0.998 acc:0.992\n",
      "2017-10-24 11:12:38,796 - INFO - step: 813,loss: 3.568 auc: 0.999 acc:0.992\n",
      "2017-10-24 11:13:38,311 - INFO - step: 814,loss: 4.698 auc: 0.999 acc:0.992\n",
      "2017-10-24 11:14:37,013 - INFO - step: 815,loss: 8.169 auc: 0.999 acc:0.977\n",
      "2017-10-24 11:15:36,944 - INFO - step: 816,loss: 4.266 auc: 1.000 acc:0.992\n",
      "2017-10-24 11:16:37,347 - INFO - step: 817,loss: 9.082 auc: 0.998 acc:0.969\n",
      "2017-10-24 11:17:37,459 - INFO - step: 818,loss: 6.507 auc: 1.000 acc:0.961\n",
      "2017-10-24 11:18:36,813 - INFO - step: 819,loss: 5.519 auc: 1.000 acc:0.984\n",
      "2017-10-24 11:19:13,219 - INFO - step: 820,loss: 4.043 auc: 0.999 acc:0.969\n",
      "2017-10-24 11:19:13,225 - INFO - \n",
      "train_epoch: 40.000\n",
      "2017-10-24 11:19:13,228 - INFO - loss_total: 5.486 accuracy_total: 0.999\n",
      "2017-10-24 11:19:13,230 - INFO - \n",
      "Evaluation: 40.000\n",
      "2017-10-24 11:19:54,365 - INFO - step: 820,loss: 54.617 auc: 0.980 acc:0.921\n",
      "2017-10-24 11:20:53,149 - INFO - step: 821,loss: 6.611 auc: 0.999 acc:0.969\n",
      "2017-10-24 11:21:53,923 - INFO - step: 822,loss: 7.423 auc: 0.998 acc:0.984\n",
      "2017-10-24 11:22:50,155 - INFO - step: 823,loss: 3.877 auc: 1.000 acc:0.992\n",
      "2017-10-24 11:23:52,282 - INFO - step: 824,loss: 3.128 auc: 1.000 acc:0.992\n",
      "2017-10-24 11:24:53,588 - INFO - step: 825,loss: 2.733 auc: 1.000 acc:0.992\n",
      "2017-10-24 11:25:55,076 - INFO - step: 826,loss: 2.077 auc: 1.000 acc:0.992\n",
      "2017-10-24 11:26:54,528 - INFO - step: 827,loss: 10.777 auc: 0.997 acc:0.961\n",
      "2017-10-24 11:27:53,205 - INFO - step: 828,loss: 6.734 auc: 0.998 acc:0.969\n",
      "2017-10-24 11:28:59,734 - INFO - step: 829,loss: 3.392 auc: 1.000 acc:0.984\n",
      "2017-10-24 11:29:59,833 - INFO - step: 830,loss: 1.474 auc: 1.000 acc:1.000\n",
      "2017-10-24 11:31:04,807 - INFO - step: 831,loss: 11.741 auc: 0.995 acc:0.961\n",
      "2017-10-24 11:32:06,130 - INFO - step: 832,loss: 7.380 auc: 0.999 acc:0.961\n",
      "2017-10-24 11:33:05,287 - INFO - step: 833,loss: 5.899 auc: 0.999 acc:0.977\n",
      "2017-10-24 11:34:07,262 - INFO - step: 834,loss: 4.547 auc: 0.999 acc:0.984\n",
      "2017-10-24 11:35:03,575 - INFO - step: 835,loss: 2.607 auc: 1.000 acc:0.992\n",
      "2017-10-24 11:36:04,267 - INFO - step: 836,loss: 3.859 auc: 1.000 acc:0.984\n",
      "2017-10-24 11:37:08,047 - INFO - step: 837,loss: 4.214 auc: 1.000 acc:0.984\n",
      "2017-10-24 11:38:09,075 - INFO - step: 838,loss: 3.441 auc: 1.000 acc:1.000\n",
      "2017-10-24 11:39:08,600 - INFO - step: 839,loss: 5.713 auc: 0.997 acc:0.992\n",
      "2017-10-24 11:39:46,316 - INFO - step: 840,loss: 0.263 auc: 1.000 acc:1.000\n",
      "2017-10-24 11:39:46,321 - INFO - \n",
      "train_epoch: 41.000\n",
      "2017-10-24 11:39:46,323 - INFO - loss_total: 4.894 accuracy_total: 0.999\n",
      "2017-10-24 11:40:45,583 - INFO - step: 841,loss: 4.283 auc: 1.000 acc:0.984\n",
      "2017-10-24 11:41:43,878 - INFO - step: 842,loss: 1.248 auc: 1.000 acc:1.000\n",
      "2017-10-24 11:42:42,955 - INFO - step: 843,loss: 5.180 auc: 0.999 acc:0.984\n",
      "2017-10-24 11:43:41,158 - INFO - step: 844,loss: 7.305 auc: 1.000 acc:0.977\n",
      "2017-10-24 11:44:38,175 - INFO - step: 845,loss: 7.977 auc: 0.998 acc:0.969\n",
      "2017-10-24 11:45:32,504 - INFO - step: 846,loss: 1.555 auc: 1.000 acc:0.992\n",
      "2017-10-24 11:46:27,362 - INFO - step: 847,loss: 2.266 auc: 1.000 acc:0.992\n",
      "2017-10-24 11:47:24,915 - INFO - step: 848,loss: 9.016 auc: 0.997 acc:0.961\n",
      "2017-10-24 11:48:23,554 - INFO - step: 849,loss: 10.817 auc: 0.996 acc:0.961\n",
      "2017-10-24 11:49:23,487 - INFO - step: 850,loss: 11.902 auc: 0.999 acc:0.961\n",
      "2017-10-24 11:50:21,823 - INFO - step: 851,loss: 5.149 auc: 1.000 acc:0.977\n",
      "2017-10-24 11:51:21,419 - INFO - step: 852,loss: 3.282 auc: 1.000 acc:0.992\n",
      "2017-10-24 11:52:24,122 - INFO - step: 853,loss: 3.382 auc: 1.000 acc:0.992\n",
      "2017-10-24 11:53:22,592 - INFO - step: 854,loss: 7.631 auc: 0.996 acc:0.992\n",
      "2017-10-24 11:54:21,128 - INFO - step: 855,loss: 6.068 auc: 0.998 acc:0.984\n",
      "2017-10-24 11:55:17,474 - INFO - step: 856,loss: 1.226 auc: 1.000 acc:1.000\n",
      "2017-10-24 11:56:16,581 - INFO - step: 857,loss: 7.231 auc: 0.998 acc:0.984\n",
      "2017-10-24 11:57:16,599 - INFO - step: 858,loss: 6.029 auc: 0.998 acc:0.992\n",
      "2017-10-24 11:58:15,147 - INFO - step: 859,loss: 8.429 auc: 0.998 acc:0.977\n",
      "2017-10-24 11:58:51,938 - INFO - step: 860,loss: 1.306 auc: 1.000 acc:0.985\n",
      "2017-10-24 11:58:51,943 - INFO - \n",
      "train_epoch: 42.000\n",
      "2017-10-24 11:58:51,946 - INFO - loss_total: 5.564 accuracy_total: 0.999\n",
      "2017-10-24 11:58:51,947 - INFO - \n",
      "Evaluation: 42.000\n",
      "2017-10-24 11:59:32,392 - INFO - step: 860,loss: 47.995 auc: 0.984 acc:0.932\n",
      "2017-10-24 12:00:34,434 - INFO - step: 861,loss: 2.715 auc: 1.000 acc:0.992\n",
      "2017-10-24 12:01:37,001 - INFO - step: 862,loss: 7.665 auc: 0.998 acc:0.961\n",
      "2017-10-24 12:02:37,968 - INFO - step: 863,loss: 5.373 auc: 1.000 acc:0.977\n",
      "2017-10-24 12:03:36,435 - INFO - step: 864,loss: 3.639 auc: 1.000 acc:0.984\n",
      "2017-10-24 12:04:33,655 - INFO - step: 865,loss: 4.937 auc: 0.999 acc:0.992\n",
      "2017-10-24 12:05:32,461 - INFO - step: 866,loss: 13.899 auc: 0.994 acc:0.977\n",
      "2017-10-24 12:06:29,565 - INFO - step: 867,loss: 1.919 auc: 1.000 acc:0.992\n",
      "2017-10-24 12:07:28,102 - INFO - step: 868,loss: 7.944 auc: 1.000 acc:0.984\n",
      "2017-10-24 12:08:25,795 - INFO - step: 869,loss: 8.087 auc: 0.999 acc:0.969\n",
      "2017-10-24 12:09:25,604 - INFO - step: 870,loss: 3.867 auc: 1.000 acc:0.984\n",
      "2017-10-24 12:10:24,673 - INFO - step: 871,loss: 4.045 auc: 1.000 acc:0.984\n",
      "2017-10-24 12:11:23,022 - INFO - step: 872,loss: 2.564 auc: 1.000 acc:0.992\n",
      "2017-10-24 12:12:20,970 - INFO - step: 873,loss: 6.242 auc: 1.000 acc:0.977\n",
      "2017-10-24 12:13:17,233 - INFO - step: 874,loss: 6.148 auc: 0.999 acc:0.984\n",
      "2017-10-24 12:14:14,954 - INFO - step: 875,loss: 6.881 auc: 0.999 acc:0.977\n",
      "2017-10-24 12:15:13,619 - INFO - step: 876,loss: 11.905 auc: 0.996 acc:0.961\n",
      "2017-10-24 12:16:14,174 - INFO - step: 877,loss: 2.253 auc: 1.000 acc:0.984\n",
      "2017-10-24 12:17:13,100 - INFO - step: 878,loss: 3.185 auc: 1.000 acc:0.992\n",
      "2017-10-24 12:18:11,234 - INFO - step: 879,loss: 4.659 auc: 1.000 acc:0.984\n",
      "2017-10-24 12:18:50,065 - INFO - step: 880,loss: 1.066 auc: 1.000 acc:1.000\n",
      "2017-10-24 12:18:50,069 - INFO - \n",
      "train_epoch: 43.000\n",
      "2017-10-24 12:18:50,072 - INFO - loss_total: 5.450 accuracy_total: 0.999\n",
      "2017-10-24 12:19:50,111 - INFO - step: 881,loss: 1.052 auc: 1.000 acc:1.000\n",
      "2017-10-24 12:20:51,414 - INFO - step: 882,loss: 7.704 auc: 0.999 acc:0.977\n",
      "2017-10-24 12:21:49,853 - INFO - step: 883,loss: 6.429 auc: 0.999 acc:0.977\n",
      "2017-10-24 12:22:49,970 - INFO - step: 884,loss: 6.026 auc: 0.998 acc:0.984\n",
      "2017-10-24 12:23:49,405 - INFO - step: 885,loss: 2.032 auc: 1.000 acc:0.992\n",
      "2017-10-24 12:24:46,801 - INFO - step: 886,loss: 4.169 auc: 0.999 acc:0.977\n",
      "2017-10-24 12:25:44,322 - INFO - step: 887,loss: 5.134 auc: 1.000 acc:0.984\n",
      "2017-10-24 12:26:40,774 - INFO - step: 888,loss: 3.508 auc: 1.000 acc:0.992\n",
      "2017-10-24 12:27:40,394 - INFO - step: 889,loss: 7.438 auc: 0.997 acc:0.992\n",
      "2017-10-24 12:28:36,161 - INFO - step: 890,loss: 7.134 auc: 0.998 acc:0.992\n",
      "2017-10-24 12:29:34,601 - INFO - step: 891,loss: 3.262 auc: 1.000 acc:0.984\n",
      "2017-10-24 12:30:34,610 - INFO - step: 892,loss: 1.683 auc: 1.000 acc:1.000\n",
      "2017-10-24 12:31:32,012 - INFO - step: 893,loss: 2.602 auc: 1.000 acc:0.992\n",
      "2017-10-24 12:32:28,316 - INFO - step: 894,loss: 3.742 auc: 1.000 acc:0.992\n",
      "2017-10-24 12:33:27,390 - INFO - step: 895,loss: 8.031 auc: 0.997 acc:0.984\n",
      "2017-10-24 12:34:26,728 - INFO - step: 896,loss: 5.267 auc: 0.999 acc:0.977\n",
      "2017-10-24 12:35:24,233 - INFO - step: 897,loss: 3.302 auc: 1.000 acc:0.992\n",
      "2017-10-24 12:36:21,012 - INFO - step: 898,loss: 4.720 auc: 0.999 acc:0.969\n",
      "2017-10-24 12:37:18,472 - INFO - step: 899,loss: 2.179 auc: 1.000 acc:0.992\n",
      "2017-10-24 12:37:53,784 - INFO - step: 900,loss: 2.045 auc: 1.000 acc:0.985\n",
      "2017-10-24 12:37:53,790 - INFO - \n",
      "train_epoch: 44.000\n",
      "2017-10-24 12:37:53,792 - INFO - loss_total: 4.373 accuracy_total: 0.999\n",
      "2017-10-24 12:37:53,795 - INFO - \n",
      "Evaluation: 44.000\n",
      "2017-10-24 12:38:35,216 - INFO - step: 900,loss: 52.348 auc: 0.981 acc:0.921\n",
      "2017-10-24 12:39:34,477 - INFO - step: 901,loss: 5.308 auc: 0.999 acc:0.969\n",
      "2017-10-24 12:40:34,935 - INFO - step: 902,loss: 0.732 auc: 1.000 acc:1.000\n",
      "2017-10-24 12:41:35,893 - INFO - step: 903,loss: 3.863 auc: 0.999 acc:0.984\n",
      "2017-10-24 12:42:44,360 - INFO - step: 904,loss: 3.440 auc: 1.000 acc:0.984\n",
      "2017-10-24 12:43:46,507 - INFO - step: 905,loss: 4.169 auc: 1.000 acc:0.992\n",
      "2017-10-24 12:44:49,287 - INFO - step: 906,loss: 4.620 auc: 1.000 acc:0.992\n",
      "2017-10-24 12:45:56,112 - INFO - step: 907,loss: 2.955 auc: 0.999 acc:0.992\n",
      "2017-10-24 12:46:55,582 - INFO - step: 908,loss: 2.379 auc: 1.000 acc:1.000\n",
      "2017-10-24 12:47:51,142 - INFO - step: 909,loss: 3.887 auc: 1.000 acc:0.984\n",
      "2017-10-24 12:48:47,113 - INFO - step: 910,loss: 3.488 auc: 1.000 acc:0.992\n",
      "2017-10-24 12:49:45,917 - INFO - step: 911,loss: 3.352 auc: 1.000 acc:0.992\n",
      "2017-10-24 12:50:44,992 - INFO - step: 912,loss: 3.683 auc: 1.000 acc:0.992\n",
      "2017-10-24 12:51:44,707 - INFO - step: 913,loss: 7.161 auc: 0.999 acc:0.984\n",
      "2017-10-24 12:52:41,453 - INFO - step: 914,loss: 1.992 auc: 1.000 acc:1.000\n",
      "2017-10-24 12:53:39,045 - INFO - step: 915,loss: 3.449 auc: 1.000 acc:0.992\n",
      "2017-10-24 12:54:37,627 - INFO - step: 916,loss: 7.881 auc: 0.998 acc:0.984\n",
      "2017-10-24 12:55:34,375 - INFO - step: 917,loss: 2.761 auc: 1.000 acc:0.984\n",
      "2017-10-24 12:56:30,827 - INFO - step: 918,loss: 10.119 auc: 0.999 acc:0.977\n",
      "2017-10-24 12:57:26,477 - INFO - step: 919,loss: 10.990 auc: 1.000 acc:0.969\n",
      "2017-10-24 12:58:03,122 - INFO - step: 920,loss: 4.303 auc: 0.999 acc:0.985\n",
      "2017-10-24 12:58:03,126 - INFO - \n",
      "train_epoch: 45.000\n",
      "2017-10-24 12:58:03,129 - INFO - loss_total: 4.527 accuracy_total: 1.000\n",
      "2017-10-24 12:59:02,927 - INFO - step: 921,loss: 5.009 auc: 0.999 acc:0.977\n",
      "2017-10-24 13:00:04,493 - INFO - step: 922,loss: 7.093 auc: 0.999 acc:0.977\n",
      "2017-10-24 13:01:03,727 - INFO - step: 923,loss: 6.451 auc: 0.999 acc:0.984\n",
      "2017-10-24 13:02:07,880 - INFO - step: 924,loss: 6.515 auc: 0.998 acc:0.984\n",
      "2017-10-24 13:03:10,471 - INFO - step: 925,loss: 3.599 auc: 1.000 acc:0.984\n",
      "2017-10-24 13:04:10,790 - INFO - step: 926,loss: 1.096 auc: 1.000 acc:1.000\n",
      "2017-10-24 13:05:09,814 - INFO - step: 927,loss: 4.835 auc: 1.000 acc:0.984\n",
      "2017-10-24 13:06:10,066 - INFO - step: 928,loss: 3.518 auc: 1.000 acc:0.984\n",
      "2017-10-24 13:07:05,445 - INFO - step: 929,loss: 7.851 auc: 0.997 acc:0.984\n",
      "2017-10-24 13:08:03,428 - INFO - step: 930,loss: 2.402 auc: 1.000 acc:0.992\n",
      "2017-10-24 13:09:00,772 - INFO - step: 931,loss: 3.648 auc: 0.999 acc:0.992\n",
      "2017-10-24 13:09:58,996 - INFO - step: 932,loss: 1.679 auc: 1.000 acc:1.000\n",
      "2017-10-24 13:10:57,929 - INFO - step: 933,loss: 2.981 auc: 1.000 acc:0.992\n",
      "2017-10-24 13:11:55,764 - INFO - step: 934,loss: 0.219 auc: 1.000 acc:1.000\n",
      "2017-10-24 13:12:52,359 - INFO - step: 935,loss: 3.756 auc: 1.000 acc:0.984\n",
      "2017-10-24 13:13:52,680 - INFO - step: 936,loss: 5.100 auc: 0.999 acc:0.977\n",
      "2017-10-24 13:14:51,446 - INFO - step: 937,loss: 1.883 auc: 1.000 acc:0.992\n",
      "2017-10-24 13:15:51,410 - INFO - step: 938,loss: 5.052 auc: 1.000 acc:0.984\n",
      "2017-10-24 13:16:48,424 - INFO - step: 939,loss: 3.453 auc: 1.000 acc:1.000\n",
      "2017-10-24 13:17:26,077 - INFO - step: 940,loss: 4.141 auc: 0.999 acc:0.985\n",
      "2017-10-24 13:17:26,082 - INFO - \n",
      "train_epoch: 46.000\n",
      "2017-10-24 13:17:26,085 - INFO - loss_total: 4.014 accuracy_total: 0.999\n",
      "2017-10-24 13:17:26,087 - INFO - \n",
      "Evaluation: 46.000\n",
      "2017-10-24 13:18:02,913 - INFO - step: 940,loss: 43.475 auc: 0.987 acc:0.942\n",
      "2017-10-24 13:18:57,716 - INFO - step: 941,loss: 3.685 auc: 1.000 acc:0.984\n",
      "2017-10-24 13:19:53,108 - INFO - step: 942,loss: 3.007 auc: 1.000 acc:0.984\n",
      "2017-10-24 13:20:50,644 - INFO - step: 943,loss: 1.551 auc: 1.000 acc:0.992\n",
      "2017-10-24 13:21:47,901 - INFO - step: 944,loss: 4.884 auc: 0.999 acc:0.992\n",
      "2017-10-24 13:22:44,598 - INFO - step: 945,loss: 2.036 auc: 1.000 acc:0.992\n",
      "2017-10-24 13:23:43,035 - INFO - step: 946,loss: 14.399 auc: 0.997 acc:0.953\n",
      "2017-10-24 13:24:40,061 - INFO - step: 947,loss: 7.392 auc: 0.996 acc:0.992\n",
      "2017-10-24 13:25:39,313 - INFO - step: 948,loss: 5.103 auc: 0.999 acc:0.992\n",
      "2017-10-24 13:26:36,424 - INFO - step: 949,loss: 2.789 auc: 1.000 acc:0.992\n",
      "2017-10-24 13:27:34,579 - INFO - step: 950,loss: 0.863 auc: 1.000 acc:1.000\n",
      "2017-10-24 13:28:35,541 - INFO - step: 951,loss: 2.826 auc: 1.000 acc:0.992\n",
      "2017-10-24 13:29:34,870 - INFO - step: 952,loss: 3.434 auc: 1.000 acc:0.984\n",
      "2017-10-24 13:30:34,260 - INFO - step: 953,loss: 3.167 auc: 1.000 acc:0.984\n",
      "2017-10-24 13:31:31,814 - INFO - step: 954,loss: 2.720 auc: 1.000 acc:0.992\n",
      "2017-10-24 13:32:26,989 - INFO - step: 955,loss: 3.981 auc: 1.000 acc:0.992\n",
      "2017-10-24 13:33:26,465 - INFO - step: 956,loss: 1.037 auc: 1.000 acc:1.000\n",
      "2017-10-24 13:34:27,422 - INFO - step: 957,loss: 1.532 auc: 1.000 acc:1.000\n",
      "2017-10-24 13:35:25,732 - INFO - step: 958,loss: 4.010 auc: 0.999 acc:0.992\n",
      "2017-10-24 13:36:23,768 - INFO - step: 959,loss: 2.682 auc: 1.000 acc:0.992\n",
      "2017-10-24 13:37:00,754 - INFO - step: 960,loss: 2.672 auc: 0.999 acc:0.985\n",
      "2017-10-24 13:37:00,759 - INFO - \n",
      "train_epoch: 47.000\n",
      "2017-10-24 13:37:00,761 - INFO - loss_total: 3.689 accuracy_total: 0.999\n",
      "2017-10-24 13:38:00,292 - INFO - step: 961,loss: 6.390 auc: 0.999 acc:0.984\n",
      "2017-10-24 13:38:58,781 - INFO - step: 962,loss: 5.012 auc: 0.999 acc:0.984\n",
      "2017-10-24 13:39:59,189 - INFO - step: 963,loss: 3.297 auc: 1.000 acc:0.992\n",
      "2017-10-24 13:40:55,842 - INFO - step: 964,loss: 3.789 auc: 1.000 acc:0.984\n",
      "2017-10-24 13:41:55,592 - INFO - step: 965,loss: 2.392 auc: 1.000 acc:1.000\n",
      "2017-10-24 13:42:53,751 - INFO - step: 966,loss: 3.048 auc: 1.000 acc:0.984\n",
      "2017-10-24 13:43:53,386 - INFO - step: 967,loss: 2.128 auc: 1.000 acc:1.000\n",
      "2017-10-24 13:44:55,134 - INFO - step: 968,loss: 1.686 auc: 1.000 acc:0.992\n",
      "2017-10-24 13:45:55,963 - INFO - step: 969,loss: 0.436 auc: 1.000 acc:1.000\n",
      "2017-10-24 13:46:49,945 - INFO - step: 970,loss: 13.821 auc: 0.997 acc:0.969\n",
      "2017-10-24 13:47:47,418 - INFO - step: 971,loss: 3.071 auc: 1.000 acc:0.992\n",
      "2017-10-24 13:48:44,915 - INFO - step: 972,loss: 4.996 auc: 1.000 acc:0.984\n",
      "2017-10-24 13:49:46,087 - INFO - step: 973,loss: 4.718 auc: 1.000 acc:0.992\n",
      "2017-10-24 13:50:43,798 - INFO - step: 974,loss: 5.464 auc: 0.999 acc:0.984\n",
      "2017-10-24 13:51:45,873 - INFO - step: 975,loss: 3.354 auc: 1.000 acc:0.984\n",
      "2017-10-24 13:52:43,639 - INFO - step: 976,loss: 2.936 auc: 1.000 acc:0.992\n",
      "2017-10-24 13:53:46,772 - INFO - step: 977,loss: 11.100 auc: 0.996 acc:0.961\n",
      "2017-10-24 13:54:46,622 - INFO - step: 978,loss: 14.727 auc: 1.000 acc:0.945\n",
      "2017-10-24 13:55:44,810 - INFO - step: 979,loss: 18.189 auc: 0.998 acc:0.953\n",
      "2017-10-24 13:56:25,687 - INFO - step: 980,loss: 2.313 auc: 1.000 acc:0.985\n",
      "2017-10-24 13:56:25,691 - INFO - \n",
      "train_epoch: 48.000\n",
      "2017-10-24 13:56:25,694 - INFO - loss_total: 5.643 accuracy_total: 0.999\n",
      "2017-10-24 13:56:25,695 - INFO - \n",
      "Evaluation: 48.000\n",
      "2017-10-24 13:57:02,813 - INFO - step: 980,loss: 51.620 auc: 0.984 acc:0.928\n",
      "2017-10-24 13:58:00,687 - INFO - step: 981,loss: 6.044 auc: 0.999 acc:0.984\n",
      "2017-10-24 13:58:59,481 - INFO - step: 982,loss: 4.119 auc: 1.000 acc:0.984\n",
      "2017-10-24 13:59:56,300 - INFO - step: 983,loss: 8.901 auc: 0.998 acc:0.977\n",
      "2017-10-24 14:00:54,645 - INFO - step: 984,loss: 2.371 auc: 1.000 acc:0.984\n",
      "2017-10-24 14:01:53,941 - INFO - step: 985,loss: 12.779 auc: 0.995 acc:0.977\n",
      "2017-10-24 14:02:54,658 - INFO - step: 986,loss: 1.587 auc: 1.000 acc:0.992\n",
      "2017-10-24 14:03:55,804 - INFO - step: 987,loss: 4.160 auc: 1.000 acc:0.992\n",
      "2017-10-24 14:04:52,968 - INFO - step: 988,loss: 0.762 auc: 1.000 acc:1.000\n",
      "2017-10-24 14:05:51,416 - INFO - step: 989,loss: 3.789 auc: 1.000 acc:0.992\n",
      "2017-10-24 14:06:52,942 - INFO - step: 990,loss: 8.904 auc: 0.998 acc:0.984\n",
      "2017-10-24 14:07:47,506 - INFO - step: 991,loss: 6.107 auc: 0.999 acc:0.984\n",
      "2017-10-24 14:08:44,531 - INFO - step: 992,loss: 3.689 auc: 0.999 acc:0.992\n",
      "2017-10-24 14:09:37,656 - INFO - step: 993,loss: 8.183 auc: 0.999 acc:0.969\n",
      "2017-10-24 14:10:32,189 - INFO - step: 994,loss: 2.103 auc: 1.000 acc:0.992\n",
      "2017-10-24 14:11:29,573 - INFO - step: 995,loss: 1.647 auc: 1.000 acc:1.000\n",
      "2017-10-24 14:12:26,996 - INFO - step: 996,loss: 9.470 auc: 0.999 acc:0.961\n",
      "2017-10-24 14:13:26,449 - INFO - step: 997,loss: 5.718 auc: 0.999 acc:0.977\n",
      "2017-10-24 14:14:25,064 - INFO - step: 998,loss: 2.692 auc: 1.000 acc:0.992\n",
      "2017-10-24 14:15:28,266 - INFO - step: 999,loss: 4.025 auc: 1.000 acc:0.984\n",
      "2017-10-24 14:16:07,244 - INFO - step: 1000,loss: 1.660 auc: 1.000 acc:0.985\n",
      "2017-10-24 14:16:07,248 - INFO - \n",
      "train_epoch: 49.000\n",
      "2017-10-24 14:16:07,252 - INFO - loss_total: 4.936 accuracy_total: 0.999\n",
      "2017-10-24 14:17:06,126 - INFO - step: 1001,loss: 2.016 auc: 1.000 acc:0.992\n",
      "2017-10-24 14:18:06,836 - INFO - step: 1002,loss: 5.795 auc: 1.000 acc:0.977\n",
      "2017-10-24 14:19:05,832 - INFO - step: 1003,loss: 2.918 auc: 1.000 acc:0.984\n",
      "2017-10-24 14:20:02,196 - INFO - step: 1004,loss: 10.411 auc: 0.997 acc:0.961\n",
      "2017-10-24 14:21:00,155 - INFO - step: 1005,loss: 2.796 auc: 1.000 acc:0.992\n",
      "2017-10-24 14:21:59,066 - INFO - step: 1006,loss: 6.747 auc: 0.999 acc:0.984\n",
      "2017-10-24 14:22:57,902 - INFO - step: 1007,loss: 0.743 auc: 1.000 acc:1.000\n",
      "2017-10-24 14:23:56,772 - INFO - step: 1008,loss: 6.770 auc: 1.000 acc:0.969\n",
      "2017-10-24 14:24:55,429 - INFO - step: 1009,loss: 10.828 auc: 0.998 acc:0.977\n",
      "2017-10-24 14:25:54,982 - INFO - step: 1010,loss: 8.189 auc: 0.999 acc:0.961\n",
      "2017-10-24 14:26:55,958 - INFO - step: 1011,loss: 3.033 auc: 1.000 acc:0.984\n",
      "2017-10-24 14:28:04,330 - INFO - step: 1012,loss: 3.519 auc: 1.000 acc:0.984\n",
      "2017-10-24 14:29:08,251 - INFO - step: 1013,loss: 8.518 auc: 0.998 acc:0.977\n",
      "2017-10-24 14:30:08,302 - INFO - step: 1014,loss: 6.209 auc: 1.000 acc:0.977\n",
      "2017-10-24 14:31:09,343 - INFO - step: 1015,loss: 0.526 auc: 1.000 acc:1.000\n",
      "2017-10-24 14:32:08,315 - INFO - step: 1016,loss: 1.877 auc: 1.000 acc:1.000\n",
      "2017-10-24 14:33:05,064 - INFO - step: 1017,loss: 5.384 auc: 1.000 acc:0.984\n",
      "2017-10-24 14:34:06,042 - INFO - step: 1018,loss: 5.815 auc: 1.000 acc:0.984\n",
      "2017-10-24 14:35:05,613 - INFO - step: 1019,loss: 4.637 auc: 1.000 acc:0.984\n",
      "2017-10-24 14:35:46,525 - INFO - step: 1020,loss: 0.094 auc: 1.000 acc:1.000\n",
      "2017-10-24 14:35:46,530 - INFO - \n",
      "train_epoch: 50.000\n",
      "2017-10-24 14:35:46,532 - INFO - loss_total: 4.841 accuracy_total: 0.999\n",
      "2017-10-24 14:35:46,534 - INFO - \n",
      "Evaluation: 50.000\n",
      "2017-10-24 14:36:22,254 - INFO - step: 1020,loss: 57.478 auc: 0.980 acc:0.928\n",
      "2017-10-24 14:37:18,375 - INFO - step: 1021,loss: 5.766 auc: 0.999 acc:0.992\n",
      "2017-10-24 14:38:16,573 - INFO - step: 1022,loss: 12.379 auc: 0.999 acc:0.969\n",
      "2017-10-24 14:39:15,295 - INFO - step: 1023,loss: 5.296 auc: 0.999 acc:0.977\n",
      "2017-10-24 14:40:12,529 - INFO - step: 1024,loss: 2.665 auc: 1.000 acc:1.000\n",
      "2017-10-24 14:41:11,326 - INFO - step: 1025,loss: 2.799 auc: 1.000 acc:0.992\n",
      "2017-10-24 14:42:12,605 - INFO - step: 1026,loss: 3.997 auc: 1.000 acc:0.984\n",
      "2017-10-24 14:43:10,596 - INFO - step: 1027,loss: 2.748 auc: 0.999 acc:0.992\n",
      "2017-10-24 14:44:09,558 - INFO - step: 1028,loss: 1.701 auc: 1.000 acc:0.992\n",
      "2017-10-24 14:45:10,618 - INFO - step: 1029,loss: 3.040 auc: 1.000 acc:0.992\n",
      "2017-10-24 14:46:08,996 - INFO - step: 1030,loss: 2.665 auc: 1.000 acc:0.992\n",
      "2017-10-24 14:47:07,192 - INFO - step: 1031,loss: 2.672 auc: 1.000 acc:0.992\n",
      "2017-10-24 14:48:06,672 - INFO - step: 1032,loss: 4.098 auc: 1.000 acc:0.992\n",
      "2017-10-24 14:49:06,108 - INFO - step: 1033,loss: 2.442 auc: 1.000 acc:0.992\n",
      "2017-10-24 14:50:06,079 - INFO - step: 1034,loss: 4.479 auc: 0.999 acc:0.992\n",
      "2017-10-24 14:51:06,186 - INFO - step: 1035,loss: 3.046 auc: 1.000 acc:0.984\n",
      "2017-10-24 14:52:04,959 - INFO - step: 1036,loss: 5.753 auc: 0.999 acc:0.969\n",
      "2017-10-24 14:53:03,322 - INFO - step: 1037,loss: 1.386 auc: 1.000 acc:1.000\n",
      "2017-10-24 14:54:05,008 - INFO - step: 1038,loss: 7.483 auc: 0.998 acc:0.984\n",
      "2017-10-24 14:55:03,409 - INFO - step: 1039,loss: 2.752 auc: 1.000 acc:0.992\n",
      "2017-10-24 14:55:40,563 - INFO - step: 1040,loss: 6.254 auc: 0.996 acc:0.985\n",
      "2017-10-24 14:55:40,569 - INFO - \n",
      "train_epoch: 51.000\n",
      "2017-10-24 14:55:40,571 - INFO - loss_total: 4.171 accuracy_total: 0.999\n",
      "2017-10-24 14:56:38,974 - INFO - step: 1041,loss: 9.745 auc: 0.999 acc:0.977\n",
      "2017-10-24 14:57:36,632 - INFO - step: 1042,loss: 4.817 auc: 1.000 acc:0.984\n",
      "2017-10-24 14:58:34,782 - INFO - step: 1043,loss: 2.792 auc: 1.000 acc:0.984\n",
      "2017-10-24 14:59:32,545 - INFO - step: 1044,loss: 4.134 auc: 1.000 acc:0.984\n",
      "2017-10-24 15:00:31,048 - INFO - step: 1045,loss: 3.252 auc: 1.000 acc:0.984\n",
      "2017-10-24 15:01:28,439 - INFO - step: 1046,loss: 2.089 auc: 1.000 acc:0.992\n",
      "2017-10-24 15:02:26,463 - INFO - step: 1047,loss: 5.025 auc: 0.999 acc:0.984\n",
      "2017-10-24 15:03:22,939 - INFO - step: 1048,loss: 3.515 auc: 1.000 acc:0.992\n",
      "2017-10-24 15:04:20,757 - INFO - step: 1049,loss: 3.922 auc: 0.999 acc:0.992\n",
      "2017-10-24 15:05:18,621 - INFO - step: 1050,loss: 3.539 auc: 1.000 acc:0.992\n",
      "2017-10-24 15:16:08,341 - INFO - step: 1061,loss: 8.825 auc: 0.999 acc:0.961\n",
      "2017-10-24 15:17:07,180 - INFO - step: 1062,loss: 4.891 auc: 0.999 acc:0.984\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
