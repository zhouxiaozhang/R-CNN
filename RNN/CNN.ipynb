{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class malware_RNN(object):\n",
    "    def __init__(self,config):\n",
    "        self._config=config\n",
    "        self._data_type = tf.float32\n",
    "        self.input_x=tf.placeholder(tf.int32,[None,self._config.sequence_length],name=\"input_x\")    \n",
    "        self.input_y=tf.placeholder(tf.float32,[None,self._config.num_classes],name=\"input_y\")\n",
    "        self.dropout_keep_prob=tf.placeholder(tf.float32,name=\"dropout_keep_prob\")\n",
    "        self.real_len = tf.placeholder(tf.int32, [None], name='real_len')\n",
    "        self.batch_size = tf.placeholder(tf.int32, [],name=\"batch_size\")\n",
    "        l2_loss=tf.constant(0.0)\n",
    "        #embedding,可以尝试用one-hot\n",
    "        with tf.device(\"/cpu:0\"),tf.name_scope(\"embedding\"):\n",
    "            embedding = tf.get_variable(name=\"embedding\",shape=[self._config.vocab_size,self._config.embedding_dim],initializer=tf.random_uniform_initializer(-1.0,1.0),dtype=self._data_type)\n",
    "            inputs = tf.nn.embedding_lookup(embedding, self.input_x)\n",
    "        with tf.variable_scope(\"LSTM\"): \n",
    "            inputs = tf.nn.dropout(inputs, self.dropout_keep_prob)\n",
    "            cell = tf.nn.rnn_cell.LSTMCell(self._config.HIDDEN_SIZE,forget_bias=self._config.CELL_FORGET_BIAS,state_is_tuple=True)\n",
    "            cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=self.dropout_keep_prob)\n",
    "            cells = tf.nn.rnn_cell.MultiRNNCell([cell] * self._config.LAYERS_NUM,state_is_tuple=True)\n",
    "            self._initial_state = cells.zero_state( self.batch_size,self._data_type)\n",
    "            outputs, states = tf.nn.dynamic_rnn(cells,inputs,initial_state=self._initial_state,parallel_iterations=1,dtype=tf.float32,\n",
    "            sequence_length=self.real_len,time_major=False)\n",
    "        with tf.variable_scope(\"max-pooling\"): \n",
    "            #masking the output vector,batch,hidden\n",
    "            index = tf.range(0, self.batch_size) * self._config.sequence_length + (self.real_len - 1)\n",
    "            flat = tf.reshape(outputs, [-1, self._config.HIDDEN_SIZE])\n",
    "            last=tf.gather(flat, index)\n",
    "            \n",
    "            #max_pooling batch_size,2*hidden_size\n",
    "            trans=tf.transpose(outputs, perm=[0, 2, 1])\n",
    "            max_pool=tf.reduce_max(trans,axis=2)\n",
    "            last_output=last\n",
    "            max_pooling = tf.concat(1, [max_pool, last_output])\n",
    "        with tf.variable_scope(\"classifier\"): \n",
    "            #init\n",
    "            init_w=tf.truncated_normal_initializer(stddev = 0.1)\n",
    "            init_b=tf.constant_initializer(value=0.1, dtype=tf.float32)\n",
    "            softmax_w = tf.get_variable(\"softmax_w\",[self._config.HIDDEN_SIZE*2, 1], dtype=self._data_type,initializer=init_w )\n",
    "            softmax_b = tf.get_variable( \"softmax_b\",[1], dtype=self._data_type,initializer=init_b)\n",
    "            l2_loss += tf.nn.l2_loss(softmax_w)\n",
    "            l2_loss += tf.nn.l2_loss(softmax_b)\n",
    "            l2_loss += tf.nn.l2_loss(embedding)\n",
    "            log = tf.matmul(max_pooling, softmax_w) + softmax_b \n",
    "        with tf.variable_scope(\"loss\"):    \n",
    "            cross_entropy =tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(log, self.input_y))\n",
    "            self.y_pre=tf.sigmoid(log,name=\"y_pred\") \n",
    "        #accuracy=roc_auc_score (self._label, y)\n",
    "            #self.y_pre=self.y_pre\n",
    "            self.loss=cross_entropy+self._config.l2_reg_lamda*l2_loss\n",
    "            self.final_state = states\n",
    "\n",
    "               \n",
    "    def batch_iter(x_data,y_data, batch_size, num_epochs, shuffle=True):\n",
    "        x_data=np.array(x_data)\n",
    "        y_data=np.array(y_data)\n",
    "        data_size =len(x_data)\n",
    "        num_batches_per_epoch = int(len(x_data)-1/batch_size) + 1\n",
    "        for epoch in range(num_epochs):\n",
    "            if shuffle:\n",
    "                shuffle_indices=np.random.permutation(data_size)\n",
    "                shuffled_data_x=x_data[shuffle_indices]\n",
    "                shuffled_data_y=y_data[shuffle_indices]\n",
    "                \n",
    "            else:\n",
    "                shuffled_data_x=x_data\n",
    "                shuffled_data_y=y_data\n",
    "            for batch_num in range(num_batches_per_epoch):\n",
    "                start_index=batch_num*batch_size\n",
    "                end_index=min((batch_num+1)*batch_size,data_size)\n",
    "                yield shuffled_data_x[start_index:end_index],shuffled_data_y[start_index:end_index]\n",
    "                \n",
    "        \n",
    "       \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "                \n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
