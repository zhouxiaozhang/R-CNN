{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import logging\n",
    "import pickle\n",
    "import CNN\n",
    "import math\n",
    "from CNN import malware_CNN\n",
    "from config import get_config\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.contrib import learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Data parameters\n",
    "tf.flags.DEFINE_string(\"data_path\", \"/home/zx/gram3_gain1000/data_process_data_original_value_train\",\"data_path\")\n",
    "tf.flags.DEFINE_string(\"save_path\" , \"/home/zx/cuckoo_1000/session_save/\",\"Model output directory.\")\n",
    "tf.flags.DEFINE_string(\"board_path\", \"/home/zx/cuckoo_1000/tensor_board/\",\"Tensor board output directory.\")\n",
    "tf.flags.DEFINE_string(\"log_path\", \"/home/zx/cuckoo_1000/log.log\", \"log output path.\")\n",
    "#Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\",True,\"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\",False,\"Log placement of ops on devices\")\n",
    "#train Parameters\n",
    "tf.flags.DEFINE_string(\"evaluate_every\",2,\"Evaluate model on dev set after this many epochs\")\n",
    "\n",
    "FLAGS =tf.flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#log\n",
    "LOG = None\n",
    "def init_logger():\n",
    "    global LOG\n",
    "\n",
    "    LOG = logging.getLogger('seq')\n",
    "    LOG.setLevel(logging.DEBUG)\n",
    "\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "    file_handler = logging.FileHandler(filename=FLAGS.log_path, encoding=\"utf-8\")\n",
    "    file_handler.setLevel(logging.DEBUG)\n",
    "    file_handler.setFormatter(formatter)\n",
    "    LOG.addHandler(file_handler)\n",
    "\n",
    "    stream_handler = logging.StreamHandler(sys.stdout)\n",
    "    stream_handler.setLevel(logging.INFO)\n",
    "    stream_handler.setFormatter(formatter)\n",
    "    LOG.addHandler(stream_handler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data(file_name):\n",
    "    with open(file_name, \"rb\") as f:\n",
    "        raw_x, raw_y = pickle.load(f)\n",
    "        x_train,x_dev,y_train,y_dev=train_test_split(raw_x, raw_y,test_size=0.1)\n",
    "        return  x_train,x_dev,y_train,y_dev\n",
    "    \n",
    "def batch_iter(x_data,y_data, batch_size, num_epochs, shuffle=True):\n",
    "    x_data=np.array(x_data)\n",
    "    y_data=np.array(y_data)\n",
    "    data_size =len(x_data)\n",
    "    num_batches_per_epoch = int((data_size-1) / batch_size)+1\n",
    "    #for epoch in range(num_epochs):\n",
    "    if shuffle:\n",
    "        shuffle_indices=np.random.permutation(data_size)\n",
    "        shuffled_data_x=x_data[shuffle_indices]\n",
    "        shuffled_data_y=y_data[shuffle_indices]\n",
    "                \n",
    "    else:\n",
    "        shuffled_data_x=x_data\n",
    "        shuffled_data_y=y_data\n",
    "    for batch_num in range(num_batches_per_epoch):\n",
    "        start_index=batch_num*batch_size\n",
    "        end_index=min((batch_num+1)*batch_size,data_size)\n",
    "        yield shuffled_data_x[start_index:end_index],shuffled_data_y[start_index:end_index]\n",
    "                    \n",
    "def run_epoch(\n",
    "        session,\n",
    "        x_batch,\n",
    "        y_batch,\n",
    "        dropout,\n",
    "        model,\n",
    "        global_step,\n",
    "        summary_op,\n",
    "        eval_op=None,\n",
    "        verbose=False,\n",
    "       \n",
    "):\n",
    "    #feed,give\n",
    "    feed_dict={\n",
    "        model.input_x:x_batch,\n",
    "        model.input_y:y_batch,\n",
    "        model.dropout_keep_prob:dropout\n",
    "              }\n",
    "    #out\n",
    "    fetches = {\n",
    "        \"loss\": model.loss,\n",
    "        \"accuracy\": model.accuracy,\n",
    "        \"global_step\":global_step,\n",
    "        \"summary_op\" :summary_op\n",
    "    }\n",
    "    if eval_op is not None:\n",
    "        fetches[\"eval_op\"] = eval_op\n",
    "    \n",
    "    fetches_ret = session.run(fetches, feed_dict)\n",
    "    if verbose:\n",
    "        LOG.info( \"step: %d,loss: %.3f accuracy: %.3f\" \n",
    "                  % ( fetches_ret[\"global_step\"], fetches_ret[\"loss\"],fetches_ret[\"accuracy\"])\n",
    "                 )\n",
    "    return fetches_ret[\"accuracy\"], fetches_ret[\"loss\"],fetches_ret[\"summary_op\"]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    \n",
    "    #training \n",
    "    x_train,x_dev,y_train,y_dev=load_data(FLAGS.data_path)\n",
    "    init_logger()\n",
    "    train_config=get_config(\"train\")\n",
    "    valid_config=get_config(\"valid\")\n",
    "    with tf.Graph().as_default():\n",
    "        session_conf=tf.ConfigProto(allow_soft_placement=FLAGS.allow_soft_placement,log_device_placement=FLAGS.log_device_placement)\n",
    "        sess=tf.Session(config=session_conf)\n",
    "        with sess.as_default():\n",
    "            cnn=malware_CNN(config=train_config)\n",
    "            global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "            optimizer=tf.train.RMSPropOptimizer(train_config.learning_rate,decay=0.9)\n",
    "            grads_and_vars=optimizer.compute_gradients(cnn.loss)\n",
    "            train_op=optimizer.apply_gradients(grads_and_vars,global_step)\n",
    "            \n",
    "            #keep track of gradient values and sparsity\n",
    "            grad_summaries=[]\n",
    "            for g,v in grads_and_vars:\n",
    "                if g is not None:\n",
    "                    grad_hist_summary=tf.summary.histogram(\"{}/grad/hist\".format(v.name),g)\n",
    "                    sparsity_summary=tf.summary.scalar(\"{}/grad/sparsity\".format(v.name),tf.nn.zero_fraction(g))\n",
    "                    grad_summaries.append(grad_hist_summary)\n",
    "                    grad_summaries.append(sparsity_summary)\n",
    "            grad_summaries_merged=tf.summary.merge(grad_summaries)\n",
    "            #Summaries for loss and acc\n",
    "            loss_summary=tf.summary.scalar(\"loss\",cnn.loss)\n",
    "            acc_summary=tf.summary.scalar(\"accuracy\",cnn.accuracy)\n",
    "            \n",
    "            #Train summaries\n",
    "            train_summary_op=tf.summary.merge([grad_summaries_merged,loss_summary,acc_summary])\n",
    "            train_summary_dir=os.path.join(FLAGS.board_path,\"summaries\",\"train\")\n",
    "            train_summary_write=tf.summary.FileWriter(train_summary_dir,sess.graph)\n",
    "            #Dev summaries\n",
    "            dev_summary_op=tf.summary.merge([loss_summary,acc_summary])\n",
    "            dev_summary_dir=os.path.join(FLAGS.board_path,\"summaries\",\"dev\")\n",
    "            dev_summary_write=tf.summary.FileWriter(dev_summary_dir,sess.graph)\n",
    "            \n",
    "            \n",
    "            #init\n",
    "            checkpoint_dir=os.path.join(FLAGS.save_path,\"checkpoints\")\n",
    "            if not os.path.exists(checkpoint_dir):\n",
    "                os.makedirs(checkpoint_dir)\n",
    "            checkpoint_prefix=os.path.join(checkpoint_dir,\"model\")\n",
    "            \n",
    "            saver=tf.train.Saver(tf.all_variables())\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            #generate batches\n",
    "            \n",
    "            best_acc,best_at_step=0,0\n",
    "            #training loop\n",
    "            accuracy_total=0.0\n",
    "            loss_total=0.0\n",
    "            for epoch_id in range(0, train_config.num_epochs):\n",
    "                accuracy_total=accuracy_total/ (int((len(x_train)-1) / train_config.batch_size)+1)\n",
    "                loss_total=loss_total/(int((len(x_train)-1) / train_config.batch_size)+1)\n",
    "                LOG.info(\"\\ntrain_epoch:\")\n",
    "                LOG.info( \"loss_total: %.3f accuracy_total: %.3f\" \n",
    "                  % ( loss_total,accuracy_total)\n",
    "                 )\n",
    "                accuracy_total=0.0\n",
    "                loss_total=0.0\n",
    "                batches=batch_iter(x_train,y_train,train_config.batch_size,train_config.num_epochs)\n",
    "                for batch_x,batch_y in batches:\n",
    "                    acc_train,loss_train,summaries=run_epoch( sess,batch_x,batch_y,dropout=train_config.dropout_keep_prob,model=cnn, eval_op=train_op,verbose=True, global_step= global_step,summary_op=train_summary_op)\n",
    "                    accuracy_total+=acc_train\n",
    "                    loss_total+=loss_train\n",
    "                    current_step=tf.train.global_step(sess,global_step)\n",
    "                    train_summary_write.add_summary(summaries,epoch_id)\n",
    "                if epoch_id%FLAGS.evaluate_every==0:\n",
    "                    LOG.info(\"\\nEvaluation:\")\n",
    "                    acc,loss,summaries=run_epoch( sess,x_dev,y_dev,dropout=valid_config.dropout_keep_prob,model=cnn,verbose=True, global_step= global_step,summary_op=dev_summary_op) \n",
    "                    dev_summary_write.add_summary(summaries,epoch_id)\n",
    "                    if acc>=best_acc:\n",
    "                        best_acc,best_at_step=acc,epoch_id\n",
    "                        path=saver.save(sess,checkpoint_prefix,global_step=global_step)\n",
    "                        LOG.info(\"Saving model to %s at epoch %d.\" % (path,epoch_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(_):\n",
    "    if not FLAGS.data_path:\n",
    "        raise ValueError(\"Must set --data_path to data file\")\n",
    "    train()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name embedded:0/grad/hist is illegal; using embedded_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedded:0/grad/sparsity is illegal; using embedded_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/W:0/grad/hist is illegal; using output/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/W:0/grad/sparsity is illegal; using output/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "WARNING:tensorflow:From <ipython-input-5-c8bb7e7418c0>:47: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n",
      "2017-08-13 22:45:36,504 - INFO - \train_epoch:\n",
      "2017-08-13 22:45:36,505 - INFO - loss_total: 0.000 accuracy_total: 0.000\n",
      "2017-08-13 22:45:40,763 - INFO - step: 1,loss: 1.620 accuracy: 0.500\n",
      "2017-08-13 22:45:40,828 - INFO - step: 2,loss: 1.512 accuracy: 0.625\n",
      "2017-08-13 22:45:40,906 - INFO - step: 3,loss: 1.953 accuracy: 0.500\n",
      "2017-08-13 22:45:40,963 - INFO - step: 4,loss: 1.990 accuracy: 0.391\n",
      "2017-08-13 22:45:41,020 - INFO - step: 5,loss: 2.370 accuracy: 0.484\n",
      "2017-08-13 22:45:41,104 - INFO - step: 6,loss: 2.276 accuracy: 0.484\n",
      "2017-08-13 22:45:41,162 - INFO - step: 7,loss: 2.101 accuracy: 0.422\n",
      "2017-08-13 22:45:41,226 - INFO - step: 8,loss: 1.449 accuracy: 0.594\n",
      "2017-08-13 22:45:41,289 - INFO - step: 9,loss: 1.690 accuracy: 0.547\n",
      "2017-08-13 22:45:41,347 - INFO - step: 10,loss: 1.762 accuracy: 0.484\n",
      "2017-08-13 22:45:41,430 - INFO - step: 11,loss: 1.956 accuracy: 0.562\n",
      "2017-08-13 22:45:41,486 - INFO - step: 12,loss: 1.754 accuracy: 0.453\n",
      "2017-08-13 22:45:41,569 - INFO - step: 13,loss: 1.303 accuracy: 0.641\n",
      "2017-08-13 22:45:41,626 - INFO - step: 14,loss: 1.640 accuracy: 0.516\n",
      "2017-08-13 22:45:41,690 - INFO - step: 15,loss: 1.802 accuracy: 0.531\n",
      "2017-08-13 22:45:41,753 - INFO - step: 16,loss: 1.895 accuracy: 0.516\n",
      "2017-08-13 22:45:41,810 - INFO - step: 17,loss: 1.848 accuracy: 0.516\n",
      "2017-08-13 22:45:41,871 - INFO - step: 18,loss: 2.159 accuracy: 0.438\n",
      "2017-08-13 22:45:41,953 - INFO - step: 19,loss: 1.910 accuracy: 0.453\n",
      "2017-08-13 22:45:42,033 - INFO - step: 20,loss: 2.039 accuracy: 0.438\n",
      "2017-08-13 22:45:42,096 - INFO - step: 21,loss: 1.588 accuracy: 0.500\n",
      "2017-08-13 22:45:42,157 - INFO - step: 22,loss: 2.192 accuracy: 0.422\n",
      "2017-08-13 22:45:42,213 - INFO - step: 23,loss: 1.726 accuracy: 0.469\n",
      "2017-08-13 22:45:42,295 - INFO - step: 24,loss: 2.151 accuracy: 0.438\n",
      "2017-08-13 22:45:42,379 - INFO - step: 25,loss: 1.693 accuracy: 0.484\n",
      "2017-08-13 22:45:42,460 - INFO - step: 26,loss: 1.664 accuracy: 0.594\n",
      "2017-08-13 22:45:42,516 - INFO - step: 27,loss: 1.957 accuracy: 0.453\n",
      "2017-08-13 22:45:42,574 - INFO - step: 28,loss: 1.696 accuracy: 0.500\n",
      "2017-08-13 22:45:42,655 - INFO - step: 29,loss: 1.786 accuracy: 0.500\n",
      "2017-08-13 22:45:42,719 - INFO - step: 30,loss: 1.628 accuracy: 0.516\n",
      "2017-08-13 22:45:42,782 - INFO - step: 31,loss: 1.584 accuracy: 0.594\n",
      "2017-08-13 22:45:42,838 - INFO - step: 32,loss: 1.845 accuracy: 0.469\n",
      "2017-08-13 22:45:42,905 - INFO - step: 33,loss: 1.687 accuracy: 0.453\n",
      "2017-08-13 22:45:42,967 - INFO - step: 34,loss: 1.151 accuracy: 0.594\n",
      "2017-08-13 22:45:43,025 - INFO - step: 35,loss: 1.566 accuracy: 0.547\n",
      "2017-08-13 22:45:43,476 - INFO - step: 36,loss: 1.721 accuracy: 0.429\n",
      "2017-08-13 22:45:43,482 - INFO - \n",
      "Evaluation:\n",
      "2017-08-13 22:45:43,791 - INFO - step: 36,loss: 0.590 accuracy: 0.688\n",
      "2017-08-13 22:45:43,882 - INFO - Saving model to /home/zx/cuckoo_1000/session_save/checkpoints/model-36 at step 0.\n",
      "2017-08-13 22:45:43,884 - INFO - \train_epoch:\n",
      "2017-08-13 22:45:43,885 - INFO - loss_total: 1.796 accuracy_total: 0.501\n",
      "2017-08-13 22:45:43,975 - INFO - step: 37,loss: 1.172 accuracy: 0.688\n",
      "2017-08-13 22:45:44,059 - INFO - step: 38,loss: 1.319 accuracy: 0.547\n",
      "2017-08-13 22:45:44,121 - INFO - step: 39,loss: 1.244 accuracy: 0.547\n",
      "2017-08-13 22:45:44,203 - INFO - step: 40,loss: 1.801 accuracy: 0.438\n",
      "2017-08-13 22:45:44,285 - INFO - step: 41,loss: 1.740 accuracy: 0.500\n",
      "2017-08-13 22:45:44,347 - INFO - step: 42,loss: 1.095 accuracy: 0.641\n",
      "2017-08-13 22:45:44,430 - INFO - step: 43,loss: 1.397 accuracy: 0.500\n",
      "2017-08-13 22:45:44,510 - INFO - step: 44,loss: 1.438 accuracy: 0.531\n",
      "2017-08-13 22:45:44,590 - INFO - step: 45,loss: 1.112 accuracy: 0.641\n",
      "2017-08-13 22:45:44,652 - INFO - step: 46,loss: 1.376 accuracy: 0.594\n",
      "2017-08-13 22:45:44,715 - INFO - step: 47,loss: 1.534 accuracy: 0.594\n",
      "2017-08-13 22:45:44,777 - INFO - step: 48,loss: 1.473 accuracy: 0.547\n",
      "2017-08-13 22:45:44,844 - INFO - step: 49,loss: 1.315 accuracy: 0.547\n",
      "2017-08-13 22:45:44,907 - INFO - step: 50,loss: 1.279 accuracy: 0.578\n",
      "2017-08-13 22:45:44,969 - INFO - step: 51,loss: 1.353 accuracy: 0.578\n",
      "2017-08-13 22:45:45,027 - INFO - step: 52,loss: 1.392 accuracy: 0.500\n",
      "2017-08-13 22:45:45,110 - INFO - step: 53,loss: 1.063 accuracy: 0.672\n",
      "2017-08-13 22:45:45,175 - INFO - step: 54,loss: 1.168 accuracy: 0.641\n",
      "2017-08-13 22:45:45,258 - INFO - step: 55,loss: 1.186 accuracy: 0.641\n",
      "2017-08-13 22:45:45,336 - INFO - step: 56,loss: 1.249 accuracy: 0.578\n",
      "2017-08-13 22:45:45,397 - INFO - step: 57,loss: 1.427 accuracy: 0.484\n",
      "2017-08-13 22:45:45,479 - INFO - step: 58,loss: 1.668 accuracy: 0.438\n",
      "2017-08-13 22:45:45,543 - INFO - step: 59,loss: 1.071 accuracy: 0.594\n",
      "2017-08-13 22:45:45,606 - INFO - step: 60,loss: 2.113 accuracy: 0.469\n",
      "2017-08-13 22:45:45,669 - INFO - step: 61,loss: 1.795 accuracy: 0.531\n",
      "2017-08-13 22:45:45,732 - INFO - step: 62,loss: 1.394 accuracy: 0.484\n",
      "2017-08-13 22:45:45,796 - INFO - step: 63,loss: 1.774 accuracy: 0.438\n",
      "2017-08-13 22:45:45,877 - INFO - step: 64,loss: 1.192 accuracy: 0.609\n",
      "2017-08-13 22:45:45,940 - INFO - step: 65,loss: 1.253 accuracy: 0.641\n",
      "2017-08-13 22:45:46,022 - INFO - step: 66,loss: 0.750 accuracy: 0.641\n",
      "2017-08-13 22:45:46,085 - INFO - step: 67,loss: 1.084 accuracy: 0.547\n",
      "2017-08-13 22:45:46,149 - INFO - step: 68,loss: 1.128 accuracy: 0.594\n",
      "2017-08-13 22:45:46,207 - INFO - step: 69,loss: 1.552 accuracy: 0.500\n",
      "2017-08-13 22:45:46,269 - INFO - step: 70,loss: 0.965 accuracy: 0.594\n",
      "2017-08-13 22:45:46,331 - INFO - step: 71,loss: 0.880 accuracy: 0.688\n",
      "2017-08-13 22:45:46,355 - INFO - step: 72,loss: 0.724 accuracy: 0.714\n",
      "2017-08-13 22:45:46,357 - INFO - \train_epoch:\n",
      "2017-08-13 22:45:46,359 - INFO - loss_total: 1.319 accuracy_total: 0.568\n",
      "2017-08-13 22:45:46,463 - INFO - step: 73,loss: 1.032 accuracy: 0.609\n",
      "2017-08-13 22:45:46,524 - INFO - step: 74,loss: 0.688 accuracy: 0.781\n",
      "2017-08-13 22:45:46,602 - INFO - step: 75,loss: 1.204 accuracy: 0.562\n",
      "2017-08-13 22:45:46,664 - INFO - step: 76,loss: 1.124 accuracy: 0.578\n",
      "2017-08-13 22:45:46,728 - INFO - step: 77,loss: 1.127 accuracy: 0.594\n",
      "2017-08-13 22:45:46,791 - INFO - step: 78,loss: 1.143 accuracy: 0.594\n",
      "2017-08-13 22:45:46,856 - INFO - step: 79,loss: 1.049 accuracy: 0.594\n",
      "2017-08-13 22:45:46,919 - INFO - step: 80,loss: 0.783 accuracy: 0.688\n",
      "2017-08-13 22:45:46,982 - INFO - step: 81,loss: 1.248 accuracy: 0.562\n",
      "2017-08-13 22:45:47,040 - INFO - step: 82,loss: 0.999 accuracy: 0.578\n",
      "2017-08-13 22:45:47,107 - INFO - step: 83,loss: 0.671 accuracy: 0.688\n",
      "2017-08-13 22:45:47,170 - INFO - step: 84,loss: 0.861 accuracy: 0.703\n",
      "2017-08-13 22:45:47,253 - INFO - step: 85,loss: 0.753 accuracy: 0.750\n",
      "2017-08-13 22:45:47,310 - INFO - step: 86,loss: 0.867 accuracy: 0.656\n",
      "2017-08-13 22:45:47,372 - INFO - step: 87,loss: 1.265 accuracy: 0.578\n",
      "2017-08-13 22:45:47,434 - INFO - step: 88,loss: 1.279 accuracy: 0.531\n",
      "2017-08-13 22:45:47,495 - INFO - step: 89,loss: 1.043 accuracy: 0.625\n",
      "2017-08-13 22:45:47,556 - INFO - step: 90,loss: 1.155 accuracy: 0.578\n",
      "2017-08-13 22:45:47,619 - INFO - step: 91,loss: 0.933 accuracy: 0.688\n",
      "2017-08-13 22:45:47,682 - INFO - step: 92,loss: 0.661 accuracy: 0.688\n",
      "2017-08-13 22:45:47,746 - INFO - step: 93,loss: 0.903 accuracy: 0.672\n",
      "2017-08-13 22:45:47,808 - INFO - step: 94,loss: 0.911 accuracy: 0.703\n",
      "2017-08-13 22:45:47,871 - INFO - step: 95,loss: 1.114 accuracy: 0.641\n",
      "2017-08-13 22:45:47,936 - INFO - step: 96,loss: 0.838 accuracy: 0.672\n",
      "2017-08-13 22:45:47,999 - INFO - step: 97,loss: 0.834 accuracy: 0.688\n",
      "2017-08-13 22:45:48,062 - INFO - step: 98,loss: 1.071 accuracy: 0.656\n",
      "2017-08-13 22:45:48,125 - INFO - step: 99,loss: 0.606 accuracy: 0.781\n",
      "2017-08-13 22:45:48,207 - INFO - step: 100,loss: 0.943 accuracy: 0.578\n",
      "2017-08-13 22:45:48,269 - INFO - step: 101,loss: 0.901 accuracy: 0.594\n",
      "2017-08-13 22:45:48,333 - INFO - step: 102,loss: 0.825 accuracy: 0.703\n",
      "2017-08-13 22:45:48,396 - INFO - step: 103,loss: 0.748 accuracy: 0.688\n",
      "2017-08-13 22:45:48,459 - INFO - step: 104,loss: 0.704 accuracy: 0.719\n",
      "2017-08-13 22:45:48,516 - INFO - step: 105,loss: 0.737 accuracy: 0.656\n",
      "2017-08-13 22:45:48,578 - INFO - step: 106,loss: 0.548 accuracy: 0.781\n",
      "2017-08-13 22:45:48,659 - INFO - step: 107,loss: 0.610 accuracy: 0.703\n",
      "2017-08-13 22:45:48,683 - INFO - step: 108,loss: 1.360 accuracy: 0.571\n",
      "2017-08-13 22:45:48,685 - INFO - \n",
      "Evaluation:\n",
      "2017-08-13 22:45:48,824 - INFO - step: 108,loss: 0.338 accuracy: 0.864\n",
      "2017-08-13 22:45:48,889 - INFO - Saving model to /home/zx/cuckoo_1000/session_save/checkpoints/model-108 at step 2.\n",
      "2017-08-13 22:45:48,891 - INFO - \train_epoch:\n",
      "2017-08-13 22:45:48,892 - INFO - loss_total: 0.932 accuracy_total: 0.651\n",
      "2017-08-13 22:45:48,959 - INFO - step: 109,loss: 0.832 accuracy: 0.734\n",
      "2017-08-13 22:45:49,023 - INFO - step: 110,loss: 0.906 accuracy: 0.688\n",
      "2017-08-13 22:45:49,086 - INFO - step: 111,loss: 0.578 accuracy: 0.750\n",
      "2017-08-13 22:45:49,149 - INFO - step: 112,loss: 0.562 accuracy: 0.750\n",
      "2017-08-13 22:45:49,212 - INFO - step: 113,loss: 0.718 accuracy: 0.703\n",
      "2017-08-13 22:45:49,296 - INFO - step: 114,loss: 0.607 accuracy: 0.719\n",
      "2017-08-13 22:45:49,357 - INFO - step: 115,loss: 0.836 accuracy: 0.703\n",
      "2017-08-13 22:45:49,439 - INFO - step: 116,loss: 0.705 accuracy: 0.703\n",
      "2017-08-13 22:45:49,502 - INFO - step: 117,loss: 0.916 accuracy: 0.750\n",
      "2017-08-13 22:45:49,565 - INFO - step: 118,loss: 0.675 accuracy: 0.688\n",
      "2017-08-13 22:45:49,627 - INFO - step: 119,loss: 0.673 accuracy: 0.672\n",
      "2017-08-13 22:45:49,690 - INFO - step: 120,loss: 0.732 accuracy: 0.625\n",
      "2017-08-13 22:45:49,753 - INFO - step: 121,loss: 0.786 accuracy: 0.641\n",
      "2017-08-13 22:45:49,816 - INFO - step: 122,loss: 0.625 accuracy: 0.719\n",
      "2017-08-13 22:45:49,879 - INFO - step: 123,loss: 0.754 accuracy: 0.703\n",
      "2017-08-13 22:45:49,941 - INFO - step: 124,loss: 0.712 accuracy: 0.766\n",
      "2017-08-13 22:45:50,005 - INFO - step: 125,loss: 0.739 accuracy: 0.719\n",
      "2017-08-13 22:45:50,068 - INFO - step: 126,loss: 0.515 accuracy: 0.844\n",
      "2017-08-13 22:45:50,129 - INFO - step: 127,loss: 0.653 accuracy: 0.734\n",
      "2017-08-13 22:45:50,194 - INFO - step: 128,loss: 0.594 accuracy: 0.781\n",
      "2017-08-13 22:45:50,255 - INFO - step: 129,loss: 0.547 accuracy: 0.719\n",
      "2017-08-13 22:45:50,318 - INFO - step: 130,loss: 0.747 accuracy: 0.656\n",
      "2017-08-13 22:45:50,380 - INFO - step: 131,loss: 0.590 accuracy: 0.719\n",
      "2017-08-13 22:45:50,444 - INFO - step: 132,loss: 0.784 accuracy: 0.719\n",
      "2017-08-13 22:45:50,506 - INFO - step: 133,loss: 0.538 accuracy: 0.719\n",
      "2017-08-13 22:45:50,570 - INFO - step: 134,loss: 0.630 accuracy: 0.766\n",
      "2017-08-13 22:45:50,634 - INFO - step: 135,loss: 0.563 accuracy: 0.750\n",
      "2017-08-13 22:45:50,699 - INFO - step: 136,loss: 0.803 accuracy: 0.656\n",
      "2017-08-13 22:45:50,761 - INFO - step: 137,loss: 0.525 accuracy: 0.750\n",
      "2017-08-13 22:45:50,823 - INFO - step: 138,loss: 0.662 accuracy: 0.719\n",
      "2017-08-13 22:45:50,886 - INFO - step: 139,loss: 0.638 accuracy: 0.750\n",
      "2017-08-13 22:45:50,950 - INFO - step: 140,loss: 0.535 accuracy: 0.781\n",
      "2017-08-13 22:45:51,012 - INFO - step: 141,loss: 0.635 accuracy: 0.688\n",
      "2017-08-13 22:45:51,074 - INFO - step: 142,loss: 0.758 accuracy: 0.703\n",
      "2017-08-13 22:45:51,138 - INFO - step: 143,loss: 0.725 accuracy: 0.734\n",
      "2017-08-13 22:45:51,159 - INFO - step: 144,loss: 0.241 accuracy: 0.857\n",
      "2017-08-13 22:45:51,161 - INFO - \train_epoch:\n",
      "2017-08-13 22:45:51,163 - INFO - loss_total: 0.668 accuracy_total: 0.724\n",
      "2017-08-13 22:45:51,235 - INFO - step: 145,loss: 0.643 accuracy: 0.703\n",
      "2017-08-13 22:45:51,299 - INFO - step: 146,loss: 0.704 accuracy: 0.703\n",
      "2017-08-13 22:45:51,362 - INFO - step: 147,loss: 0.726 accuracy: 0.750\n",
      "2017-08-13 22:45:51,425 - INFO - step: 148,loss: 0.376 accuracy: 0.875\n",
      "2017-08-13 22:45:51,488 - INFO - step: 149,loss: 0.621 accuracy: 0.781\n",
      "2017-08-13 22:45:51,551 - INFO - step: 150,loss: 0.571 accuracy: 0.750\n",
      "2017-08-13 22:45:51,613 - INFO - step: 151,loss: 0.751 accuracy: 0.625\n",
      "2017-08-13 22:45:51,675 - INFO - step: 152,loss: 0.630 accuracy: 0.781\n",
      "2017-08-13 22:45:51,739 - INFO - step: 153,loss: 0.546 accuracy: 0.781\n",
      "2017-08-13 22:45:51,801 - INFO - step: 154,loss: 0.657 accuracy: 0.750\n",
      "2017-08-13 22:45:51,864 - INFO - step: 155,loss: 0.595 accuracy: 0.688\n",
      "2017-08-13 22:45:51,925 - INFO - step: 156,loss: 0.459 accuracy: 0.828\n",
      "2017-08-13 22:45:51,989 - INFO - step: 157,loss: 0.469 accuracy: 0.797\n",
      "2017-08-13 22:45:52,052 - INFO - step: 158,loss: 0.588 accuracy: 0.812\n",
      "2017-08-13 22:45:52,115 - INFO - step: 159,loss: 0.707 accuracy: 0.797\n",
      "2017-08-13 22:45:52,183 - INFO - step: 160,loss: 0.632 accuracy: 0.766\n",
      "2017-08-13 22:45:52,243 - INFO - step: 161,loss: 0.473 accuracy: 0.781\n",
      "2017-08-13 22:45:52,308 - INFO - step: 162,loss: 0.801 accuracy: 0.750\n",
      "2017-08-13 22:45:52,373 - INFO - step: 163,loss: 0.571 accuracy: 0.797\n",
      "2017-08-13 22:45:52,437 - INFO - step: 164,loss: 0.495 accuracy: 0.781\n",
      "2017-08-13 22:45:52,497 - INFO - step: 165,loss: 0.493 accuracy: 0.766\n",
      "2017-08-13 22:45:52,580 - INFO - step: 166,loss: 0.489 accuracy: 0.766\n",
      "2017-08-13 22:45:52,662 - INFO - step: 167,loss: 0.642 accuracy: 0.734\n",
      "2017-08-13 22:45:52,725 - INFO - step: 168,loss: 0.339 accuracy: 0.797\n",
      "2017-08-13 22:45:52,787 - INFO - step: 169,loss: 0.498 accuracy: 0.734\n",
      "2017-08-13 22:45:52,850 - INFO - step: 170,loss: 0.620 accuracy: 0.719\n",
      "2017-08-13 22:45:52,912 - INFO - step: 171,loss: 0.703 accuracy: 0.656\n",
      "2017-08-13 22:45:52,974 - INFO - step: 172,loss: 0.594 accuracy: 0.781\n",
      "2017-08-13 22:45:53,037 - INFO - step: 173,loss: 0.281 accuracy: 0.922\n",
      "2017-08-13 22:45:53,098 - INFO - step: 174,loss: 0.386 accuracy: 0.812\n",
      "2017-08-13 22:45:53,161 - INFO - step: 175,loss: 0.635 accuracy: 0.703\n",
      "2017-08-13 22:45:53,223 - INFO - step: 176,loss: 0.477 accuracy: 0.781\n",
      "2017-08-13 22:45:53,284 - INFO - step: 177,loss: 0.429 accuracy: 0.781\n",
      "2017-08-13 22:45:53,345 - INFO - step: 178,loss: 0.576 accuracy: 0.781\n",
      "2017-08-13 22:45:53,407 - INFO - step: 179,loss: 0.476 accuracy: 0.781\n",
      "2017-08-13 22:45:53,429 - INFO - step: 180,loss: 0.358 accuracy: 0.714\n",
      "2017-08-13 22:45:53,431 - INFO - \n",
      "Evaluation:\n",
      "2017-08-13 22:45:53,490 - INFO - step: 180,loss: 0.274 accuracy: 0.884\n",
      "2017-08-13 22:45:53,584 - INFO - Saving model to /home/zx/cuckoo_1000/session_save/checkpoints/model-180 at step 4.\n",
      "2017-08-13 22:45:53,587 - INFO - \train_epoch:\n",
      "2017-08-13 22:45:53,589 - INFO - loss_total: 0.556 accuracy_total: 0.765\n",
      "2017-08-13 22:45:53,658 - INFO - step: 181,loss: 0.454 accuracy: 0.750\n",
      "2017-08-13 22:45:53,735 - INFO - step: 182,loss: 0.327 accuracy: 0.844\n",
      "2017-08-13 22:45:53,798 - INFO - step: 183,loss: 0.514 accuracy: 0.828\n",
      "2017-08-13 22:45:53,860 - INFO - step: 184,loss: 0.535 accuracy: 0.781\n",
      "2017-08-13 22:45:53,923 - INFO - step: 185,loss: 0.434 accuracy: 0.812\n",
      "2017-08-13 22:45:53,984 - INFO - step: 186,loss: 0.597 accuracy: 0.703\n",
      "2017-08-13 22:45:54,048 - INFO - step: 187,loss: 0.620 accuracy: 0.734\n",
      "2017-08-13 22:45:54,111 - INFO - step: 188,loss: 0.524 accuracy: 0.797\n",
      "2017-08-13 22:45:54,179 - INFO - step: 189,loss: 0.457 accuracy: 0.797\n",
      "2017-08-13 22:45:54,241 - INFO - step: 190,loss: 0.618 accuracy: 0.734\n",
      "2017-08-13 22:45:54,303 - INFO - step: 191,loss: 0.497 accuracy: 0.781\n",
      "2017-08-13 22:45:54,366 - INFO - step: 192,loss: 0.450 accuracy: 0.781\n",
      "2017-08-13 22:45:54,429 - INFO - step: 193,loss: 0.454 accuracy: 0.734\n",
      "2017-08-13 22:45:54,491 - INFO - step: 194,loss: 0.475 accuracy: 0.797\n",
      "2017-08-13 22:45:54,554 - INFO - step: 195,loss: 0.658 accuracy: 0.734\n",
      "2017-08-13 22:45:54,617 - INFO - step: 196,loss: 0.492 accuracy: 0.766\n",
      "2017-08-13 22:45:54,679 - INFO - step: 197,loss: 0.436 accuracy: 0.797\n",
      "2017-08-13 22:45:54,734 - INFO - step: 198,loss: 0.588 accuracy: 0.812\n",
      "2017-08-13 22:45:54,797 - INFO - step: 199,loss: 0.401 accuracy: 0.891\n",
      "2017-08-13 22:45:54,860 - INFO - step: 200,loss: 0.525 accuracy: 0.781\n",
      "2017-08-13 22:45:54,923 - INFO - step: 201,loss: 0.332 accuracy: 0.844\n",
      "2017-08-13 22:45:54,986 - INFO - step: 202,loss: 0.343 accuracy: 0.859\n",
      "2017-08-13 22:45:55,049 - INFO - step: 203,loss: 0.368 accuracy: 0.828\n",
      "2017-08-13 22:45:55,113 - INFO - step: 204,loss: 0.491 accuracy: 0.719\n",
      "2017-08-13 22:45:55,169 - INFO - step: 205,loss: 0.402 accuracy: 0.859\n",
      "2017-08-13 22:45:55,231 - INFO - step: 206,loss: 0.564 accuracy: 0.844\n",
      "2017-08-13 22:45:55,288 - INFO - step: 207,loss: 0.619 accuracy: 0.750\n",
      "2017-08-13 22:45:55,351 - INFO - step: 208,loss: 0.670 accuracy: 0.703\n",
      "2017-08-13 22:45:55,413 - INFO - step: 209,loss: 0.332 accuracy: 0.812\n",
      "2017-08-13 22:45:55,475 - INFO - step: 210,loss: 0.306 accuracy: 0.891\n",
      "2017-08-13 22:45:55,540 - INFO - step: 211,loss: 0.437 accuracy: 0.844\n",
      "2017-08-13 22:45:55,603 - INFO - step: 212,loss: 0.605 accuracy: 0.766\n",
      "2017-08-13 22:45:55,664 - INFO - step: 213,loss: 0.601 accuracy: 0.766\n",
      "2017-08-13 22:45:55,729 - INFO - step: 214,loss: 0.383 accuracy: 0.797\n",
      "2017-08-13 22:45:55,786 - INFO - step: 215,loss: 0.689 accuracy: 0.719\n",
      "2017-08-13 22:45:55,809 - INFO - step: 216,loss: 0.218 accuracy: 0.857\n",
      "2017-08-13 22:45:55,811 - INFO - \train_epoch:\n",
      "2017-08-13 22:45:55,813 - INFO - loss_total: 0.484 accuracy_total: 0.792\n",
      "2017-08-13 22:45:55,883 - INFO - step: 217,loss: 0.334 accuracy: 0.844\n",
      "2017-08-13 22:45:55,946 - INFO - step: 218,loss: 0.379 accuracy: 0.797\n",
      "2017-08-13 22:45:56,014 - INFO - step: 219,loss: 0.287 accuracy: 0.875\n",
      "2017-08-13 22:45:56,078 - INFO - step: 220,loss: 0.519 accuracy: 0.703\n",
      "2017-08-13 22:45:56,155 - INFO - step: 221,loss: 0.453 accuracy: 0.797\n",
      "2017-08-13 22:45:56,218 - INFO - step: 222,loss: 0.245 accuracy: 0.906\n",
      "2017-08-13 22:45:56,298 - INFO - step: 223,loss: 0.493 accuracy: 0.797\n",
      "2017-08-13 22:45:56,361 - INFO - step: 224,loss: 0.420 accuracy: 0.812\n",
      "2017-08-13 22:45:56,424 - INFO - step: 225,loss: 0.589 accuracy: 0.750\n",
      "2017-08-13 22:45:56,486 - INFO - step: 226,loss: 0.522 accuracy: 0.797\n",
      "2017-08-13 22:45:56,548 - INFO - step: 227,loss: 0.396 accuracy: 0.797\n",
      "2017-08-13 22:45:56,612 - INFO - step: 228,loss: 0.455 accuracy: 0.797\n",
      "2017-08-13 22:45:56,674 - INFO - step: 229,loss: 0.461 accuracy: 0.797\n",
      "2017-08-13 22:45:56,736 - INFO - step: 230,loss: 0.538 accuracy: 0.672\n",
      "2017-08-13 22:45:56,800 - INFO - step: 231,loss: 0.421 accuracy: 0.812\n",
      "2017-08-13 22:45:56,864 - INFO - step: 232,loss: 0.596 accuracy: 0.734\n",
      "2017-08-13 22:45:56,927 - INFO - step: 233,loss: 0.431 accuracy: 0.828\n",
      "2017-08-13 22:45:56,990 - INFO - step: 234,loss: 0.346 accuracy: 0.844\n",
      "2017-08-13 22:45:57,052 - INFO - step: 235,loss: 0.436 accuracy: 0.781\n",
      "2017-08-13 22:45:57,115 - INFO - step: 236,loss: 0.369 accuracy: 0.828\n",
      "2017-08-13 22:45:57,179 - INFO - step: 237,loss: 0.342 accuracy: 0.859\n",
      "2017-08-13 22:45:57,242 - INFO - step: 238,loss: 0.471 accuracy: 0.734\n",
      "2017-08-13 22:45:57,304 - INFO - step: 239,loss: 0.448 accuracy: 0.812\n",
      "2017-08-13 22:45:57,367 - INFO - step: 240,loss: 0.453 accuracy: 0.812\n",
      "2017-08-13 22:45:57,431 - INFO - step: 241,loss: 0.405 accuracy: 0.781\n",
      "2017-08-13 22:45:57,494 - INFO - step: 242,loss: 0.404 accuracy: 0.812\n",
      "2017-08-13 22:45:57,556 - INFO - step: 243,loss: 0.520 accuracy: 0.812\n",
      "2017-08-13 22:45:57,618 - INFO - step: 244,loss: 0.393 accuracy: 0.828\n",
      "2017-08-13 22:45:57,681 - INFO - step: 245,loss: 0.507 accuracy: 0.766\n",
      "2017-08-13 22:45:57,744 - INFO - step: 246,loss: 0.557 accuracy: 0.734\n",
      "2017-08-13 22:45:57,807 - INFO - step: 247,loss: 0.331 accuracy: 0.844\n",
      "2017-08-13 22:45:57,869 - INFO - step: 248,loss: 0.238 accuracy: 0.953\n",
      "2017-08-13 22:45:57,932 - INFO - step: 249,loss: 0.337 accuracy: 0.859\n",
      "2017-08-13 22:45:57,996 - INFO - step: 250,loss: 0.330 accuracy: 0.938\n",
      "2017-08-13 22:45:58,058 - INFO - step: 251,loss: 0.511 accuracy: 0.734\n",
      "2017-08-13 22:45:58,081 - INFO - step: 252,loss: 0.292 accuracy: 0.857\n",
      "2017-08-13 22:45:58,083 - INFO - \n",
      "Evaluation:\n",
      "2017-08-13 22:45:58,142 - INFO - step: 252,loss: 0.294 accuracy: 0.864\n",
      "2017-08-13 22:45:58,144 - INFO - \train_epoch:\n",
      "2017-08-13 22:45:58,145 - INFO - loss_total: 0.423 accuracy_total: 0.809\n",
      "2017-08-13 22:45:58,216 - INFO - step: 253,loss: 0.394 accuracy: 0.859\n",
      "2017-08-13 22:45:58,278 - INFO - step: 254,loss: 0.413 accuracy: 0.812\n",
      "2017-08-13 22:45:58,340 - INFO - step: 255,loss: 0.339 accuracy: 0.812\n",
      "2017-08-13 22:45:58,402 - INFO - step: 256,loss: 0.467 accuracy: 0.812\n",
      "2017-08-13 22:45:58,465 - INFO - step: 257,loss: 0.258 accuracy: 0.891\n",
      "2017-08-13 22:45:58,522 - INFO - step: 258,loss: 0.342 accuracy: 0.859\n",
      "2017-08-13 22:45:58,584 - INFO - step: 259,loss: 0.241 accuracy: 0.922\n",
      "2017-08-13 22:45:58,647 - INFO - step: 260,loss: 0.371 accuracy: 0.812\n",
      "2017-08-13 22:45:58,711 - INFO - step: 261,loss: 0.292 accuracy: 0.875\n",
      "2017-08-13 22:45:58,772 - INFO - step: 262,loss: 0.352 accuracy: 0.812\n",
      "2017-08-13 22:45:58,836 - INFO - step: 263,loss: 0.485 accuracy: 0.812\n",
      "2017-08-13 22:45:58,899 - INFO - step: 264,loss: 0.320 accuracy: 0.844\n",
      "2017-08-13 22:45:58,962 - INFO - step: 265,loss: 0.333 accuracy: 0.875\n",
      "2017-08-13 22:45:59,026 - INFO - step: 266,loss: 0.371 accuracy: 0.781\n",
      "2017-08-13 22:45:59,090 - INFO - step: 267,loss: 0.519 accuracy: 0.781\n",
      "2017-08-13 22:45:59,156 - INFO - step: 268,loss: 0.357 accuracy: 0.844\n",
      "2017-08-13 22:45:59,218 - INFO - step: 269,loss: 0.419 accuracy: 0.781\n",
      "2017-08-13 22:45:59,280 - INFO - step: 270,loss: 0.419 accuracy: 0.828\n",
      "2017-08-13 22:45:59,341 - INFO - step: 271,loss: 0.325 accuracy: 0.828\n",
      "2017-08-13 22:45:59,405 - INFO - step: 272,loss: 0.269 accuracy: 0.906\n",
      "2017-08-13 22:45:59,467 - INFO - step: 273,loss: 0.474 accuracy: 0.797\n",
      "2017-08-13 22:45:59,528 - INFO - step: 274,loss: 0.394 accuracy: 0.781\n",
      "2017-08-13 22:45:59,591 - INFO - step: 275,loss: 0.504 accuracy: 0.797\n",
      "2017-08-13 22:45:59,654 - INFO - step: 276,loss: 0.432 accuracy: 0.812\n",
      "2017-08-13 22:45:59,717 - INFO - step: 277,loss: 0.308 accuracy: 0.859\n",
      "2017-08-13 22:45:59,780 - INFO - step: 278,loss: 0.430 accuracy: 0.797\n",
      "2017-08-13 22:45:59,837 - INFO - step: 279,loss: 0.334 accuracy: 0.859\n",
      "2017-08-13 22:45:59,899 - INFO - step: 280,loss: 0.347 accuracy: 0.859\n",
      "2017-08-13 22:45:59,963 - INFO - step: 281,loss: 0.513 accuracy: 0.812\n",
      "2017-08-13 22:46:00,024 - INFO - step: 282,loss: 0.342 accuracy: 0.875\n",
      "2017-08-13 22:46:00,088 - INFO - step: 283,loss: 0.410 accuracy: 0.812\n",
      "2017-08-13 22:46:00,151 - INFO - step: 284,loss: 0.378 accuracy: 0.766\n",
      "2017-08-13 22:46:00,214 - INFO - step: 285,loss: 0.371 accuracy: 0.844\n",
      "2017-08-13 22:46:00,277 - INFO - step: 286,loss: 0.432 accuracy: 0.828\n",
      "2017-08-13 22:46:00,338 - INFO - step: 287,loss: 0.321 accuracy: 0.844\n",
      "2017-08-13 22:46:00,359 - INFO - step: 288,loss: 0.780 accuracy: 0.714\n",
      "2017-08-13 22:46:00,361 - INFO - \train_epoch:\n",
      "2017-08-13 22:46:00,363 - INFO - loss_total: 0.390 accuracy_total: 0.828\n",
      "2017-08-13 22:46:00,432 - INFO - step: 289,loss: 0.447 accuracy: 0.797\n",
      "2017-08-13 22:46:00,493 - INFO - step: 290,loss: 0.364 accuracy: 0.828\n",
      "2017-08-13 22:46:00,556 - INFO - step: 291,loss: 0.312 accuracy: 0.906\n",
      "2017-08-13 22:46:00,619 - INFO - step: 292,loss: 0.568 accuracy: 0.672\n",
      "2017-08-13 22:46:00,683 - INFO - step: 293,loss: 0.237 accuracy: 0.938\n",
      "2017-08-13 22:46:00,746 - INFO - step: 294,loss: 0.385 accuracy: 0.828\n",
      "2017-08-13 22:46:00,802 - INFO - step: 295,loss: 0.456 accuracy: 0.812\n",
      "2017-08-13 22:46:00,864 - INFO - step: 296,loss: 0.354 accuracy: 0.906\n",
      "2017-08-13 22:46:00,927 - INFO - step: 297,loss: 0.347 accuracy: 0.859\n",
      "2017-08-13 22:46:00,991 - INFO - step: 298,loss: 0.573 accuracy: 0.719\n",
      "2017-08-13 22:46:01,054 - INFO - step: 299,loss: 0.429 accuracy: 0.828\n",
      "2017-08-13 22:46:01,116 - INFO - step: 300,loss: 0.266 accuracy: 0.953\n",
      "2017-08-13 22:46:01,180 - INFO - step: 301,loss: 0.478 accuracy: 0.750\n",
      "2017-08-13 22:46:01,243 - INFO - step: 302,loss: 0.351 accuracy: 0.844\n",
      "2017-08-13 22:46:01,305 - INFO - step: 303,loss: 0.159 accuracy: 0.938\n",
      "2017-08-13 22:46:01,368 - INFO - step: 304,loss: 0.361 accuracy: 0.781\n",
      "2017-08-13 22:46:01,431 - INFO - step: 305,loss: 0.332 accuracy: 0.875\n",
      "2017-08-13 22:46:01,494 - INFO - step: 306,loss: 0.280 accuracy: 0.906\n",
      "2017-08-13 22:46:01,555 - INFO - step: 307,loss: 0.427 accuracy: 0.797\n",
      "2017-08-13 22:46:01,619 - INFO - step: 308,loss: 0.451 accuracy: 0.828\n",
      "2017-08-13 22:46:01,674 - INFO - step: 309,loss: 0.373 accuracy: 0.844\n",
      "2017-08-13 22:46:01,737 - INFO - step: 310,loss: 0.404 accuracy: 0.812\n",
      "2017-08-13 22:46:01,801 - INFO - step: 311,loss: 0.291 accuracy: 0.844\n",
      "2017-08-13 22:46:01,866 - INFO - step: 312,loss: 0.402 accuracy: 0.812\n",
      "2017-08-13 22:46:01,930 - INFO - step: 313,loss: 0.328 accuracy: 0.891\n",
      "2017-08-13 22:46:01,993 - INFO - step: 314,loss: 0.343 accuracy: 0.844\n",
      "2017-08-13 22:46:02,057 - INFO - step: 315,loss: 0.252 accuracy: 0.938\n",
      "2017-08-13 22:46:02,139 - INFO - step: 316,loss: 0.286 accuracy: 0.891\n",
      "2017-08-13 22:46:02,205 - INFO - step: 317,loss: 0.390 accuracy: 0.859\n",
      "2017-08-13 22:46:02,267 - INFO - step: 318,loss: 0.377 accuracy: 0.844\n",
      "2017-08-13 22:46:02,331 - INFO - step: 319,loss: 0.277 accuracy: 0.906\n",
      "2017-08-13 22:46:02,391 - INFO - step: 320,loss: 0.447 accuracy: 0.891\n",
      "2017-08-13 22:46:02,454 - INFO - step: 321,loss: 0.274 accuracy: 0.891\n",
      "2017-08-13 22:46:02,517 - INFO - step: 322,loss: 0.362 accuracy: 0.875\n",
      "2017-08-13 22:46:02,579 - INFO - step: 323,loss: 0.481 accuracy: 0.766\n",
      "2017-08-13 22:46:02,602 - INFO - step: 324,loss: 0.168 accuracy: 1.000\n",
      "2017-08-13 22:46:02,604 - INFO - \n",
      "Evaluation:\n",
      "2017-08-13 22:46:02,664 - INFO - step: 324,loss: 0.256 accuracy: 0.896\n",
      "2017-08-13 22:46:02,748 - INFO - Saving model to /home/zx/cuckoo_1000/session_save/checkpoints/model-324 at step 8.\n",
      "2017-08-13 22:46:02,749 - INFO - \train_epoch:\n",
      "2017-08-13 22:46:02,750 - INFO - loss_total: 0.362 accuracy_total: 0.852\n",
      "2017-08-13 22:46:02,831 - INFO - step: 325,loss: 0.282 accuracy: 0.891\n",
      "2017-08-13 22:46:02,897 - INFO - step: 326,loss: 0.212 accuracy: 0.906\n",
      "2017-08-13 22:46:02,960 - INFO - step: 327,loss: 0.439 accuracy: 0.797\n",
      "2017-08-13 22:46:03,022 - INFO - step: 328,loss: 0.316 accuracy: 0.875\n",
      "2017-08-13 22:46:03,085 - INFO - step: 329,loss: 0.398 accuracy: 0.812\n",
      "2017-08-13 22:46:03,149 - INFO - step: 330,loss: 0.369 accuracy: 0.844\n",
      "2017-08-13 22:46:03,212 - INFO - step: 331,loss: 0.279 accuracy: 0.891\n",
      "2017-08-13 22:46:03,276 - INFO - step: 332,loss: 0.415 accuracy: 0.859\n",
      "2017-08-13 22:46:03,339 - INFO - step: 333,loss: 0.223 accuracy: 0.906\n",
      "2017-08-13 22:46:03,403 - INFO - step: 334,loss: 0.423 accuracy: 0.797\n",
      "2017-08-13 22:46:03,466 - INFO - step: 335,loss: 0.329 accuracy: 0.844\n",
      "2017-08-13 22:46:03,529 - INFO - step: 336,loss: 0.272 accuracy: 0.922\n",
      "2017-08-13 22:46:03,592 - INFO - step: 337,loss: 0.348 accuracy: 0.844\n",
      "2017-08-13 22:46:03,659 - INFO - step: 338,loss: 0.395 accuracy: 0.828\n",
      "2017-08-13 22:46:03,723 - INFO - step: 339,loss: 0.292 accuracy: 0.859\n",
      "2017-08-13 22:46:03,786 - INFO - step: 340,loss: 0.488 accuracy: 0.766\n",
      "2017-08-13 22:46:03,848 - INFO - step: 341,loss: 0.260 accuracy: 0.906\n",
      "2017-08-13 22:46:03,909 - INFO - step: 342,loss: 0.276 accuracy: 0.875\n",
      "2017-08-13 22:46:03,972 - INFO - step: 343,loss: 0.299 accuracy: 0.891\n",
      "2017-08-13 22:46:04,034 - INFO - step: 344,loss: 0.313 accuracy: 0.859\n",
      "2017-08-13 22:46:04,099 - INFO - step: 345,loss: 0.437 accuracy: 0.812\n",
      "2017-08-13 22:46:04,162 - INFO - step: 346,loss: 0.256 accuracy: 0.938\n",
      "2017-08-13 22:46:04,229 - INFO - step: 347,loss: 0.221 accuracy: 0.922\n",
      "2017-08-13 22:46:04,287 - INFO - step: 348,loss: 0.242 accuracy: 0.906\n",
      "2017-08-13 22:46:04,350 - INFO - step: 349,loss: 0.396 accuracy: 0.844\n",
      "2017-08-13 22:46:04,413 - INFO - step: 350,loss: 0.467 accuracy: 0.781\n",
      "2017-08-13 22:46:04,492 - INFO - step: 351,loss: 0.380 accuracy: 0.828\n",
      "2017-08-13 22:46:04,557 - INFO - step: 352,loss: 0.196 accuracy: 0.938\n",
      "2017-08-13 22:46:04,619 - INFO - step: 353,loss: 0.315 accuracy: 0.859\n",
      "2017-08-13 22:46:04,681 - INFO - step: 354,loss: 0.421 accuracy: 0.750\n",
      "2017-08-13 22:46:04,748 - INFO - step: 355,loss: 0.308 accuracy: 0.844\n",
      "2017-08-13 22:46:04,812 - INFO - step: 356,loss: 0.325 accuracy: 0.875\n",
      "2017-08-13 22:46:04,875 - INFO - step: 357,loss: 0.385 accuracy: 0.797\n",
      "2017-08-13 22:46:04,939 - INFO - step: 358,loss: 0.206 accuracy: 0.906\n",
      "2017-08-13 22:46:05,001 - INFO - step: 359,loss: 0.423 accuracy: 0.844\n",
      "2017-08-13 22:46:05,026 - INFO - step: 360,loss: 0.432 accuracy: 0.714\n",
      "2017-08-13 22:46:05,028 - INFO - \train_epoch:\n",
      "2017-08-13 22:46:05,029 - INFO - loss_total: 0.334 accuracy_total: 0.854\n",
      "2017-08-13 22:46:05,111 - INFO - step: 361,loss: 0.322 accuracy: 0.844\n",
      "2017-08-13 22:46:05,168 - INFO - step: 362,loss: 0.278 accuracy: 0.828\n",
      "2017-08-13 22:46:05,231 - INFO - step: 363,loss: 0.195 accuracy: 0.922\n",
      "2017-08-13 22:46:05,294 - INFO - step: 364,loss: 0.224 accuracy: 0.891\n",
      "2017-08-13 22:46:05,357 - INFO - step: 365,loss: 0.368 accuracy: 0.859\n",
      "2017-08-13 22:46:05,420 - INFO - step: 366,loss: 0.629 accuracy: 0.750\n",
      "2017-08-13 22:46:05,483 - INFO - step: 367,loss: 0.376 accuracy: 0.797\n",
      "2017-08-13 22:46:05,547 - INFO - step: 368,loss: 0.312 accuracy: 0.875\n",
      "2017-08-13 22:46:05,609 - INFO - step: 369,loss: 0.357 accuracy: 0.781\n",
      "2017-08-13 22:46:05,670 - INFO - step: 370,loss: 0.304 accuracy: 0.859\n",
      "2017-08-13 22:46:05,753 - INFO - step: 371,loss: 0.308 accuracy: 0.906\n",
      "2017-08-13 22:46:05,816 - INFO - step: 372,loss: 0.298 accuracy: 0.875\n",
      "2017-08-13 22:46:05,876 - INFO - step: 373,loss: 0.315 accuracy: 0.828\n",
      "2017-08-13 22:46:05,939 - INFO - step: 374,loss: 0.440 accuracy: 0.844\n",
      "2017-08-13 22:46:06,001 - INFO - step: 375,loss: 0.375 accuracy: 0.859\n",
      "2017-08-13 22:46:06,064 - INFO - step: 376,loss: 0.325 accuracy: 0.859\n",
      "2017-08-13 22:46:06,127 - INFO - step: 377,loss: 0.356 accuracy: 0.844\n",
      "2017-08-13 22:46:06,190 - INFO - step: 378,loss: 0.293 accuracy: 0.875\n",
      "2017-08-13 22:46:06,252 - INFO - step: 379,loss: 0.328 accuracy: 0.859\n",
      "2017-08-13 22:46:06,313 - INFO - step: 380,loss: 0.307 accuracy: 0.891\n",
      "2017-08-13 22:46:06,376 - INFO - step: 381,loss: 0.200 accuracy: 0.922\n",
      "2017-08-13 22:46:06,441 - INFO - step: 382,loss: 0.284 accuracy: 0.844\n",
      "2017-08-13 22:46:06,503 - INFO - step: 383,loss: 0.285 accuracy: 0.891\n",
      "2017-08-13 22:46:06,568 - INFO - step: 384,loss: 0.303 accuracy: 0.844\n",
      "2017-08-13 22:46:06,632 - INFO - step: 385,loss: 0.349 accuracy: 0.859\n",
      "2017-08-13 22:46:06,694 - INFO - step: 386,loss: 0.355 accuracy: 0.812\n",
      "2017-08-13 22:46:06,757 - INFO - step: 387,loss: 0.227 accuracy: 0.938\n",
      "2017-08-13 22:46:06,818 - INFO - step: 388,loss: 0.147 accuracy: 0.953\n",
      "2017-08-13 22:46:06,879 - INFO - step: 389,loss: 0.341 accuracy: 0.859\n",
      "2017-08-13 22:46:06,940 - INFO - step: 390,loss: 0.291 accuracy: 0.891\n",
      "2017-08-13 22:46:07,002 - INFO - step: 391,loss: 0.511 accuracy: 0.797\n",
      "2017-08-13 22:46:07,065 - INFO - step: 392,loss: 0.333 accuracy: 0.844\n",
      "2017-08-13 22:46:07,126 - INFO - step: 393,loss: 0.295 accuracy: 0.891\n",
      "2017-08-13 22:46:07,188 - INFO - step: 394,loss: 0.391 accuracy: 0.797\n",
      "2017-08-13 22:46:07,250 - INFO - step: 395,loss: 0.307 accuracy: 0.891\n",
      "2017-08-13 22:46:07,272 - INFO - step: 396,loss: 0.590 accuracy: 0.714\n",
      "2017-08-13 22:46:07,274 - INFO - \n",
      "Evaluation:\n",
      "2017-08-13 22:46:07,334 - INFO - step: 396,loss: 0.254 accuracy: 0.892\n",
      "2017-08-13 22:46:07,336 - INFO - \train_epoch:\n",
      "2017-08-13 22:46:07,338 - INFO - loss_total: 0.331 accuracy_total: 0.855\n",
      "2017-08-13 22:46:07,413 - INFO - step: 397,loss: 0.292 accuracy: 0.891\n",
      "2017-08-13 22:46:07,474 - INFO - step: 398,loss: 0.277 accuracy: 0.891\n",
      "2017-08-13 22:46:07,536 - INFO - step: 399,loss: 0.314 accuracy: 0.891\n",
      "2017-08-13 22:46:07,594 - INFO - step: 400,loss: 0.219 accuracy: 0.922\n",
      "2017-08-13 22:46:07,655 - INFO - step: 401,loss: 0.304 accuracy: 0.891\n",
      "2017-08-13 22:46:07,717 - INFO - step: 402,loss: 0.227 accuracy: 0.922\n",
      "2017-08-13 22:46:07,781 - INFO - step: 403,loss: 0.411 accuracy: 0.812\n",
      "2017-08-13 22:46:07,844 - INFO - step: 404,loss: 0.320 accuracy: 0.859\n",
      "2017-08-13 22:46:07,908 - INFO - step: 405,loss: 0.306 accuracy: 0.875\n",
      "2017-08-13 22:46:07,971 - INFO - step: 406,loss: 0.244 accuracy: 0.875\n",
      "2017-08-13 22:46:08,035 - INFO - step: 407,loss: 0.392 accuracy: 0.781\n",
      "2017-08-13 22:46:08,096 - INFO - step: 408,loss: 0.274 accuracy: 0.938\n",
      "2017-08-13 22:46:08,160 - INFO - step: 409,loss: 0.318 accuracy: 0.891\n",
      "2017-08-13 22:46:08,224 - INFO - step: 410,loss: 0.342 accuracy: 0.875\n",
      "2017-08-13 22:46:08,289 - INFO - step: 411,loss: 0.245 accuracy: 0.906\n",
      "2017-08-13 22:46:08,353 - INFO - step: 412,loss: 0.338 accuracy: 0.812\n",
      "2017-08-13 22:46:08,420 - INFO - step: 413,loss: 0.207 accuracy: 0.922\n",
      "2017-08-13 22:46:08,482 - INFO - step: 414,loss: 0.229 accuracy: 0.922\n",
      "2017-08-13 22:46:08,545 - INFO - step: 415,loss: 0.391 accuracy: 0.859\n",
      "2017-08-13 22:46:08,608 - INFO - step: 416,loss: 0.324 accuracy: 0.891\n",
      "2017-08-13 22:46:08,677 - INFO - step: 417,loss: 0.351 accuracy: 0.812\n",
      "2017-08-13 22:46:08,741 - INFO - step: 418,loss: 0.277 accuracy: 0.875\n",
      "2017-08-13 22:46:08,803 - INFO - step: 419,loss: 0.355 accuracy: 0.844\n",
      "2017-08-13 22:46:08,867 - INFO - step: 420,loss: 0.350 accuracy: 0.875\n",
      "2017-08-13 22:46:08,929 - INFO - step: 421,loss: 0.302 accuracy: 0.828\n",
      "2017-08-13 22:46:08,992 - INFO - step: 422,loss: 0.308 accuracy: 0.875\n",
      "2017-08-13 22:46:09,055 - INFO - step: 423,loss: 0.244 accuracy: 0.906\n",
      "2017-08-13 22:46:09,119 - INFO - step: 424,loss: 0.324 accuracy: 0.844\n",
      "2017-08-13 22:46:09,183 - INFO - step: 425,loss: 0.233 accuracy: 0.875\n",
      "2017-08-13 22:46:09,246 - INFO - step: 426,loss: 0.355 accuracy: 0.875\n",
      "2017-08-13 22:46:09,308 - INFO - step: 427,loss: 0.311 accuracy: 0.875\n",
      "2017-08-13 22:46:09,372 - INFO - step: 428,loss: 0.353 accuracy: 0.812\n",
      "2017-08-13 22:46:09,434 - INFO - step: 429,loss: 0.212 accuracy: 0.906\n",
      "2017-08-13 22:46:09,497 - INFO - step: 430,loss: 0.254 accuracy: 0.891\n",
      "2017-08-13 22:46:09,558 - INFO - step: 431,loss: 0.264 accuracy: 0.906\n",
      "2017-08-13 22:46:09,582 - INFO - step: 432,loss: 0.308 accuracy: 0.857\n",
      "2017-08-13 22:46:09,584 - INFO - \train_epoch:\n",
      "2017-08-13 22:46:09,586 - INFO - loss_total: 0.299 accuracy_total: 0.875\n",
      "2017-08-13 22:46:09,657 - INFO - step: 433,loss: 0.332 accuracy: 0.859\n",
      "2017-08-13 22:46:09,721 - INFO - step: 434,loss: 0.344 accuracy: 0.828\n",
      "2017-08-13 22:46:09,783 - INFO - step: 435,loss: 0.472 accuracy: 0.797\n",
      "2017-08-13 22:46:09,846 - INFO - step: 436,loss: 0.306 accuracy: 0.859\n",
      "2017-08-13 22:46:09,908 - INFO - step: 437,loss: 0.319 accuracy: 0.875\n",
      "2017-08-13 22:46:09,972 - INFO - step: 438,loss: 0.347 accuracy: 0.828\n",
      "2017-08-13 22:46:10,034 - INFO - step: 439,loss: 0.253 accuracy: 0.891\n",
      "2017-08-13 22:46:10,098 - INFO - step: 440,loss: 0.309 accuracy: 0.891\n",
      "2017-08-13 22:46:10,162 - INFO - step: 441,loss: 0.224 accuracy: 0.875\n",
      "2017-08-13 22:46:10,227 - INFO - step: 442,loss: 0.331 accuracy: 0.859\n",
      "2017-08-13 22:46:10,284 - INFO - step: 443,loss: 0.286 accuracy: 0.859\n",
      "2017-08-13 22:46:10,347 - INFO - step: 444,loss: 0.239 accuracy: 0.875\n",
      "2017-08-13 22:46:10,410 - INFO - step: 445,loss: 0.255 accuracy: 0.875\n",
      "2017-08-13 22:46:10,473 - INFO - step: 446,loss: 0.269 accuracy: 0.875\n",
      "2017-08-13 22:46:10,535 - INFO - step: 447,loss: 0.340 accuracy: 0.875\n",
      "2017-08-13 22:46:10,599 - INFO - step: 448,loss: 0.220 accuracy: 0.906\n",
      "2017-08-13 22:46:10,661 - INFO - step: 449,loss: 0.211 accuracy: 0.922\n",
      "2017-08-13 22:46:10,726 - INFO - step: 450,loss: 0.474 accuracy: 0.781\n",
      "2017-08-13 22:46:10,789 - INFO - step: 451,loss: 0.274 accuracy: 0.875\n",
      "2017-08-13 22:46:10,850 - INFO - step: 452,loss: 0.275 accuracy: 0.875\n",
      "2017-08-13 22:46:10,910 - INFO - step: 453,loss: 0.298 accuracy: 0.891\n",
      "2017-08-13 22:46:10,972 - INFO - step: 454,loss: 0.306 accuracy: 0.906\n",
      "2017-08-13 22:46:11,035 - INFO - step: 455,loss: 0.474 accuracy: 0.797\n",
      "2017-08-13 22:46:11,099 - INFO - step: 456,loss: 0.377 accuracy: 0.812\n",
      "2017-08-13 22:46:11,163 - INFO - step: 457,loss: 0.242 accuracy: 0.875\n",
      "2017-08-13 22:46:11,224 - INFO - step: 458,loss: 0.365 accuracy: 0.859\n",
      "2017-08-13 22:46:11,287 - INFO - step: 459,loss: 0.397 accuracy: 0.828\n",
      "2017-08-13 22:46:11,352 - INFO - step: 460,loss: 0.253 accuracy: 0.922\n",
      "2017-08-13 22:46:11,419 - INFO - step: 461,loss: 0.203 accuracy: 0.938\n",
      "2017-08-13 22:46:11,483 - INFO - step: 462,loss: 0.292 accuracy: 0.859\n",
      "2017-08-13 22:46:11,545 - INFO - step: 463,loss: 0.397 accuracy: 0.906\n",
      "2017-08-13 22:46:11,608 - INFO - step: 464,loss: 0.171 accuracy: 0.922\n",
      "2017-08-13 22:46:11,672 - INFO - step: 465,loss: 0.386 accuracy: 0.844\n",
      "2017-08-13 22:46:11,739 - INFO - step: 466,loss: 0.254 accuracy: 0.891\n",
      "2017-08-13 22:46:11,801 - INFO - step: 467,loss: 0.136 accuracy: 0.953\n",
      "2017-08-13 22:46:11,825 - INFO - step: 468,loss: 0.564 accuracy: 0.714\n",
      "2017-08-13 22:46:11,827 - INFO - \n",
      "Evaluation:\n",
      "2017-08-13 22:46:11,970 - INFO - step: 468,loss: 0.236 accuracy: 0.904\n",
      "2017-08-13 22:46:12,036 - INFO - Saving model to /home/zx/cuckoo_1000/session_save/checkpoints/model-468 at step 12.\n",
      "2017-08-13 22:46:12,038 - INFO - \train_epoch:\n",
      "2017-08-13 22:46:12,039 - INFO - loss_total: 0.311 accuracy_total: 0.867\n",
      "2017-08-13 22:46:12,107 - INFO - step: 469,loss: 0.240 accuracy: 0.906\n",
      "2017-08-13 22:46:12,171 - INFO - step: 470,loss: 0.270 accuracy: 0.891\n",
      "2017-08-13 22:46:12,235 - INFO - step: 471,loss: 0.220 accuracy: 0.922\n",
      "2017-08-13 22:46:12,299 - INFO - step: 472,loss: 0.203 accuracy: 0.922\n",
      "2017-08-13 22:46:12,363 - INFO - step: 473,loss: 0.456 accuracy: 0.828\n",
      "2017-08-13 22:46:12,429 - INFO - step: 474,loss: 0.249 accuracy: 0.922\n",
      "2017-08-13 22:46:12,494 - INFO - step: 475,loss: 0.300 accuracy: 0.859\n",
      "2017-08-13 22:46:12,555 - INFO - step: 476,loss: 0.231 accuracy: 0.922\n",
      "2017-08-13 22:46:12,618 - INFO - step: 477,loss: 0.259 accuracy: 0.922\n",
      "2017-08-13 22:46:12,681 - INFO - step: 478,loss: 0.397 accuracy: 0.828\n",
      "2017-08-13 22:46:12,745 - INFO - step: 479,loss: 0.352 accuracy: 0.875\n",
      "2017-08-13 22:46:12,810 - INFO - step: 480,loss: 0.225 accuracy: 0.938\n",
      "2017-08-13 22:46:12,877 - INFO - step: 481,loss: 0.285 accuracy: 0.891\n",
      "2017-08-13 22:46:12,941 - INFO - step: 482,loss: 0.429 accuracy: 0.828\n",
      "2017-08-13 22:46:13,004 - INFO - step: 483,loss: 0.302 accuracy: 0.859\n",
      "2017-08-13 22:46:13,068 - INFO - step: 484,loss: 0.226 accuracy: 0.906\n",
      "2017-08-13 22:46:13,130 - INFO - step: 485,loss: 0.367 accuracy: 0.859\n",
      "2017-08-13 22:46:13,194 - INFO - step: 486,loss: 0.399 accuracy: 0.859\n",
      "2017-08-13 22:46:13,277 - INFO - step: 487,loss: 0.144 accuracy: 0.953\n",
      "2017-08-13 22:46:13,341 - INFO - step: 488,loss: 0.247 accuracy: 0.922\n",
      "2017-08-13 22:46:13,404 - INFO - step: 489,loss: 0.234 accuracy: 0.844\n",
      "2017-08-13 22:46:13,469 - INFO - step: 490,loss: 0.308 accuracy: 0.844\n",
      "2017-08-13 22:46:13,532 - INFO - step: 491,loss: 0.244 accuracy: 0.891\n",
      "2017-08-13 22:46:13,594 - INFO - step: 492,loss: 0.296 accuracy: 0.906\n",
      "2017-08-13 22:46:13,656 - INFO - step: 493,loss: 0.290 accuracy: 0.891\n",
      "2017-08-13 22:46:13,721 - INFO - step: 494,loss: 0.291 accuracy: 0.859\n",
      "2017-08-13 22:46:13,785 - INFO - step: 495,loss: 0.259 accuracy: 0.891\n",
      "2017-08-13 22:46:13,847 - INFO - step: 496,loss: 0.224 accuracy: 0.891\n",
      "2017-08-13 22:46:13,911 - INFO - step: 497,loss: 0.280 accuracy: 0.875\n",
      "2017-08-13 22:46:13,971 - INFO - step: 498,loss: 0.382 accuracy: 0.797\n",
      "2017-08-13 22:46:14,036 - INFO - step: 499,loss: 0.368 accuracy: 0.812\n",
      "2017-08-13 22:46:14,097 - INFO - step: 500,loss: 0.237 accuracy: 0.891\n",
      "2017-08-13 22:46:14,163 - INFO - step: 501,loss: 0.416 accuracy: 0.844\n",
      "2017-08-13 22:46:14,227 - INFO - step: 502,loss: 0.241 accuracy: 0.859\n",
      "2017-08-13 22:46:14,291 - INFO - step: 503,loss: 0.279 accuracy: 0.859\n",
      "2017-08-13 22:46:14,315 - INFO - step: 504,loss: 0.524 accuracy: 0.714\n",
      "2017-08-13 22:46:14,317 - INFO - \train_epoch:\n",
      "2017-08-13 22:46:14,319 - INFO - loss_total: 0.296 accuracy_total: 0.874\n",
      "2017-08-13 22:46:14,401 - INFO - step: 505,loss: 0.199 accuracy: 0.922\n",
      "2017-08-13 22:46:14,464 - INFO - step: 506,loss: 0.298 accuracy: 0.891\n",
      "2017-08-13 22:46:14,527 - INFO - step: 507,loss: 0.175 accuracy: 0.953\n",
      "2017-08-13 22:46:14,591 - INFO - step: 508,loss: 0.160 accuracy: 0.969\n",
      "2017-08-13 22:46:14,653 - INFO - step: 509,loss: 0.277 accuracy: 0.844\n",
      "2017-08-13 22:46:14,717 - INFO - step: 510,loss: 0.331 accuracy: 0.891\n",
      "2017-08-13 22:46:14,780 - INFO - step: 511,loss: 0.327 accuracy: 0.812\n",
      "2017-08-13 22:46:14,847 - INFO - step: 512,loss: 0.328 accuracy: 0.828\n",
      "2017-08-13 22:46:14,910 - INFO - step: 513,loss: 0.292 accuracy: 0.875\n",
      "2017-08-13 22:46:14,975 - INFO - step: 514,loss: 0.214 accuracy: 0.922\n",
      "2017-08-13 22:46:15,039 - INFO - step: 515,loss: 0.246 accuracy: 0.922\n",
      "2017-08-13 22:46:15,101 - INFO - step: 516,loss: 0.215 accuracy: 0.875\n",
      "2017-08-13 22:46:15,162 - INFO - step: 517,loss: 0.289 accuracy: 0.859\n",
      "2017-08-13 22:46:15,224 - INFO - step: 518,loss: 0.222 accuracy: 0.938\n",
      "2017-08-13 22:46:15,282 - INFO - step: 519,loss: 0.304 accuracy: 0.859\n",
      "2017-08-13 22:46:15,344 - INFO - step: 520,loss: 0.363 accuracy: 0.891\n",
      "2017-08-13 22:46:15,408 - INFO - step: 521,loss: 0.218 accuracy: 0.906\n",
      "2017-08-13 22:46:15,472 - INFO - step: 522,loss: 0.221 accuracy: 0.875\n",
      "2017-08-13 22:46:15,536 - INFO - step: 523,loss: 0.369 accuracy: 0.859\n",
      "2017-08-13 22:46:15,593 - INFO - step: 524,loss: 0.273 accuracy: 0.891\n",
      "2017-08-13 22:46:15,656 - INFO - step: 525,loss: 0.337 accuracy: 0.828\n",
      "2017-08-13 22:46:15,718 - INFO - step: 526,loss: 0.325 accuracy: 0.844\n",
      "2017-08-13 22:46:15,783 - INFO - step: 527,loss: 0.227 accuracy: 0.906\n",
      "2017-08-13 22:46:15,847 - INFO - step: 528,loss: 0.327 accuracy: 0.891\n",
      "2017-08-13 22:46:15,912 - INFO - step: 529,loss: 0.312 accuracy: 0.891\n",
      "2017-08-13 22:46:15,975 - INFO - step: 530,loss: 0.391 accuracy: 0.828\n",
      "2017-08-13 22:46:16,039 - INFO - step: 531,loss: 0.312 accuracy: 0.844\n",
      "2017-08-13 22:46:16,103 - INFO - step: 532,loss: 0.149 accuracy: 0.953\n",
      "2017-08-13 22:46:16,169 - INFO - step: 533,loss: 0.350 accuracy: 0.812\n",
      "2017-08-13 22:46:16,235 - INFO - step: 534,loss: 0.119 accuracy: 0.969\n",
      "2017-08-13 22:46:16,298 - INFO - step: 535,loss: 0.298 accuracy: 0.875\n",
      "2017-08-13 22:46:16,362 - INFO - step: 536,loss: 0.123 accuracy: 0.969\n",
      "2017-08-13 22:46:16,427 - INFO - step: 537,loss: 0.266 accuracy: 0.859\n",
      "2017-08-13 22:46:16,490 - INFO - step: 538,loss: 0.347 accuracy: 0.844\n",
      "2017-08-13 22:46:16,552 - INFO - step: 539,loss: 0.194 accuracy: 0.922\n",
      "2017-08-13 22:46:16,573 - INFO - step: 540,loss: 0.122 accuracy: 1.000\n",
      "2017-08-13 22:46:16,576 - INFO - \n",
      "Evaluation:\n",
      "2017-08-13 22:46:16,715 - INFO - step: 540,loss: 0.246 accuracy: 0.888\n",
      "2017-08-13 22:46:16,717 - INFO - \train_epoch:\n",
      "2017-08-13 22:46:16,718 - INFO - loss_total: 0.264 accuracy_total: 0.889\n",
      "2017-08-13 22:46:16,800 - INFO - step: 541,loss: 0.264 accuracy: 0.875\n",
      "2017-08-13 22:46:16,865 - INFO - step: 542,loss: 0.238 accuracy: 0.922\n",
      "2017-08-13 22:46:16,944 - INFO - step: 543,loss: 0.220 accuracy: 0.891\n",
      "2017-08-13 22:46:17,009 - INFO - step: 544,loss: 0.260 accuracy: 0.859\n",
      "2017-08-13 22:46:17,069 - INFO - step: 545,loss: 0.279 accuracy: 0.875\n",
      "2017-08-13 22:46:17,134 - INFO - step: 546,loss: 0.282 accuracy: 0.859\n",
      "2017-08-13 22:46:17,197 - INFO - step: 547,loss: 0.174 accuracy: 0.906\n",
      "2017-08-13 22:46:17,260 - INFO - step: 548,loss: 0.273 accuracy: 0.859\n",
      "2017-08-13 22:46:17,324 - INFO - step: 549,loss: 0.238 accuracy: 0.906\n",
      "2017-08-13 22:46:17,386 - INFO - step: 550,loss: 0.248 accuracy: 0.844\n",
      "2017-08-13 22:46:17,449 - INFO - step: 551,loss: 0.376 accuracy: 0.781\n",
      "2017-08-13 22:46:17,512 - INFO - step: 552,loss: 0.207 accuracy: 0.906\n",
      "2017-08-13 22:46:17,576 - INFO - step: 553,loss: 0.242 accuracy: 0.891\n",
      "2017-08-13 22:46:17,643 - INFO - step: 554,loss: 0.229 accuracy: 0.906\n",
      "2017-08-13 22:46:17,705 - INFO - step: 555,loss: 0.285 accuracy: 0.906\n",
      "2017-08-13 22:46:17,769 - INFO - step: 556,loss: 0.197 accuracy: 0.922\n",
      "2017-08-13 22:46:17,832 - INFO - step: 557,loss: 0.288 accuracy: 0.891\n",
      "2017-08-13 22:46:17,895 - INFO - step: 558,loss: 0.299 accuracy: 0.938\n",
      "2017-08-13 22:46:17,958 - INFO - step: 559,loss: 0.215 accuracy: 0.875\n",
      "2017-08-13 22:46:18,024 - INFO - step: 560,loss: 0.264 accuracy: 0.891\n",
      "2017-08-13 22:46:18,088 - INFO - step: 561,loss: 0.250 accuracy: 0.891\n",
      "2017-08-13 22:46:18,151 - INFO - step: 562,loss: 0.180 accuracy: 0.984\n",
      "2017-08-13 22:46:18,213 - INFO - step: 563,loss: 0.312 accuracy: 0.875\n",
      "2017-08-13 22:46:18,277 - INFO - step: 564,loss: 0.183 accuracy: 0.906\n",
      "2017-08-13 22:46:18,340 - INFO - step: 565,loss: 0.177 accuracy: 0.938\n",
      "2017-08-13 22:46:18,403 - INFO - step: 566,loss: 0.330 accuracy: 0.891\n",
      "2017-08-13 22:46:18,469 - INFO - step: 567,loss: 0.287 accuracy: 0.875\n",
      "2017-08-13 22:46:18,533 - INFO - step: 568,loss: 0.404 accuracy: 0.812\n",
      "2017-08-13 22:46:18,597 - INFO - step: 569,loss: 0.297 accuracy: 0.922\n",
      "2017-08-13 22:46:18,660 - INFO - step: 570,loss: 0.352 accuracy: 0.844\n",
      "2017-08-13 22:46:18,724 - INFO - step: 571,loss: 0.231 accuracy: 0.859\n",
      "2017-08-13 22:46:18,786 - INFO - step: 572,loss: 0.249 accuracy: 0.938\n",
      "2017-08-13 22:46:18,849 - INFO - step: 573,loss: 0.297 accuracy: 0.844\n",
      "2017-08-13 22:46:18,925 - INFO - step: 574,loss: 0.258 accuracy: 0.859\n",
      "2017-08-13 22:46:18,988 - INFO - step: 575,loss: 0.349 accuracy: 0.844\n",
      "2017-08-13 22:46:19,011 - INFO - step: 576,loss: 0.461 accuracy: 0.857\n",
      "2017-08-13 22:46:19,013 - INFO - \train_epoch:\n",
      "2017-08-13 22:46:19,015 - INFO - loss_total: 0.269 accuracy_total: 0.884\n",
      "2017-08-13 22:46:19,102 - INFO - step: 577,loss: 0.165 accuracy: 0.969\n",
      "2017-08-13 22:46:19,164 - INFO - step: 578,loss: 0.176 accuracy: 0.938\n",
      "2017-08-13 22:46:19,228 - INFO - step: 579,loss: 0.343 accuracy: 0.844\n",
      "2017-08-13 22:46:19,290 - INFO - step: 580,loss: 0.279 accuracy: 0.891\n",
      "2017-08-13 22:46:19,351 - INFO - step: 581,loss: 0.150 accuracy: 0.938\n",
      "2017-08-13 22:46:19,414 - INFO - step: 582,loss: 0.237 accuracy: 0.906\n",
      "2017-08-13 22:46:19,477 - INFO - step: 583,loss: 0.212 accuracy: 0.906\n",
      "2017-08-13 22:46:19,542 - INFO - step: 584,loss: 0.333 accuracy: 0.859\n",
      "2017-08-13 22:46:19,618 - INFO - step: 585,loss: 0.214 accuracy: 0.938\n",
      "2017-08-13 22:46:19,683 - INFO - step: 586,loss: 0.269 accuracy: 0.891\n",
      "2017-08-13 22:46:19,749 - INFO - step: 587,loss: 0.246 accuracy: 0.922\n",
      "2017-08-13 22:46:19,814 - INFO - step: 588,loss: 0.349 accuracy: 0.859\n",
      "2017-08-13 22:46:19,877 - INFO - step: 589,loss: 0.177 accuracy: 0.922\n",
      "2017-08-13 22:46:19,939 - INFO - step: 590,loss: 0.311 accuracy: 0.891\n",
      "2017-08-13 22:46:20,003 - INFO - step: 591,loss: 0.234 accuracy: 0.906\n",
      "2017-08-13 22:46:20,068 - INFO - step: 592,loss: 0.315 accuracy: 0.859\n",
      "2017-08-13 22:46:20,132 - INFO - step: 593,loss: 0.207 accuracy: 0.922\n",
      "2017-08-13 22:46:20,194 - INFO - step: 594,loss: 0.272 accuracy: 0.844\n",
      "2017-08-13 22:46:20,257 - INFO - step: 595,loss: 0.186 accuracy: 0.938\n",
      "2017-08-13 22:46:20,325 - INFO - step: 596,loss: 0.251 accuracy: 0.875\n",
      "2017-08-13 22:46:20,387 - INFO - step: 597,loss: 0.217 accuracy: 0.906\n",
      "2017-08-13 22:46:20,450 - INFO - step: 598,loss: 0.292 accuracy: 0.891\n",
      "2017-08-13 22:46:20,514 - INFO - step: 599,loss: 0.250 accuracy: 0.953\n",
      "2017-08-13 22:46:20,577 - INFO - step: 600,loss: 0.246 accuracy: 0.938\n",
      "2017-08-13 22:46:20,643 - INFO - step: 601,loss: 0.324 accuracy: 0.859\n",
      "2017-08-13 22:46:20,705 - INFO - step: 602,loss: 0.471 accuracy: 0.797\n",
      "2017-08-13 22:46:20,767 - INFO - step: 603,loss: 0.252 accuracy: 0.875\n",
      "2017-08-13 22:46:20,830 - INFO - step: 604,loss: 0.210 accuracy: 0.922\n",
      "2017-08-13 22:46:20,894 - INFO - step: 605,loss: 0.283 accuracy: 0.844\n",
      "2017-08-13 22:46:20,958 - INFO - step: 606,loss: 0.208 accuracy: 0.906\n",
      "2017-08-13 22:46:21,021 - INFO - step: 607,loss: 0.382 accuracy: 0.859\n",
      "2017-08-13 22:46:21,083 - INFO - step: 608,loss: 0.208 accuracy: 0.891\n",
      "2017-08-13 22:46:21,145 - INFO - step: 609,loss: 0.359 accuracy: 0.828\n",
      "2017-08-13 22:46:21,224 - INFO - step: 610,loss: 0.201 accuracy: 0.922\n",
      "2017-08-13 22:46:21,288 - INFO - step: 611,loss: 0.255 accuracy: 0.875\n",
      "2017-08-13 22:46:21,311 - INFO - step: 612,loss: 0.098 accuracy: 1.000\n",
      "2017-08-13 22:46:21,313 - INFO - \n",
      "Evaluation:\n",
      "2017-08-13 22:46:21,453 - INFO - step: 612,loss: 0.226 accuracy: 0.908\n",
      "2017-08-13 22:46:21,561 - INFO - Saving model to /home/zx/cuckoo_1000/session_save/checkpoints/model-612 at step 16.\n",
      "2017-08-13 22:46:21,564 - INFO - \train_epoch:\n",
      "2017-08-13 22:46:21,565 - INFO - loss_total: 0.255 accuracy_total: 0.897\n",
      "2017-08-13 22:46:21,642 - INFO - step: 613,loss: 0.181 accuracy: 0.938\n",
      "2017-08-13 22:46:21,706 - INFO - step: 614,loss: 0.133 accuracy: 0.969\n",
      "2017-08-13 22:46:21,769 - INFO - step: 615,loss: 0.242 accuracy: 0.906\n",
      "2017-08-13 22:46:21,831 - INFO - step: 616,loss: 0.267 accuracy: 0.891\n",
      "2017-08-13 22:46:21,893 - INFO - step: 617,loss: 0.249 accuracy: 0.906\n",
      "2017-08-13 22:46:21,957 - INFO - step: 618,loss: 0.393 accuracy: 0.797\n",
      "2017-08-13 22:46:22,017 - INFO - step: 619,loss: 0.283 accuracy: 0.875\n",
      "2017-08-13 22:46:22,078 - INFO - step: 620,loss: 0.149 accuracy: 0.938\n",
      "2017-08-13 22:46:22,142 - INFO - step: 621,loss: 0.204 accuracy: 0.922\n",
      "2017-08-13 22:46:22,206 - INFO - step: 622,loss: 0.189 accuracy: 0.953\n",
      "2017-08-13 22:46:22,269 - INFO - step: 623,loss: 0.228 accuracy: 0.906\n",
      "2017-08-13 22:46:22,332 - INFO - step: 624,loss: 0.281 accuracy: 0.922\n",
      "2017-08-13 22:46:22,394 - INFO - step: 625,loss: 0.248 accuracy: 0.922\n",
      "2017-08-13 22:46:22,457 - INFO - step: 626,loss: 0.218 accuracy: 0.906\n",
      "2017-08-13 22:46:22,521 - INFO - step: 627,loss: 0.320 accuracy: 0.859\n",
      "2017-08-13 22:46:22,584 - INFO - step: 628,loss: 0.237 accuracy: 0.906\n",
      "2017-08-13 22:46:22,648 - INFO - step: 629,loss: 0.235 accuracy: 0.859\n",
      "2017-08-13 22:46:22,710 - INFO - step: 630,loss: 0.255 accuracy: 0.891\n",
      "2017-08-13 22:46:22,779 - INFO - step: 631,loss: 0.149 accuracy: 0.969\n",
      "2017-08-13 22:46:22,841 - INFO - step: 632,loss: 0.312 accuracy: 0.859\n",
      "2017-08-13 22:46:22,904 - INFO - step: 633,loss: 0.208 accuracy: 0.938\n",
      "2017-08-13 22:46:22,969 - INFO - step: 634,loss: 0.331 accuracy: 0.859\n",
      "2017-08-13 22:46:23,032 - INFO - step: 635,loss: 0.385 accuracy: 0.844\n",
      "2017-08-13 22:46:23,095 - INFO - step: 636,loss: 0.190 accuracy: 0.938\n",
      "2017-08-13 22:46:23,161 - INFO - step: 637,loss: 0.234 accuracy: 0.891\n",
      "2017-08-13 22:46:23,225 - INFO - step: 638,loss: 0.298 accuracy: 0.891\n",
      "2017-08-13 22:46:23,289 - INFO - step: 639,loss: 0.306 accuracy: 0.875\n",
      "2017-08-13 22:46:23,351 - INFO - step: 640,loss: 0.234 accuracy: 0.938\n",
      "2017-08-13 22:46:23,414 - INFO - step: 641,loss: 0.366 accuracy: 0.828\n",
      "2017-08-13 22:46:23,477 - INFO - step: 642,loss: 0.235 accuracy: 0.938\n",
      "2017-08-13 22:46:23,545 - INFO - step: 643,loss: 0.147 accuracy: 0.969\n",
      "2017-08-13 22:46:23,607 - INFO - step: 644,loss: 0.218 accuracy: 0.906\n",
      "2017-08-13 22:46:23,670 - INFO - step: 645,loss: 0.265 accuracy: 0.875\n",
      "2017-08-13 22:46:23,733 - INFO - step: 646,loss: 0.260 accuracy: 0.906\n",
      "2017-08-13 22:46:23,797 - INFO - step: 647,loss: 0.308 accuracy: 0.859\n",
      "2017-08-13 22:46:23,819 - INFO - step: 648,loss: 0.079 accuracy: 1.000\n",
      "2017-08-13 22:46:23,821 - INFO - \train_epoch:\n",
      "2017-08-13 22:46:23,823 - INFO - loss_total: 0.245 accuracy_total: 0.904\n",
      "2017-08-13 22:46:23,907 - INFO - step: 649,loss: 0.256 accuracy: 0.891\n",
      "2017-08-13 22:46:23,972 - INFO - step: 650,loss: 0.249 accuracy: 0.891\n",
      "2017-08-13 22:46:24,035 - INFO - step: 651,loss: 0.171 accuracy: 0.922\n",
      "2017-08-13 22:46:24,099 - INFO - step: 652,loss: 0.295 accuracy: 0.875\n",
      "2017-08-13 22:46:24,162 - INFO - step: 653,loss: 0.274 accuracy: 0.875\n",
      "2017-08-13 22:46:24,225 - INFO - step: 654,loss: 0.238 accuracy: 0.859\n",
      "2017-08-13 22:46:24,288 - INFO - step: 655,loss: 0.256 accuracy: 0.922\n",
      "2017-08-13 22:46:24,351 - INFO - step: 656,loss: 0.236 accuracy: 0.938\n",
      "2017-08-13 22:46:24,413 - INFO - step: 657,loss: 0.227 accuracy: 0.906\n",
      "2017-08-13 22:46:24,477 - INFO - step: 658,loss: 0.264 accuracy: 0.891\n",
      "2017-08-13 22:46:24,540 - INFO - step: 659,loss: 0.261 accuracy: 0.906\n",
      "2017-08-13 22:46:24,605 - INFO - step: 660,loss: 0.257 accuracy: 0.875\n",
      "2017-08-13 22:46:24,670 - INFO - step: 661,loss: 0.139 accuracy: 0.969\n",
      "2017-08-13 22:46:24,732 - INFO - step: 662,loss: 0.276 accuracy: 0.859\n",
      "2017-08-13 22:46:24,796 - INFO - step: 663,loss: 0.349 accuracy: 0.797\n",
      "2017-08-13 22:46:24,860 - INFO - step: 664,loss: 0.195 accuracy: 0.938\n",
      "2017-08-13 22:46:24,923 - INFO - step: 665,loss: 0.338 accuracy: 0.859\n",
      "2017-08-13 22:46:24,986 - INFO - step: 666,loss: 0.196 accuracy: 0.922\n",
      "2017-08-13 22:46:25,050 - INFO - step: 667,loss: 0.291 accuracy: 0.875\n",
      "2017-08-13 22:46:25,113 - INFO - step: 668,loss: 0.174 accuracy: 0.922\n",
      "2017-08-13 22:46:25,178 - INFO - step: 669,loss: 0.169 accuracy: 0.891\n",
      "2017-08-13 22:46:25,244 - INFO - step: 670,loss: 0.147 accuracy: 0.953\n",
      "2017-08-13 22:46:25,308 - INFO - step: 671,loss: 0.267 accuracy: 0.938\n",
      "2017-08-13 22:46:25,372 - INFO - step: 672,loss: 0.203 accuracy: 0.906\n",
      "2017-08-13 22:46:25,435 - INFO - step: 673,loss: 0.322 accuracy: 0.922\n",
      "2017-08-13 22:46:25,499 - INFO - step: 674,loss: 0.287 accuracy: 0.828\n",
      "2017-08-13 22:46:25,564 - INFO - step: 675,loss: 0.177 accuracy: 0.922\n",
      "2017-08-13 22:46:25,626 - INFO - step: 676,loss: 0.359 accuracy: 0.859\n",
      "2017-08-13 22:46:25,689 - INFO - step: 677,loss: 0.220 accuracy: 0.906\n",
      "2017-08-13 22:46:25,752 - INFO - step: 678,loss: 0.397 accuracy: 0.828\n",
      "2017-08-13 22:46:25,816 - INFO - step: 679,loss: 0.209 accuracy: 0.906\n",
      "2017-08-13 22:46:25,878 - INFO - step: 680,loss: 0.188 accuracy: 0.938\n",
      "2017-08-13 22:46:25,941 - INFO - step: 681,loss: 0.249 accuracy: 0.891\n",
      "2017-08-13 22:46:26,005 - INFO - step: 682,loss: 0.321 accuracy: 0.906\n",
      "2017-08-13 22:46:26,068 - INFO - step: 683,loss: 0.134 accuracy: 0.953\n",
      "2017-08-13 22:46:26,091 - INFO - step: 684,loss: 0.154 accuracy: 1.000\n",
      "2017-08-13 22:46:26,093 - INFO - \n",
      "Evaluation:\n",
      "2017-08-13 22:46:26,179 - INFO - step: 684,loss: 0.223 accuracy: 0.900\n",
      "2017-08-13 22:46:26,181 - INFO - \train_epoch:\n",
      "2017-08-13 22:46:26,195 - INFO - loss_total: 0.243 accuracy_total: 0.901\n",
      "2017-08-13 22:46:26,281 - INFO - step: 685,loss: 0.198 accuracy: 0.906\n",
      "2017-08-13 22:46:26,345 - INFO - step: 686,loss: 0.289 accuracy: 0.891\n",
      "2017-08-13 22:46:26,408 - INFO - step: 687,loss: 0.276 accuracy: 0.922\n",
      "2017-08-13 22:46:26,471 - INFO - step: 688,loss: 0.220 accuracy: 0.859\n",
      "2017-08-13 22:46:26,535 - INFO - step: 689,loss: 0.115 accuracy: 0.984\n",
      "2017-08-13 22:46:26,599 - INFO - step: 690,loss: 0.164 accuracy: 0.922\n",
      "2017-08-13 22:46:26,663 - INFO - step: 691,loss: 0.346 accuracy: 0.828\n",
      "2017-08-13 22:46:26,727 - INFO - step: 692,loss: 0.202 accuracy: 0.891\n",
      "2017-08-13 22:46:26,790 - INFO - step: 693,loss: 0.291 accuracy: 0.922\n",
      "2017-08-13 22:46:26,855 - INFO - step: 694,loss: 0.366 accuracy: 0.859\n",
      "2017-08-13 22:46:26,921 - INFO - step: 695,loss: 0.191 accuracy: 0.906\n",
      "2017-08-13 22:46:26,983 - INFO - step: 696,loss: 0.220 accuracy: 0.906\n",
      "2017-08-13 22:46:27,046 - INFO - step: 697,loss: 0.208 accuracy: 0.938\n",
      "2017-08-13 22:46:27,110 - INFO - step: 698,loss: 0.209 accuracy: 0.875\n",
      "2017-08-13 22:46:27,174 - INFO - step: 699,loss: 0.323 accuracy: 0.828\n",
      "2017-08-13 22:46:27,237 - INFO - step: 700,loss: 0.210 accuracy: 0.938\n",
      "2017-08-13 22:46:27,299 - INFO - step: 701,loss: 0.155 accuracy: 0.953\n",
      "2017-08-13 22:46:27,364 - INFO - step: 702,loss: 0.210 accuracy: 0.906\n",
      "2017-08-13 22:46:27,427 - INFO - step: 703,loss: 0.269 accuracy: 0.844\n",
      "2017-08-13 22:46:27,490 - INFO - step: 704,loss: 0.154 accuracy: 0.938\n",
      "2017-08-13 22:46:27,554 - INFO - step: 705,loss: 0.278 accuracy: 0.891\n",
      "2017-08-13 22:46:27,614 - INFO - step: 706,loss: 0.224 accuracy: 0.906\n",
      "2017-08-13 22:46:27,677 - INFO - step: 707,loss: 0.220 accuracy: 0.859\n",
      "2017-08-13 22:46:27,740 - INFO - step: 708,loss: 0.243 accuracy: 0.906\n",
      "2017-08-13 22:46:27,804 - INFO - step: 709,loss: 0.243 accuracy: 0.938\n",
      "2017-08-13 22:46:27,869 - INFO - step: 710,loss: 0.292 accuracy: 0.844\n",
      "2017-08-13 22:46:27,931 - INFO - step: 711,loss: 0.263 accuracy: 0.875\n",
      "2017-08-13 22:46:27,995 - INFO - step: 712,loss: 0.336 accuracy: 0.859\n",
      "2017-08-13 22:46:28,057 - INFO - step: 713,loss: 0.183 accuracy: 0.906\n",
      "2017-08-13 22:46:28,121 - INFO - step: 714,loss: 0.258 accuracy: 0.875\n",
      "2017-08-13 22:46:28,184 - INFO - step: 715,loss: 0.153 accuracy: 0.969\n",
      "2017-08-13 22:46:28,247 - INFO - step: 716,loss: 0.201 accuracy: 0.906\n",
      "2017-08-13 22:46:28,310 - INFO - step: 717,loss: 0.456 accuracy: 0.859\n",
      "2017-08-13 22:46:28,373 - INFO - step: 718,loss: 0.215 accuracy: 0.891\n",
      "2017-08-13 22:46:28,438 - INFO - step: 719,loss: 0.209 accuracy: 0.891\n",
      "2017-08-13 22:46:28,462 - INFO - step: 720,loss: 0.036 accuracy: 1.000\n",
      "2017-08-13 22:46:28,465 - INFO - \train_epoch:\n",
      "2017-08-13 22:46:28,467 - INFO - loss_total: 0.234 accuracy_total: 0.900\n",
      "2017-08-13 22:46:28,552 - INFO - step: 721,loss: 0.149 accuracy: 0.953\n",
      "2017-08-13 22:46:28,615 - INFO - step: 722,loss: 0.358 accuracy: 0.844\n",
      "2017-08-13 22:46:28,680 - INFO - step: 723,loss: 0.183 accuracy: 0.922\n",
      "2017-08-13 22:46:28,747 - INFO - step: 724,loss: 0.247 accuracy: 0.891\n",
      "2017-08-13 22:46:28,812 - INFO - step: 725,loss: 0.151 accuracy: 0.969\n",
      "2017-08-13 22:46:28,873 - INFO - step: 726,loss: 0.227 accuracy: 0.891\n",
      "2017-08-13 22:46:28,935 - INFO - step: 727,loss: 0.202 accuracy: 0.891\n",
      "2017-08-13 22:46:28,998 - INFO - step: 728,loss: 0.263 accuracy: 0.891\n",
      "2017-08-13 22:46:29,061 - INFO - step: 729,loss: 0.121 accuracy: 0.969\n",
      "2017-08-13 22:46:29,127 - INFO - step: 730,loss: 0.307 accuracy: 0.844\n",
      "2017-08-13 22:46:29,190 - INFO - step: 731,loss: 0.236 accuracy: 0.875\n",
      "2017-08-13 22:46:29,253 - INFO - step: 732,loss: 0.251 accuracy: 0.891\n",
      "2017-08-13 22:46:29,317 - INFO - step: 733,loss: 0.288 accuracy: 0.938\n",
      "2017-08-13 22:46:29,380 - INFO - step: 734,loss: 0.249 accuracy: 0.906\n",
      "2017-08-13 22:46:29,445 - INFO - step: 735,loss: 0.247 accuracy: 0.875\n",
      "2017-08-13 22:46:29,508 - INFO - step: 736,loss: 0.217 accuracy: 0.891\n",
      "2017-08-13 22:46:29,571 - INFO - step: 737,loss: 0.232 accuracy: 0.922\n",
      "2017-08-13 22:46:29,633 - INFO - step: 738,loss: 0.192 accuracy: 0.922\n",
      "2017-08-13 22:46:29,696 - INFO - step: 739,loss: 0.395 accuracy: 0.875\n",
      "2017-08-13 22:46:29,777 - INFO - step: 740,loss: 0.199 accuracy: 0.891\n",
      "2017-08-13 22:46:29,841 - INFO - step: 741,loss: 0.191 accuracy: 0.922\n",
      "2017-08-13 22:46:29,905 - INFO - step: 742,loss: 0.261 accuracy: 0.891\n",
      "2017-08-13 22:46:29,968 - INFO - step: 743,loss: 0.333 accuracy: 0.828\n",
      "2017-08-13 22:46:30,030 - INFO - step: 744,loss: 0.337 accuracy: 0.844\n",
      "2017-08-13 22:46:30,094 - INFO - step: 745,loss: 0.312 accuracy: 0.875\n",
      "2017-08-13 22:46:30,155 - INFO - step: 746,loss: 0.244 accuracy: 0.891\n",
      "2017-08-13 22:46:30,219 - INFO - step: 747,loss: 0.190 accuracy: 0.922\n",
      "2017-08-13 22:46:30,280 - INFO - step: 748,loss: 0.127 accuracy: 0.953\n",
      "2017-08-13 22:46:30,344 - INFO - step: 749,loss: 0.291 accuracy: 0.844\n",
      "2017-08-13 22:46:30,408 - INFO - step: 750,loss: 0.277 accuracy: 0.891\n",
      "2017-08-13 22:46:30,476 - INFO - step: 751,loss: 0.185 accuracy: 0.922\n",
      "2017-08-13 22:46:30,539 - INFO - step: 752,loss: 0.200 accuracy: 0.906\n",
      "2017-08-13 22:46:30,602 - INFO - step: 753,loss: 0.258 accuracy: 0.906\n",
      "2017-08-13 22:46:30,665 - INFO - step: 754,loss: 0.199 accuracy: 0.922\n",
      "2017-08-13 22:46:30,729 - INFO - step: 755,loss: 0.141 accuracy: 0.922\n",
      "2017-08-13 22:46:30,752 - INFO - step: 756,loss: 0.256 accuracy: 0.857\n",
      "2017-08-13 22:46:30,754 - INFO - \n",
      "Evaluation:\n",
      "2017-08-13 22:46:30,838 - INFO - step: 756,loss: 0.219 accuracy: 0.904\n",
      "2017-08-13 22:46:30,840 - INFO - \train_epoch:\n",
      "2017-08-13 22:46:30,841 - INFO - loss_total: 0.237 accuracy_total: 0.898\n",
      "2017-08-13 22:46:30,925 - INFO - step: 757,loss: 0.199 accuracy: 0.922\n",
      "2017-08-13 22:46:30,988 - INFO - step: 758,loss: 0.206 accuracy: 0.922\n",
      "2017-08-13 22:46:31,048 - INFO - step: 759,loss: 0.227 accuracy: 0.891\n",
      "2017-08-13 22:46:31,113 - INFO - step: 760,loss: 0.185 accuracy: 0.906\n",
      "2017-08-13 22:46:31,173 - INFO - step: 761,loss: 0.230 accuracy: 0.922\n",
      "2017-08-13 22:46:31,237 - INFO - step: 762,loss: 0.174 accuracy: 0.953\n",
      "2017-08-13 22:46:31,311 - INFO - step: 763,loss: 0.215 accuracy: 0.891\n",
      "2017-08-13 22:46:31,373 - INFO - step: 764,loss: 0.110 accuracy: 0.938\n",
      "2017-08-13 22:46:31,436 - INFO - step: 765,loss: 0.222 accuracy: 0.906\n",
      "2017-08-13 22:46:31,499 - INFO - step: 766,loss: 0.196 accuracy: 0.906\n",
      "2017-08-13 22:46:31,563 - INFO - step: 767,loss: 0.316 accuracy: 0.844\n",
      "2017-08-13 22:46:31,625 - INFO - step: 768,loss: 0.209 accuracy: 0.891\n",
      "2017-08-13 22:46:31,690 - INFO - step: 769,loss: 0.165 accuracy: 0.938\n",
      "2017-08-13 22:46:31,753 - INFO - step: 770,loss: 0.173 accuracy: 0.922\n",
      "2017-08-13 22:46:31,816 - INFO - step: 771,loss: 0.181 accuracy: 0.938\n",
      "2017-08-13 22:46:31,879 - INFO - step: 772,loss: 0.166 accuracy: 0.906\n",
      "2017-08-13 22:46:31,942 - INFO - step: 773,loss: 0.292 accuracy: 0.859\n",
      "2017-08-13 22:46:32,006 - INFO - step: 774,loss: 0.220 accuracy: 0.906\n",
      "2017-08-13 22:46:32,067 - INFO - step: 775,loss: 0.168 accuracy: 0.938\n",
      "2017-08-13 22:46:32,130 - INFO - step: 776,loss: 0.117 accuracy: 0.984\n",
      "2017-08-13 22:46:32,194 - INFO - step: 777,loss: 0.144 accuracy: 0.938\n",
      "2017-08-13 22:46:32,258 - INFO - step: 778,loss: 0.263 accuracy: 0.891\n",
      "2017-08-13 22:46:32,325 - INFO - step: 779,loss: 0.291 accuracy: 0.906\n",
      "2017-08-13 22:46:32,387 - INFO - step: 780,loss: 0.230 accuracy: 0.922\n",
      "2017-08-13 22:46:32,451 - INFO - step: 781,loss: 0.143 accuracy: 0.938\n",
      "2017-08-13 22:46:32,514 - INFO - step: 782,loss: 0.272 accuracy: 0.906\n",
      "2017-08-13 22:46:32,577 - INFO - step: 783,loss: 0.257 accuracy: 0.859\n",
      "2017-08-13 22:46:32,639 - INFO - step: 784,loss: 0.172 accuracy: 0.922\n",
      "2017-08-13 22:46:32,703 - INFO - step: 785,loss: 0.153 accuracy: 0.953\n",
      "2017-08-13 22:46:32,768 - INFO - step: 786,loss: 0.249 accuracy: 0.922\n",
      "2017-08-13 22:46:32,833 - INFO - step: 787,loss: 0.270 accuracy: 0.891\n",
      "2017-08-13 22:46:32,897 - INFO - step: 788,loss: 0.317 accuracy: 0.859\n",
      "2017-08-13 22:46:32,959 - INFO - step: 789,loss: 0.218 accuracy: 0.922\n",
      "2017-08-13 22:46:33,021 - INFO - step: 790,loss: 0.339 accuracy: 0.891\n",
      "2017-08-13 22:46:33,083 - INFO - step: 791,loss: 0.192 accuracy: 0.953\n",
      "2017-08-13 22:46:33,106 - INFO - step: 792,loss: 0.328 accuracy: 0.857\n",
      "2017-08-13 22:46:33,108 - INFO - \train_epoch:\n",
      "2017-08-13 22:46:33,110 - INFO - loss_total: 0.217 accuracy_total: 0.911\n",
      "2017-08-13 22:46:33,193 - INFO - step: 793,loss: 0.298 accuracy: 0.875\n",
      "2017-08-13 22:46:33,257 - INFO - step: 794,loss: 0.159 accuracy: 0.938\n",
      "2017-08-13 22:46:33,319 - INFO - step: 795,loss: 0.238 accuracy: 0.922\n",
      "2017-08-13 22:46:33,380 - INFO - step: 796,loss: 0.227 accuracy: 0.922\n",
      "2017-08-13 22:46:33,442 - INFO - step: 797,loss: 0.277 accuracy: 0.875\n",
      "2017-08-13 22:46:33,505 - INFO - step: 798,loss: 0.162 accuracy: 0.922\n",
      "2017-08-13 22:46:33,566 - INFO - step: 799,loss: 0.183 accuracy: 0.938\n",
      "2017-08-13 22:46:33,629 - INFO - step: 800,loss: 0.253 accuracy: 0.922\n",
      "2017-08-13 22:46:33,693 - INFO - step: 801,loss: 0.280 accuracy: 0.906\n",
      "2017-08-13 22:46:33,755 - INFO - step: 802,loss: 0.218 accuracy: 0.891\n",
      "2017-08-13 22:46:33,817 - INFO - step: 803,loss: 0.266 accuracy: 0.922\n",
      "2017-08-13 22:46:33,881 - INFO - step: 804,loss: 0.254 accuracy: 0.922\n",
      "2017-08-13 22:46:33,947 - INFO - step: 805,loss: 0.150 accuracy: 0.938\n",
      "2017-08-13 22:46:34,009 - INFO - step: 806,loss: 0.249 accuracy: 0.906\n",
      "2017-08-13 22:46:34,072 - INFO - step: 807,loss: 0.203 accuracy: 0.906\n",
      "2017-08-13 22:46:34,136 - INFO - step: 808,loss: 0.172 accuracy: 0.953\n",
      "2017-08-13 22:46:34,200 - INFO - step: 809,loss: 0.325 accuracy: 0.859\n",
      "2017-08-13 22:46:34,264 - INFO - step: 810,loss: 0.224 accuracy: 0.906\n",
      "2017-08-13 22:46:34,345 - INFO - step: 811,loss: 0.206 accuracy: 0.906\n",
      "2017-08-13 22:46:34,408 - INFO - step: 812,loss: 0.171 accuracy: 0.969\n",
      "2017-08-13 22:46:34,472 - INFO - step: 813,loss: 0.230 accuracy: 0.906\n",
      "2017-08-13 22:46:34,537 - INFO - step: 814,loss: 0.158 accuracy: 0.922\n",
      "2017-08-13 22:46:34,599 - INFO - step: 815,loss: 0.174 accuracy: 0.922\n",
      "2017-08-13 22:46:34,661 - INFO - step: 816,loss: 0.266 accuracy: 0.844\n",
      "2017-08-13 22:46:34,724 - INFO - step: 817,loss: 0.097 accuracy: 1.000\n",
      "2017-08-13 22:46:34,786 - INFO - step: 818,loss: 0.191 accuracy: 0.922\n",
      "2017-08-13 22:46:34,850 - INFO - step: 819,loss: 0.141 accuracy: 0.922\n",
      "2017-08-13 22:46:34,913 - INFO - step: 820,loss: 0.224 accuracy: 0.922\n",
      "2017-08-13 22:46:34,976 - INFO - step: 821,loss: 0.266 accuracy: 0.875\n",
      "2017-08-13 22:46:35,037 - INFO - step: 822,loss: 0.240 accuracy: 0.906\n",
      "2017-08-13 22:46:35,101 - INFO - step: 823,loss: 0.079 accuracy: 1.000\n",
      "2017-08-13 22:46:35,163 - INFO - step: 824,loss: 0.131 accuracy: 0.969\n",
      "2017-08-13 22:46:35,227 - INFO - step: 825,loss: 0.333 accuracy: 0.844\n",
      "2017-08-13 22:46:35,290 - INFO - step: 826,loss: 0.226 accuracy: 0.859\n",
      "2017-08-13 22:46:35,352 - INFO - step: 827,loss: 0.156 accuracy: 0.969\n",
      "2017-08-13 22:46:35,374 - INFO - step: 828,loss: 0.413 accuracy: 0.714\n",
      "2017-08-13 22:46:35,376 - INFO - \n",
      "Evaluation:\n",
      "2017-08-13 22:46:35,518 - INFO - step: 828,loss: 0.223 accuracy: 0.884\n",
      "2017-08-13 22:46:35,519 - INFO - \train_epoch:\n",
      "2017-08-13 22:46:35,521 - INFO - loss_total: 0.218 accuracy_total: 0.911\n",
      "2017-08-13 22:46:35,592 - INFO - step: 829,loss: 0.207 accuracy: 0.938\n",
      "2017-08-13 22:46:35,656 - INFO - step: 830,loss: 0.272 accuracy: 0.891\n",
      "2017-08-13 22:46:35,733 - INFO - step: 831,loss: 0.179 accuracy: 0.922\n",
      "2017-08-13 22:46:35,797 - INFO - step: 832,loss: 0.136 accuracy: 0.953\n",
      "2017-08-13 22:46:35,861 - INFO - step: 833,loss: 0.246 accuracy: 0.938\n",
      "2017-08-13 22:46:35,924 - INFO - step: 834,loss: 0.215 accuracy: 0.922\n",
      "2017-08-13 22:46:35,988 - INFO - step: 835,loss: 0.201 accuracy: 0.938\n",
      "2017-08-13 22:46:36,051 - INFO - step: 836,loss: 0.268 accuracy: 0.891\n",
      "2017-08-13 22:46:36,114 - INFO - step: 837,loss: 0.266 accuracy: 0.891\n",
      "2017-08-13 22:46:36,176 - INFO - step: 838,loss: 0.172 accuracy: 0.938\n",
      "2017-08-13 22:46:36,238 - INFO - step: 839,loss: 0.212 accuracy: 0.891\n",
      "2017-08-13 22:46:36,300 - INFO - step: 840,loss: 0.204 accuracy: 0.922\n",
      "2017-08-13 22:46:36,364 - INFO - step: 841,loss: 0.148 accuracy: 0.969\n",
      "2017-08-13 22:46:36,427 - INFO - step: 842,loss: 0.245 accuracy: 0.891\n",
      "2017-08-13 22:46:36,490 - INFO - step: 843,loss: 0.203 accuracy: 0.922\n",
      "2017-08-13 22:46:36,553 - INFO - step: 844,loss: 0.229 accuracy: 0.938\n",
      "2017-08-13 22:46:36,618 - INFO - step: 845,loss: 0.134 accuracy: 0.969\n",
      "2017-08-13 22:46:36,681 - INFO - step: 846,loss: 0.138 accuracy: 0.953\n",
      "2017-08-13 22:46:36,743 - INFO - step: 847,loss: 0.117 accuracy: 0.969\n",
      "2017-08-13 22:46:36,806 - INFO - step: 848,loss: 0.190 accuracy: 0.938\n",
      "2017-08-13 22:46:36,869 - INFO - step: 849,loss: 0.205 accuracy: 0.906\n",
      "2017-08-13 22:46:36,933 - INFO - step: 850,loss: 0.279 accuracy: 0.875\n",
      "2017-08-13 22:46:36,994 - INFO - step: 851,loss: 0.209 accuracy: 0.906\n",
      "2017-08-13 22:46:37,056 - INFO - step: 852,loss: 0.142 accuracy: 0.969\n",
      "2017-08-13 22:46:37,118 - INFO - step: 853,loss: 0.225 accuracy: 0.906\n",
      "2017-08-13 22:46:37,180 - INFO - step: 854,loss: 0.148 accuracy: 0.953\n",
      "2017-08-13 22:46:37,240 - INFO - step: 855,loss: 0.295 accuracy: 0.844\n",
      "2017-08-13 22:46:37,304 - INFO - step: 856,loss: 0.273 accuracy: 0.891\n",
      "2017-08-13 22:46:37,368 - INFO - step: 857,loss: 0.284 accuracy: 0.875\n",
      "2017-08-13 22:46:37,431 - INFO - step: 858,loss: 0.168 accuracy: 0.938\n",
      "2017-08-13 22:46:37,489 - INFO - step: 859,loss: 0.155 accuracy: 0.938\n",
      "2017-08-13 22:46:37,552 - INFO - step: 860,loss: 0.203 accuracy: 0.906\n",
      "2017-08-13 22:46:37,617 - INFO - step: 861,loss: 0.143 accuracy: 0.938\n",
      "2017-08-13 22:46:37,681 - INFO - step: 862,loss: 0.182 accuracy: 0.922\n",
      "2017-08-13 22:46:37,744 - INFO - step: 863,loss: 0.219 accuracy: 0.906\n",
      "2017-08-13 22:46:37,767 - INFO - step: 864,loss: 0.015 accuracy: 1.000\n",
      "2017-08-13 22:46:37,770 - INFO - \train_epoch:\n",
      "2017-08-13 22:46:37,771 - INFO - loss_total: 0.198 accuracy_total: 0.924\n",
      "2017-08-13 22:46:37,841 - INFO - step: 865,loss: 0.164 accuracy: 0.906\n",
      "2017-08-13 22:46:37,903 - INFO - step: 866,loss: 0.101 accuracy: 0.953\n",
      "2017-08-13 22:46:37,965 - INFO - step: 867,loss: 0.225 accuracy: 0.922\n",
      "2017-08-13 22:46:38,028 - INFO - step: 868,loss: 0.165 accuracy: 0.922\n",
      "2017-08-13 22:46:38,090 - INFO - step: 869,loss: 0.172 accuracy: 0.953\n",
      "2017-08-13 22:46:38,155 - INFO - step: 870,loss: 0.170 accuracy: 0.953\n",
      "2017-08-13 22:46:38,218 - INFO - step: 871,loss: 0.247 accuracy: 0.875\n",
      "2017-08-13 22:46:38,281 - INFO - step: 872,loss: 0.314 accuracy: 0.891\n",
      "2017-08-13 22:46:38,343 - INFO - step: 873,loss: 0.253 accuracy: 0.922\n",
      "2017-08-13 22:46:38,404 - INFO - step: 874,loss: 0.094 accuracy: 0.984\n",
      "2017-08-13 22:46:38,464 - INFO - step: 875,loss: 0.157 accuracy: 0.922\n",
      "2017-08-13 22:46:38,526 - INFO - step: 876,loss: 0.249 accuracy: 0.875\n",
      "2017-08-13 22:46:38,594 - INFO - step: 877,loss: 0.271 accuracy: 0.938\n",
      "2017-08-13 22:46:38,656 - INFO - step: 878,loss: 0.151 accuracy: 0.938\n",
      "2017-08-13 22:46:38,721 - INFO - step: 879,loss: 0.131 accuracy: 0.984\n",
      "2017-08-13 22:46:38,783 - INFO - step: 880,loss: 0.267 accuracy: 0.906\n",
      "2017-08-13 22:46:38,847 - INFO - step: 881,loss: 0.157 accuracy: 0.922\n",
      "2017-08-13 22:46:38,910 - INFO - step: 882,loss: 0.200 accuracy: 0.953\n",
      "2017-08-13 22:46:38,973 - INFO - step: 883,loss: 0.184 accuracy: 0.891\n",
      "2017-08-13 22:46:39,036 - INFO - step: 884,loss: 0.200 accuracy: 0.906\n",
      "2017-08-13 22:46:39,098 - INFO - step: 885,loss: 0.174 accuracy: 0.922\n",
      "2017-08-13 22:46:39,162 - INFO - step: 886,loss: 0.111 accuracy: 0.984\n",
      "2017-08-13 22:46:39,228 - INFO - step: 887,loss: 0.214 accuracy: 0.906\n",
      "2017-08-13 22:46:39,291 - INFO - step: 888,loss: 0.195 accuracy: 0.906\n",
      "2017-08-13 22:46:39,356 - INFO - step: 889,loss: 0.356 accuracy: 0.922\n",
      "2017-08-13 22:46:39,419 - INFO - step: 890,loss: 0.104 accuracy: 0.969\n",
      "2017-08-13 22:46:39,483 - INFO - step: 891,loss: 0.223 accuracy: 0.922\n",
      "2017-08-13 22:46:39,549 - INFO - step: 892,loss: 0.295 accuracy: 0.891\n",
      "2017-08-13 22:46:39,611 - INFO - step: 893,loss: 0.233 accuracy: 0.875\n",
      "2017-08-13 22:46:39,672 - INFO - step: 894,loss: 0.186 accuracy: 0.906\n",
      "2017-08-13 22:46:39,735 - INFO - step: 895,loss: 0.205 accuracy: 0.891\n",
      "2017-08-13 22:46:39,799 - INFO - step: 896,loss: 0.123 accuracy: 0.969\n",
      "2017-08-13 22:46:39,863 - INFO - step: 897,loss: 0.216 accuracy: 0.906\n",
      "2017-08-13 22:46:39,926 - INFO - step: 898,loss: 0.216 accuracy: 0.859\n",
      "2017-08-13 22:46:39,989 - INFO - step: 899,loss: 0.176 accuracy: 0.922\n",
      "2017-08-13 22:46:40,012 - INFO - step: 900,loss: 0.584 accuracy: 0.714\n",
      "2017-08-13 22:46:40,014 - INFO - \n",
      "Evaluation:\n",
      "2017-08-13 22:46:40,095 - INFO - step: 900,loss: 0.214 accuracy: 0.900\n",
      "2017-08-13 22:46:40,098 - INFO - \train_epoch:\n",
      "2017-08-13 22:46:40,099 - INFO - loss_total: 0.208 accuracy_total: 0.916\n",
      "2017-08-13 22:46:40,171 - INFO - step: 901,loss: 0.139 accuracy: 0.906\n",
      "2017-08-13 22:46:40,235 - INFO - step: 902,loss: 0.427 accuracy: 0.875\n",
      "2017-08-13 22:46:40,298 - INFO - step: 903,loss: 0.164 accuracy: 0.969\n",
      "2017-08-13 22:46:40,361 - INFO - step: 904,loss: 0.209 accuracy: 0.922\n",
      "2017-08-13 22:46:40,429 - INFO - step: 905,loss: 0.236 accuracy: 0.938\n",
      "2017-08-13 22:46:40,492 - INFO - step: 906,loss: 0.314 accuracy: 0.906\n",
      "2017-08-13 22:46:40,556 - INFO - step: 907,loss: 0.109 accuracy: 0.969\n",
      "2017-08-13 22:46:40,618 - INFO - step: 908,loss: 0.228 accuracy: 0.875\n",
      "2017-08-13 22:46:40,680 - INFO - step: 909,loss: 0.228 accuracy: 0.906\n",
      "2017-08-13 22:46:40,743 - INFO - step: 910,loss: 0.161 accuracy: 0.938\n",
      "2017-08-13 22:46:40,805 - INFO - step: 911,loss: 0.215 accuracy: 0.922\n",
      "2017-08-13 22:46:40,870 - INFO - step: 912,loss: 0.146 accuracy: 0.938\n",
      "2017-08-13 22:46:40,935 - INFO - step: 913,loss: 0.234 accuracy: 0.891\n",
      "2017-08-13 22:46:40,998 - INFO - step: 914,loss: 0.178 accuracy: 0.953\n",
      "2017-08-13 22:46:41,059 - INFO - step: 915,loss: 0.286 accuracy: 0.859\n",
      "2017-08-13 22:46:41,122 - INFO - step: 916,loss: 0.245 accuracy: 0.859\n",
      "2017-08-13 22:46:41,185 - INFO - step: 917,loss: 0.151 accuracy: 0.953\n",
      "2017-08-13 22:46:41,249 - INFO - step: 918,loss: 0.109 accuracy: 1.000\n",
      "2017-08-13 22:46:41,311 - INFO - step: 919,loss: 0.080 accuracy: 0.953\n",
      "2017-08-13 22:46:41,376 - INFO - step: 920,loss: 0.229 accuracy: 0.891\n",
      "2017-08-13 22:46:41,439 - INFO - step: 921,loss: 0.256 accuracy: 0.922\n",
      "2017-08-13 22:46:41,501 - INFO - step: 922,loss: 0.123 accuracy: 0.953\n",
      "2017-08-13 22:46:41,564 - INFO - step: 923,loss: 0.157 accuracy: 0.938\n",
      "2017-08-13 22:46:41,626 - INFO - step: 924,loss: 0.283 accuracy: 0.859\n",
      "2017-08-13 22:46:41,689 - INFO - step: 925,loss: 0.159 accuracy: 0.953\n",
      "2017-08-13 22:46:41,750 - INFO - step: 926,loss: 0.248 accuracy: 0.906\n",
      "2017-08-13 22:46:41,814 - INFO - step: 927,loss: 0.319 accuracy: 0.938\n",
      "2017-08-13 22:46:41,879 - INFO - step: 928,loss: 0.143 accuracy: 0.953\n",
      "2017-08-13 22:46:41,941 - INFO - step: 929,loss: 0.280 accuracy: 0.891\n",
      "2017-08-13 22:46:42,005 - INFO - step: 930,loss: 0.295 accuracy: 0.812\n",
      "2017-08-13 22:46:42,070 - INFO - step: 931,loss: 0.186 accuracy: 0.938\n",
      "2017-08-13 22:46:42,136 - INFO - step: 932,loss: 0.220 accuracy: 0.891\n",
      "2017-08-13 22:46:42,199 - INFO - step: 933,loss: 0.156 accuracy: 0.938\n",
      "2017-08-13 22:46:42,263 - INFO - step: 934,loss: 0.235 accuracy: 0.906\n",
      "2017-08-13 22:46:42,326 - INFO - step: 935,loss: 0.149 accuracy: 0.938\n",
      "2017-08-13 22:46:42,350 - INFO - step: 936,loss: 0.034 accuracy: 1.000\n",
      "2017-08-13 22:46:42,352 - INFO - \train_epoch:\n",
      "2017-08-13 22:46:42,354 - INFO - loss_total: 0.204 accuracy_total: 0.921\n",
      "2017-08-13 22:46:42,434 - INFO - step: 937,loss: 0.179 accuracy: 0.938\n",
      "2017-08-13 22:46:42,497 - INFO - step: 938,loss: 0.277 accuracy: 0.828\n",
      "2017-08-13 22:46:42,564 - INFO - step: 939,loss: 0.243 accuracy: 0.844\n",
      "2017-08-13 22:46:42,627 - INFO - step: 940,loss: 0.188 accuracy: 0.938\n",
      "2017-08-13 22:46:42,691 - INFO - step: 941,loss: 0.206 accuracy: 0.953\n",
      "2017-08-13 22:46:42,756 - INFO - step: 942,loss: 0.096 accuracy: 0.984\n",
      "2017-08-13 22:46:42,820 - INFO - step: 943,loss: 0.285 accuracy: 0.859\n",
      "2017-08-13 22:46:42,883 - INFO - step: 944,loss: 0.266 accuracy: 0.891\n",
      "2017-08-13 22:46:42,947 - INFO - step: 945,loss: 0.177 accuracy: 0.922\n",
      "2017-08-13 22:46:43,011 - INFO - step: 946,loss: 0.281 accuracy: 0.891\n",
      "2017-08-13 22:46:43,074 - INFO - step: 947,loss: 0.251 accuracy: 0.891\n",
      "2017-08-13 22:46:43,139 - INFO - step: 948,loss: 0.132 accuracy: 0.938\n",
      "2017-08-13 22:46:43,202 - INFO - step: 949,loss: 0.201 accuracy: 0.938\n",
      "2017-08-13 22:46:43,264 - INFO - step: 950,loss: 0.174 accuracy: 0.906\n",
      "2017-08-13 22:46:43,328 - INFO - step: 951,loss: 0.213 accuracy: 0.891\n",
      "2017-08-13 22:46:43,406 - INFO - step: 952,loss: 0.144 accuracy: 0.953\n",
      "2017-08-13 22:46:43,468 - INFO - step: 953,loss: 0.131 accuracy: 0.969\n",
      "2017-08-13 22:46:43,535 - INFO - step: 954,loss: 0.264 accuracy: 0.859\n",
      "2017-08-13 22:46:43,599 - INFO - step: 955,loss: 0.133 accuracy: 0.953\n",
      "2017-08-13 22:46:43,662 - INFO - step: 956,loss: 0.225 accuracy: 0.922\n",
      "2017-08-13 22:46:43,727 - INFO - step: 957,loss: 0.132 accuracy: 0.953\n",
      "2017-08-13 22:46:43,791 - INFO - step: 958,loss: 0.200 accuracy: 0.938\n",
      "2017-08-13 22:46:43,853 - INFO - step: 959,loss: 0.204 accuracy: 0.906\n",
      "2017-08-13 22:46:43,916 - INFO - step: 960,loss: 0.344 accuracy: 0.875\n",
      "2017-08-13 22:46:43,979 - INFO - step: 961,loss: 0.098 accuracy: 0.953\n",
      "2017-08-13 22:46:44,042 - INFO - step: 962,loss: 0.177 accuracy: 0.922\n",
      "2017-08-13 22:46:44,105 - INFO - step: 963,loss: 0.278 accuracy: 0.844\n",
      "2017-08-13 22:46:44,168 - INFO - step: 964,loss: 0.166 accuracy: 0.938\n",
      "2017-08-13 22:46:44,231 - INFO - step: 965,loss: 0.180 accuracy: 0.938\n",
      "2017-08-13 22:46:44,296 - INFO - step: 966,loss: 0.085 accuracy: 0.969\n",
      "2017-08-13 22:46:44,358 - INFO - step: 967,loss: 0.258 accuracy: 0.891\n",
      "2017-08-13 22:46:44,422 - INFO - step: 968,loss: 0.173 accuracy: 0.938\n",
      "2017-08-13 22:46:44,488 - INFO - step: 969,loss: 0.172 accuracy: 0.906\n",
      "2017-08-13 22:46:44,552 - INFO - step: 970,loss: 0.235 accuracy: 0.922\n",
      "2017-08-13 22:46:44,615 - INFO - step: 971,loss: 0.182 accuracy: 0.938\n",
      "2017-08-13 22:46:44,637 - INFO - step: 972,loss: 0.254 accuracy: 0.857\n",
      "2017-08-13 22:46:44,640 - INFO - \n",
      "Evaluation:\n",
      "2017-08-13 22:46:44,702 - INFO - step: 972,loss: 0.210 accuracy: 0.904\n",
      "2017-08-13 22:46:44,704 - INFO - \train_epoch:\n",
      "2017-08-13 22:46:44,705 - INFO - loss_total: 0.200 accuracy_total: 0.915\n",
      "2017-08-13 22:46:44,786 - INFO - step: 973,loss: 0.197 accuracy: 0.922\n",
      "2017-08-13 22:46:44,854 - INFO - step: 974,loss: 0.267 accuracy: 0.891\n",
      "2017-08-13 22:46:44,917 - INFO - step: 975,loss: 0.229 accuracy: 0.891\n",
      "2017-08-13 22:46:44,982 - INFO - step: 976,loss: 0.287 accuracy: 0.938\n",
      "2017-08-13 22:46:45,043 - INFO - step: 977,loss: 0.217 accuracy: 0.906\n",
      "2017-08-13 22:46:45,108 - INFO - step: 978,loss: 0.194 accuracy: 0.938\n",
      "2017-08-13 22:46:45,170 - INFO - step: 979,loss: 0.205 accuracy: 0.922\n",
      "2017-08-13 22:46:45,234 - INFO - step: 980,loss: 0.102 accuracy: 0.984\n",
      "2017-08-13 22:46:45,299 - INFO - step: 981,loss: 0.158 accuracy: 0.953\n",
      "2017-08-13 22:46:45,363 - INFO - step: 982,loss: 0.148 accuracy: 0.953\n",
      "2017-08-13 22:46:45,428 - INFO - step: 983,loss: 0.225 accuracy: 0.938\n",
      "2017-08-13 22:46:45,493 - INFO - step: 984,loss: 0.256 accuracy: 0.859\n",
      "2017-08-13 22:46:45,557 - INFO - step: 985,loss: 0.219 accuracy: 0.922\n",
      "2017-08-13 22:46:45,621 - INFO - step: 986,loss: 0.134 accuracy: 0.984\n",
      "2017-08-13 22:46:45,684 - INFO - step: 987,loss: 0.217 accuracy: 0.891\n",
      "2017-08-13 22:46:45,747 - INFO - step: 988,loss: 0.139 accuracy: 0.969\n",
      "2017-08-13 22:46:45,810 - INFO - step: 989,loss: 0.194 accuracy: 0.922\n",
      "2017-08-13 22:46:45,880 - INFO - step: 990,loss: 0.165 accuracy: 0.922\n",
      "2017-08-13 22:46:45,945 - INFO - step: 991,loss: 0.187 accuracy: 0.953\n",
      "2017-08-13 22:46:46,009 - INFO - step: 992,loss: 0.159 accuracy: 0.938\n",
      "2017-08-13 22:46:46,073 - INFO - step: 993,loss: 0.309 accuracy: 0.812\n",
      "2017-08-13 22:46:46,137 - INFO - step: 994,loss: 0.261 accuracy: 0.906\n",
      "2017-08-13 22:46:46,201 - INFO - step: 995,loss: 0.121 accuracy: 0.953\n",
      "2017-08-13 22:46:46,264 - INFO - step: 996,loss: 0.101 accuracy: 0.969\n",
      "2017-08-13 22:46:46,327 - INFO - step: 997,loss: 0.216 accuracy: 0.891\n",
      "2017-08-13 22:46:46,388 - INFO - step: 998,loss: 0.255 accuracy: 0.875\n",
      "2017-08-13 22:46:46,450 - INFO - step: 999,loss: 0.174 accuracy: 0.938\n",
      "2017-08-13 22:46:46,513 - INFO - step: 1000,loss: 0.200 accuracy: 0.938\n",
      "2017-08-13 22:46:46,577 - INFO - step: 1001,loss: 0.198 accuracy: 0.906\n",
      "2017-08-13 22:46:46,641 - INFO - step: 1002,loss: 0.175 accuracy: 0.906\n",
      "2017-08-13 22:46:46,703 - INFO - step: 1003,loss: 0.107 accuracy: 0.969\n",
      "2017-08-13 22:46:46,765 - INFO - step: 1004,loss: 0.171 accuracy: 0.922\n",
      "2017-08-13 22:46:46,829 - INFO - step: 1005,loss: 0.127 accuracy: 0.938\n",
      "2017-08-13 22:46:46,893 - INFO - step: 1006,loss: 0.232 accuracy: 0.891\n",
      "2017-08-13 22:46:46,957 - INFO - step: 1007,loss: 0.149 accuracy: 0.938\n",
      "2017-08-13 22:46:46,980 - INFO - step: 1008,loss: 0.117 accuracy: 1.000\n",
      "2017-08-13 22:46:46,982 - INFO - \train_epoch:\n",
      "2017-08-13 22:46:46,984 - INFO - loss_total: 0.189 accuracy_total: 0.926\n",
      "2017-08-13 22:46:47,066 - INFO - step: 1009,loss: 0.205 accuracy: 0.906\n",
      "2017-08-13 22:46:47,128 - INFO - step: 1010,loss: 0.143 accuracy: 0.969\n",
      "2017-08-13 22:46:47,191 - INFO - step: 1011,loss: 0.174 accuracy: 0.938\n",
      "2017-08-13 22:46:47,254 - INFO - step: 1012,loss: 0.187 accuracy: 0.922\n",
      "2017-08-13 22:46:47,316 - INFO - step: 1013,loss: 0.187 accuracy: 0.953\n",
      "2017-08-13 22:46:47,380 - INFO - step: 1014,loss: 0.133 accuracy: 0.938\n",
      "2017-08-13 22:46:47,442 - INFO - step: 1015,loss: 0.285 accuracy: 0.906\n",
      "2017-08-13 22:46:47,504 - INFO - step: 1016,loss: 0.201 accuracy: 0.875\n",
      "2017-08-13 22:46:47,568 - INFO - step: 1017,loss: 0.270 accuracy: 0.844\n",
      "2017-08-13 22:46:47,631 - INFO - step: 1018,loss: 0.158 accuracy: 0.938\n",
      "2017-08-13 22:46:47,695 - INFO - step: 1019,loss: 0.238 accuracy: 0.891\n",
      "2017-08-13 22:46:47,758 - INFO - step: 1020,loss: 0.171 accuracy: 0.922\n",
      "2017-08-13 22:46:47,824 - INFO - step: 1021,loss: 0.274 accuracy: 0.859\n",
      "2017-08-13 22:46:47,885 - INFO - step: 1022,loss: 0.334 accuracy: 0.812\n",
      "2017-08-13 22:46:47,946 - INFO - step: 1023,loss: 0.176 accuracy: 0.922\n",
      "2017-08-13 22:46:48,004 - INFO - step: 1024,loss: 0.245 accuracy: 0.859\n",
      "2017-08-13 22:46:48,067 - INFO - step: 1025,loss: 0.212 accuracy: 0.922\n",
      "2017-08-13 22:46:48,131 - INFO - step: 1026,loss: 0.173 accuracy: 0.906\n",
      "2017-08-13 22:46:48,195 - INFO - step: 1027,loss: 0.170 accuracy: 0.953\n",
      "2017-08-13 22:46:48,259 - INFO - step: 1028,loss: 0.204 accuracy: 0.922\n",
      "2017-08-13 22:46:48,322 - INFO - step: 1029,loss: 0.216 accuracy: 0.891\n",
      "2017-08-13 22:46:48,383 - INFO - step: 1030,loss: 0.126 accuracy: 0.969\n",
      "2017-08-13 22:46:48,447 - INFO - step: 1031,loss: 0.256 accuracy: 0.891\n",
      "2017-08-13 22:46:48,511 - INFO - step: 1032,loss: 0.150 accuracy: 0.938\n",
      "2017-08-13 22:46:48,574 - INFO - step: 1033,loss: 0.107 accuracy: 0.969\n",
      "2017-08-13 22:46:48,636 - INFO - step: 1034,loss: 0.176 accuracy: 0.938\n",
      "2017-08-13 22:46:48,699 - INFO - step: 1035,loss: 0.212 accuracy: 0.891\n",
      "2017-08-13 22:46:48,762 - INFO - step: 1036,loss: 0.050 accuracy: 1.000\n",
      "2017-08-13 22:46:48,826 - INFO - step: 1037,loss: 0.175 accuracy: 0.953\n",
      "2017-08-13 22:46:48,889 - INFO - step: 1038,loss: 0.174 accuracy: 0.953\n",
      "2017-08-13 22:46:48,952 - INFO - step: 1039,loss: 0.198 accuracy: 0.906\n",
      "2017-08-13 22:46:49,014 - INFO - step: 1040,loss: 0.101 accuracy: 0.969\n",
      "2017-08-13 22:46:49,076 - INFO - step: 1041,loss: 0.195 accuracy: 0.922\n",
      "2017-08-13 22:46:49,140 - INFO - step: 1042,loss: 0.075 accuracy: 0.984\n",
      "2017-08-13 22:46:49,202 - INFO - step: 1043,loss: 0.227 accuracy: 0.938\n",
      "2017-08-13 22:46:49,225 - INFO - step: 1044,loss: 0.044 accuracy: 1.000\n",
      "2017-08-13 22:46:49,227 - INFO - \n",
      "Evaluation:\n",
      "2017-08-13 22:46:49,368 - INFO - step: 1044,loss: 0.203 accuracy: 0.908\n",
      "2017-08-13 22:46:49,474 - INFO - Saving model to /home/zx/cuckoo_1000/session_save/checkpoints/model-1044 at step 28.\n",
      "2017-08-13 22:46:49,477 - INFO - \train_epoch:\n",
      "2017-08-13 22:46:49,478 - INFO - loss_total: 0.184 accuracy_total: 0.924\n",
      "2017-08-13 22:46:49,548 - INFO - step: 1045,loss: 0.144 accuracy: 0.969\n",
      "2017-08-13 22:46:49,609 - INFO - step: 1046,loss: 0.182 accuracy: 0.922\n",
      "2017-08-13 22:46:49,671 - INFO - step: 1047,loss: 0.177 accuracy: 0.938\n",
      "2017-08-13 22:46:49,734 - INFO - step: 1048,loss: 0.192 accuracy: 0.922\n",
      "2017-08-13 22:46:49,799 - INFO - step: 1049,loss: 0.182 accuracy: 0.953\n",
      "2017-08-13 22:46:49,861 - INFO - step: 1050,loss: 0.180 accuracy: 0.938\n",
      "2017-08-13 22:46:49,925 - INFO - step: 1051,loss: 0.211 accuracy: 0.891\n",
      "2017-08-13 22:46:49,988 - INFO - step: 1052,loss: 0.171 accuracy: 0.922\n",
      "2017-08-13 22:46:50,051 - INFO - step: 1053,loss: 0.214 accuracy: 0.906\n",
      "2017-08-13 22:46:50,113 - INFO - step: 1054,loss: 0.222 accuracy: 0.938\n",
      "2017-08-13 22:46:50,176 - INFO - step: 1055,loss: 0.204 accuracy: 0.891\n",
      "2017-08-13 22:46:50,240 - INFO - step: 1056,loss: 0.153 accuracy: 0.922\n",
      "2017-08-13 22:46:50,303 - INFO - step: 1057,loss: 0.238 accuracy: 0.891\n",
      "2017-08-13 22:46:50,368 - INFO - step: 1058,loss: 0.249 accuracy: 0.891\n",
      "2017-08-13 22:46:50,430 - INFO - step: 1059,loss: 0.109 accuracy: 0.953\n",
      "2017-08-13 22:46:50,487 - INFO - step: 1060,loss: 0.325 accuracy: 0.797\n",
      "2017-08-13 22:46:50,551 - INFO - step: 1061,loss: 0.125 accuracy: 0.953\n",
      "2017-08-13 22:46:50,614 - INFO - step: 1062,loss: 0.127 accuracy: 0.953\n",
      "2017-08-13 22:46:50,679 - INFO - step: 1063,loss: 0.226 accuracy: 0.906\n",
      "2017-08-13 22:46:50,760 - INFO - step: 1064,loss: 0.047 accuracy: 1.000\n",
      "2017-08-13 22:46:50,826 - INFO - step: 1065,loss: 0.199 accuracy: 0.938\n",
      "2017-08-13 22:46:50,889 - INFO - step: 1066,loss: 0.224 accuracy: 0.922\n",
      "2017-08-13 22:46:50,953 - INFO - step: 1067,loss: 0.137 accuracy: 0.938\n",
      "2017-08-13 22:46:51,016 - INFO - step: 1068,loss: 0.169 accuracy: 0.953\n",
      "2017-08-13 22:46:51,079 - INFO - step: 1069,loss: 0.421 accuracy: 0.828\n",
      "2017-08-13 22:46:51,142 - INFO - step: 1070,loss: 0.145 accuracy: 0.938\n",
      "2017-08-13 22:46:51,206 - INFO - step: 1071,loss: 0.183 accuracy: 0.906\n",
      "2017-08-13 22:46:51,271 - INFO - step: 1072,loss: 0.158 accuracy: 0.938\n",
      "2017-08-13 22:46:51,333 - INFO - step: 1073,loss: 0.339 accuracy: 0.922\n",
      "2017-08-13 22:46:51,396 - INFO - step: 1074,loss: 0.175 accuracy: 0.891\n",
      "2017-08-13 22:46:51,458 - INFO - step: 1075,loss: 0.194 accuracy: 0.938\n",
      "2017-08-13 22:46:51,522 - INFO - step: 1076,loss: 0.140 accuracy: 0.953\n",
      "2017-08-13 22:46:51,585 - INFO - step: 1077,loss: 0.239 accuracy: 0.922\n",
      "2017-08-13 22:46:51,648 - INFO - step: 1078,loss: 0.226 accuracy: 0.875\n",
      "2017-08-13 22:46:51,711 - INFO - step: 1079,loss: 0.176 accuracy: 0.906\n",
      "2017-08-13 22:46:51,735 - INFO - step: 1080,loss: 0.106 accuracy: 1.000\n",
      "2017-08-13 22:46:51,738 - INFO - \train_epoch:\n",
      "2017-08-13 22:46:51,740 - INFO - loss_total: 0.192 accuracy_total: 0.923\n",
      "2017-08-13 22:46:51,824 - INFO - step: 1081,loss: 0.172 accuracy: 0.922\n",
      "2017-08-13 22:46:51,887 - INFO - step: 1082,loss: 0.341 accuracy: 0.859\n",
      "2017-08-13 22:46:51,951 - INFO - step: 1083,loss: 0.247 accuracy: 0.906\n",
      "2017-08-13 22:46:52,014 - INFO - step: 1084,loss: 0.175 accuracy: 0.922\n",
      "2017-08-13 22:46:52,076 - INFO - step: 1085,loss: 0.191 accuracy: 0.922\n",
      "2017-08-13 22:46:52,138 - INFO - step: 1086,loss: 0.103 accuracy: 0.969\n",
      "2017-08-13 22:46:52,199 - INFO - step: 1087,loss: 0.135 accuracy: 0.953\n",
      "2017-08-13 22:46:52,263 - INFO - step: 1088,loss: 0.194 accuracy: 0.891\n",
      "2017-08-13 22:46:52,327 - INFO - step: 1089,loss: 0.154 accuracy: 0.938\n",
      "2017-08-13 22:46:52,389 - INFO - step: 1090,loss: 0.192 accuracy: 0.938\n",
      "2017-08-13 22:46:52,452 - INFO - step: 1091,loss: 0.177 accuracy: 0.891\n",
      "2017-08-13 22:46:52,515 - INFO - step: 1092,loss: 0.084 accuracy: 0.969\n",
      "2017-08-13 22:46:52,578 - INFO - step: 1093,loss: 0.177 accuracy: 0.938\n",
      "2017-08-13 22:46:52,640 - INFO - step: 1094,loss: 0.101 accuracy: 0.984\n",
      "2017-08-13 22:46:52,721 - INFO - step: 1095,loss: 0.113 accuracy: 0.969\n",
      "2017-08-13 22:46:52,783 - INFO - step: 1096,loss: 0.123 accuracy: 0.953\n",
      "2017-08-13 22:46:52,848 - INFO - step: 1097,loss: 0.105 accuracy: 0.969\n",
      "2017-08-13 22:46:52,911 - INFO - step: 1098,loss: 0.133 accuracy: 0.953\n",
      "2017-08-13 22:46:52,973 - INFO - step: 1099,loss: 0.138 accuracy: 0.922\n",
      "2017-08-13 22:46:53,036 - INFO - step: 1100,loss: 0.227 accuracy: 0.922\n",
      "2017-08-13 22:46:53,101 - INFO - step: 1101,loss: 0.243 accuracy: 0.891\n",
      "2017-08-13 22:46:53,163 - INFO - step: 1102,loss: 0.095 accuracy: 0.969\n",
      "2017-08-13 22:46:53,228 - INFO - step: 1103,loss: 0.300 accuracy: 0.938\n",
      "2017-08-13 22:46:53,292 - INFO - step: 1104,loss: 0.180 accuracy: 0.922\n",
      "2017-08-13 22:46:53,354 - INFO - step: 1105,loss: 0.263 accuracy: 0.906\n",
      "2017-08-13 22:46:53,416 - INFO - step: 1106,loss: 0.160 accuracy: 0.953\n",
      "2017-08-13 22:46:53,478 - INFO - step: 1107,loss: 0.169 accuracy: 0.938\n",
      "2017-08-13 22:46:53,541 - INFO - step: 1108,loss: 0.096 accuracy: 0.969\n",
      "2017-08-13 22:46:53,604 - INFO - step: 1109,loss: 0.330 accuracy: 0.875\n",
      "2017-08-13 22:46:53,667 - INFO - step: 1110,loss: 0.212 accuracy: 0.922\n",
      "2017-08-13 22:46:53,731 - INFO - step: 1111,loss: 0.187 accuracy: 0.922\n",
      "2017-08-13 22:46:53,788 - INFO - step: 1112,loss: 0.199 accuracy: 0.938\n",
      "2017-08-13 22:46:53,851 - INFO - step: 1113,loss: 0.165 accuracy: 0.922\n",
      "2017-08-13 22:46:53,931 - INFO - step: 1114,loss: 0.161 accuracy: 0.938\n",
      "2017-08-13 22:46:53,993 - INFO - step: 1115,loss: 0.212 accuracy: 0.906\n",
      "2017-08-13 22:46:54,016 - INFO - step: 1116,loss: 0.462 accuracy: 0.714\n",
      "2017-08-13 22:46:54,018 - INFO - \n",
      "Evaluation:\n",
      "2017-08-13 22:46:54,078 - INFO - step: 1116,loss: 0.218 accuracy: 0.896\n",
      "2017-08-13 22:46:54,080 - INFO - \train_epoch:\n",
      "2017-08-13 22:46:54,081 - INFO - loss_total: 0.187 accuracy_total: 0.925\n",
      "2017-08-13 22:46:54,167 - INFO - step: 1117,loss: 0.157 accuracy: 0.922\n",
      "2017-08-13 22:46:54,230 - INFO - step: 1118,loss: 0.191 accuracy: 0.906\n",
      "2017-08-13 22:46:54,292 - INFO - step: 1119,loss: 0.188 accuracy: 0.906\n",
      "2017-08-13 22:46:54,357 - INFO - step: 1120,loss: 0.095 accuracy: 0.953\n",
      "2017-08-13 22:46:54,420 - INFO - step: 1121,loss: 0.229 accuracy: 0.891\n",
      "2017-08-13 22:46:54,486 - INFO - step: 1122,loss: 0.064 accuracy: 0.984\n",
      "2017-08-13 22:46:54,553 - INFO - step: 1123,loss: 0.116 accuracy: 0.969\n",
      "2017-08-13 22:46:54,614 - INFO - step: 1124,loss: 0.187 accuracy: 0.938\n",
      "2017-08-13 22:46:54,681 - INFO - step: 1125,loss: 0.123 accuracy: 0.953\n",
      "2017-08-13 22:46:54,743 - INFO - step: 1126,loss: 0.164 accuracy: 0.938\n",
      "2017-08-13 22:46:54,806 - INFO - step: 1127,loss: 0.094 accuracy: 0.969\n",
      "2017-08-13 22:46:54,871 - INFO - step: 1128,loss: 0.162 accuracy: 0.953\n",
      "2017-08-13 22:46:54,936 - INFO - step: 1129,loss: 0.224 accuracy: 0.891\n",
      "2017-08-13 22:46:55,000 - INFO - step: 1130,loss: 0.151 accuracy: 0.922\n",
      "2017-08-13 22:46:55,061 - INFO - step: 1131,loss: 0.327 accuracy: 0.875\n",
      "2017-08-13 22:46:55,124 - INFO - step: 1132,loss: 0.329 accuracy: 0.875\n",
      "2017-08-13 22:46:55,187 - INFO - step: 1133,loss: 0.183 accuracy: 0.922\n",
      "2017-08-13 22:46:55,251 - INFO - step: 1134,loss: 0.197 accuracy: 0.938\n",
      "2017-08-13 22:46:55,315 - INFO - step: 1135,loss: 0.171 accuracy: 0.938\n",
      "2017-08-13 22:46:55,377 - INFO - step: 1136,loss: 0.270 accuracy: 0.938\n",
      "2017-08-13 22:46:55,450 - INFO - step: 1137,loss: 0.236 accuracy: 0.953\n",
      "2017-08-13 22:46:55,511 - INFO - step: 1138,loss: 0.158 accuracy: 0.938\n",
      "2017-08-13 22:46:55,575 - INFO - step: 1139,loss: 0.240 accuracy: 0.938\n",
      "2017-08-13 22:46:55,640 - INFO - step: 1140,loss: 0.231 accuracy: 0.906\n",
      "2017-08-13 22:46:55,704 - INFO - step: 1141,loss: 0.247 accuracy: 0.875\n",
      "2017-08-13 22:46:55,772 - INFO - step: 1142,loss: 0.099 accuracy: 0.953\n",
      "2017-08-13 22:46:55,836 - INFO - step: 1143,loss: 0.168 accuracy: 0.938\n",
      "2017-08-13 22:46:55,901 - INFO - step: 1144,loss: 0.152 accuracy: 0.922\n",
      "2017-08-13 22:46:55,965 - INFO - step: 1145,loss: 0.186 accuracy: 0.938\n",
      "2017-08-13 22:46:56,024 - INFO - step: 1146,loss: 0.116 accuracy: 0.969\n",
      "2017-08-13 22:46:56,088 - INFO - step: 1147,loss: 0.096 accuracy: 0.984\n",
      "2017-08-13 22:46:56,152 - INFO - step: 1148,loss: 0.197 accuracy: 0.875\n",
      "2017-08-13 22:46:56,222 - INFO - step: 1149,loss: 0.259 accuracy: 0.875\n",
      "2017-08-13 22:46:56,286 - INFO - step: 1150,loss: 0.188 accuracy: 0.906\n",
      "2017-08-13 22:46:56,364 - INFO - step: 1151,loss: 0.171 accuracy: 0.922\n",
      "2017-08-13 22:46:56,387 - INFO - step: 1152,loss: 0.076 accuracy: 1.000\n",
      "2017-08-13 22:46:56,389 - INFO - \train_epoch:\n",
      "2017-08-13 22:46:56,391 - INFO - loss_total: 0.179 accuracy_total: 0.930\n",
      "2017-08-13 22:46:56,476 - INFO - step: 1153,loss: 0.189 accuracy: 0.922\n",
      "2017-08-13 22:46:56,540 - INFO - step: 1154,loss: 0.188 accuracy: 0.922\n",
      "2017-08-13 22:46:56,603 - INFO - step: 1155,loss: 0.192 accuracy: 0.922\n",
      "2017-08-13 22:46:56,667 - INFO - step: 1156,loss: 0.119 accuracy: 0.969\n",
      "2017-08-13 22:46:56,732 - INFO - step: 1157,loss: 0.143 accuracy: 0.938\n",
      "2017-08-13 22:46:56,795 - INFO - step: 1158,loss: 0.165 accuracy: 0.938\n",
      "2017-08-13 22:46:56,859 - INFO - step: 1159,loss: 0.288 accuracy: 0.859\n",
      "2017-08-13 22:46:56,924 - INFO - step: 1160,loss: 0.099 accuracy: 0.984\n",
      "2017-08-13 22:46:56,989 - INFO - step: 1161,loss: 0.110 accuracy: 0.938\n",
      "2017-08-13 22:46:57,052 - INFO - step: 1162,loss: 0.158 accuracy: 0.953\n",
      "2017-08-13 22:46:57,116 - INFO - step: 1163,loss: 0.198 accuracy: 0.938\n",
      "2017-08-13 22:46:57,179 - INFO - step: 1164,loss: 0.200 accuracy: 0.938\n",
      "2017-08-13 22:46:57,242 - INFO - step: 1165,loss: 0.318 accuracy: 0.891\n",
      "2017-08-13 22:46:57,304 - INFO - step: 1166,loss: 0.134 accuracy: 0.953\n",
      "2017-08-13 22:46:57,369 - INFO - step: 1167,loss: 0.148 accuracy: 0.938\n",
      "2017-08-13 22:46:57,432 - INFO - step: 1168,loss: 0.169 accuracy: 0.938\n",
      "2017-08-13 22:46:57,496 - INFO - step: 1169,loss: 0.156 accuracy: 0.938\n",
      "2017-08-13 22:46:57,560 - INFO - step: 1170,loss: 0.227 accuracy: 0.906\n",
      "2017-08-13 22:46:57,623 - INFO - step: 1171,loss: 0.237 accuracy: 0.891\n",
      "2017-08-13 22:46:57,691 - INFO - step: 1172,loss: 0.074 accuracy: 0.984\n",
      "2017-08-13 22:46:57,756 - INFO - step: 1173,loss: 0.230 accuracy: 0.922\n",
      "2017-08-13 22:46:57,820 - INFO - step: 1174,loss: 0.130 accuracy: 0.984\n",
      "2017-08-13 22:46:57,888 - INFO - step: 1175,loss: 0.206 accuracy: 0.906\n",
      "2017-08-13 22:46:57,950 - INFO - step: 1176,loss: 0.169 accuracy: 0.906\n",
      "2017-08-13 22:46:58,013 - INFO - step: 1177,loss: 0.212 accuracy: 0.922\n",
      "2017-08-13 22:46:58,076 - INFO - step: 1178,loss: 0.216 accuracy: 0.875\n",
      "2017-08-13 22:46:58,139 - INFO - step: 1179,loss: 0.159 accuracy: 0.906\n",
      "2017-08-13 22:46:58,203 - INFO - step: 1180,loss: 0.167 accuracy: 0.938\n",
      "2017-08-13 22:46:58,266 - INFO - step: 1181,loss: 0.065 accuracy: 0.984\n",
      "2017-08-13 22:46:58,331 - INFO - step: 1182,loss: 0.133 accuracy: 0.938\n",
      "2017-08-13 22:46:58,389 - INFO - step: 1183,loss: 0.300 accuracy: 0.891\n",
      "2017-08-13 22:46:58,457 - INFO - step: 1184,loss: 0.120 accuracy: 0.969\n",
      "2017-08-13 22:46:58,521 - INFO - step: 1185,loss: 0.132 accuracy: 0.953\n",
      "2017-08-13 22:46:58,586 - INFO - step: 1186,loss: 0.148 accuracy: 0.953\n",
      "2017-08-13 22:46:58,650 - INFO - step: 1187,loss: 0.165 accuracy: 0.906\n",
      "2017-08-13 22:46:58,674 - INFO - step: 1188,loss: 0.255 accuracy: 0.857\n",
      "2017-08-13 22:46:58,676 - INFO - \n",
      "Evaluation:\n",
      "2017-08-13 22:46:58,818 - INFO - step: 1188,loss: 0.205 accuracy: 0.916\n",
      "2017-08-13 22:46:58,920 - INFO - Saving model to /home/zx/cuckoo_1000/session_save/checkpoints/model-1188 at step 32.\n",
      "2017-08-13 22:46:58,923 - INFO - \train_epoch:\n",
      "2017-08-13 22:46:58,924 - INFO - loss_total: 0.176 accuracy_total: 0.930\n",
      "2017-08-13 22:46:59,000 - INFO - step: 1189,loss: 0.111 accuracy: 0.984\n",
      "2017-08-13 22:46:59,064 - INFO - step: 1190,loss: 0.093 accuracy: 0.984\n",
      "2017-08-13 22:46:59,123 - INFO - step: 1191,loss: 0.173 accuracy: 0.938\n",
      "2017-08-13 22:46:59,184 - INFO - step: 1192,loss: 0.129 accuracy: 0.938\n",
      "2017-08-13 22:46:59,246 - INFO - step: 1193,loss: 0.294 accuracy: 0.875\n",
      "2017-08-13 22:46:59,309 - INFO - step: 1194,loss: 0.255 accuracy: 0.938\n",
      "2017-08-13 22:46:59,372 - INFO - step: 1195,loss: 0.208 accuracy: 0.891\n",
      "2017-08-13 22:46:59,435 - INFO - step: 1196,loss: 0.187 accuracy: 0.938\n",
      "2017-08-13 22:46:59,499 - INFO - step: 1197,loss: 0.093 accuracy: 0.969\n",
      "2017-08-13 22:46:59,562 - INFO - step: 1198,loss: 0.205 accuracy: 0.922\n",
      "2017-08-13 22:46:59,626 - INFO - step: 1199,loss: 0.150 accuracy: 0.938\n",
      "2017-08-13 22:46:59,691 - INFO - step: 1200,loss: 0.133 accuracy: 0.953\n",
      "2017-08-13 22:46:59,750 - INFO - step: 1201,loss: 0.143 accuracy: 0.953\n",
      "2017-08-13 22:46:59,814 - INFO - step: 1202,loss: 0.096 accuracy: 0.984\n",
      "2017-08-13 22:46:59,877 - INFO - step: 1203,loss: 0.171 accuracy: 0.891\n",
      "2017-08-13 22:46:59,939 - INFO - step: 1204,loss: 0.162 accuracy: 0.922\n",
      "2017-08-13 22:47:00,003 - INFO - step: 1205,loss: 0.119 accuracy: 0.953\n",
      "2017-08-13 22:47:00,067 - INFO - step: 1206,loss: 0.106 accuracy: 0.969\n",
      "2017-08-13 22:47:00,130 - INFO - step: 1207,loss: 0.235 accuracy: 0.891\n",
      "2017-08-13 22:47:00,194 - INFO - step: 1208,loss: 0.193 accuracy: 0.906\n",
      "2017-08-13 22:47:00,256 - INFO - step: 1209,loss: 0.206 accuracy: 0.891\n",
      "2017-08-13 22:47:00,319 - INFO - step: 1210,loss: 0.172 accuracy: 0.906\n",
      "2017-08-13 22:47:00,382 - INFO - step: 1211,loss: 0.173 accuracy: 0.906\n",
      "2017-08-13 22:47:00,444 - INFO - step: 1212,loss: 0.110 accuracy: 0.953\n",
      "2017-08-13 22:47:00,508 - INFO - step: 1213,loss: 0.231 accuracy: 0.922\n",
      "2017-08-13 22:47:00,572 - INFO - step: 1214,loss: 0.117 accuracy: 0.953\n",
      "2017-08-13 22:47:00,635 - INFO - step: 1215,loss: 0.209 accuracy: 0.891\n",
      "2017-08-13 22:47:00,697 - INFO - step: 1216,loss: 0.141 accuracy: 0.984\n",
      "2017-08-13 22:47:00,761 - INFO - step: 1217,loss: 0.234 accuracy: 0.922\n",
      "2017-08-13 22:47:00,826 - INFO - step: 1218,loss: 0.177 accuracy: 0.938\n",
      "2017-08-13 22:47:00,890 - INFO - step: 1219,loss: 0.126 accuracy: 0.953\n",
      "2017-08-13 22:47:00,955 - INFO - step: 1220,loss: 0.236 accuracy: 0.938\n",
      "2017-08-13 22:47:01,018 - INFO - step: 1221,loss: 0.167 accuracy: 0.953\n",
      "2017-08-13 22:47:01,082 - INFO - step: 1222,loss: 0.126 accuracy: 0.969\n",
      "2017-08-13 22:47:01,146 - INFO - step: 1223,loss: 0.154 accuracy: 0.969\n",
      "2017-08-13 22:47:01,170 - INFO - step: 1224,loss: 0.164 accuracy: 0.857\n",
      "2017-08-13 22:47:01,172 - INFO - \train_epoch:\n",
      "2017-08-13 22:47:01,174 - INFO - loss_total: 0.167 accuracy_total: 0.934\n",
      "2017-08-13 22:47:01,277 - INFO - step: 1225,loss: 0.127 accuracy: 0.984\n",
      "2017-08-13 22:47:01,345 - INFO - step: 1226,loss: 0.111 accuracy: 0.969\n",
      "2017-08-13 22:47:01,425 - INFO - step: 1227,loss: 0.184 accuracy: 0.953\n",
      "2017-08-13 22:47:01,488 - INFO - step: 1228,loss: 0.088 accuracy: 0.969\n",
      "2017-08-13 22:47:01,551 - INFO - step: 1229,loss: 0.169 accuracy: 0.938\n",
      "2017-08-13 22:47:01,615 - INFO - step: 1230,loss: 0.237 accuracy: 0.922\n",
      "2017-08-13 22:47:01,679 - INFO - step: 1231,loss: 0.175 accuracy: 0.953\n",
      "2017-08-13 22:47:01,742 - INFO - step: 1232,loss: 0.061 accuracy: 0.984\n",
      "2017-08-13 22:47:01,806 - INFO - step: 1233,loss: 0.106 accuracy: 0.969\n",
      "2017-08-13 22:47:01,870 - INFO - step: 1234,loss: 0.167 accuracy: 0.938\n",
      "2017-08-13 22:47:01,934 - INFO - step: 1235,loss: 0.195 accuracy: 0.922\n",
      "2017-08-13 22:47:01,998 - INFO - step: 1236,loss: 0.070 accuracy: 0.984\n",
      "2017-08-13 22:47:02,063 - INFO - step: 1237,loss: 0.137 accuracy: 0.938\n",
      "2017-08-13 22:47:02,126 - INFO - step: 1238,loss: 0.192 accuracy: 0.906\n",
      "2017-08-13 22:47:02,186 - INFO - step: 1239,loss: 0.162 accuracy: 0.922\n",
      "2017-08-13 22:47:02,249 - INFO - step: 1240,loss: 0.211 accuracy: 0.922\n",
      "2017-08-13 22:47:02,312 - INFO - step: 1241,loss: 0.135 accuracy: 0.969\n",
      "2017-08-13 22:47:02,373 - INFO - step: 1242,loss: 0.161 accuracy: 0.906\n",
      "2017-08-13 22:47:02,436 - INFO - step: 1243,loss: 0.146 accuracy: 0.969\n",
      "2017-08-13 22:47:02,500 - INFO - step: 1244,loss: 0.144 accuracy: 0.922\n",
      "2017-08-13 22:47:02,563 - INFO - step: 1245,loss: 0.124 accuracy: 0.953\n",
      "2017-08-13 22:47:02,628 - INFO - step: 1246,loss: 0.153 accuracy: 0.922\n",
      "2017-08-13 22:47:02,694 - INFO - step: 1247,loss: 0.122 accuracy: 0.953\n",
      "2017-08-13 22:47:02,756 - INFO - step: 1248,loss: 0.291 accuracy: 0.891\n",
      "2017-08-13 22:47:02,819 - INFO - step: 1249,loss: 0.168 accuracy: 0.953\n",
      "2017-08-13 22:47:02,883 - INFO - step: 1250,loss: 0.152 accuracy: 0.938\n",
      "2017-08-13 22:47:02,946 - INFO - step: 1251,loss: 0.102 accuracy: 0.969\n",
      "2017-08-13 22:47:03,010 - INFO - step: 1252,loss: 0.184 accuracy: 0.938\n",
      "2017-08-13 22:47:03,074 - INFO - step: 1253,loss: 0.284 accuracy: 0.891\n",
      "2017-08-13 22:47:03,135 - INFO - step: 1254,loss: 0.103 accuracy: 0.953\n",
      "2017-08-13 22:47:03,197 - INFO - step: 1255,loss: 0.118 accuracy: 0.953\n",
      "2017-08-13 22:47:03,260 - INFO - step: 1256,loss: 0.121 accuracy: 0.953\n",
      "2017-08-13 22:47:03,324 - INFO - step: 1257,loss: 0.167 accuracy: 0.922\n",
      "2017-08-13 22:47:03,386 - INFO - step: 1258,loss: 0.190 accuracy: 0.906\n",
      "2017-08-13 22:47:03,450 - INFO - step: 1259,loss: 0.196 accuracy: 0.906\n",
      "2017-08-13 22:47:03,474 - INFO - step: 1260,loss: 0.052 accuracy: 1.000\n",
      "2017-08-13 22:47:03,476 - INFO - \n",
      "Evaluation:\n",
      "2017-08-13 22:47:03,558 - INFO - step: 1260,loss: 0.209 accuracy: 0.912\n",
      "2017-08-13 22:47:03,560 - INFO - \train_epoch:\n",
      "2017-08-13 22:47:03,561 - INFO - loss_total: 0.153 accuracy_total: 0.943\n",
      "2017-08-13 22:47:03,647 - INFO - step: 1261,loss: 0.224 accuracy: 0.875\n",
      "2017-08-13 22:47:03,711 - INFO - step: 1262,loss: 0.145 accuracy: 0.953\n",
      "2017-08-13 22:47:03,775 - INFO - step: 1263,loss: 0.213 accuracy: 0.938\n",
      "2017-08-13 22:47:03,837 - INFO - step: 1264,loss: 0.116 accuracy: 0.953\n",
      "2017-08-13 22:47:03,901 - INFO - step: 1265,loss: 0.164 accuracy: 0.953\n",
      "2017-08-13 22:47:03,966 - INFO - step: 1266,loss: 0.143 accuracy: 0.969\n",
      "2017-08-13 22:47:04,029 - INFO - step: 1267,loss: 0.175 accuracy: 0.906\n",
      "2017-08-13 22:47:04,094 - INFO - step: 1268,loss: 0.164 accuracy: 0.953\n",
      "2017-08-13 22:47:04,158 - INFO - step: 1269,loss: 0.108 accuracy: 0.953\n",
      "2017-08-13 22:47:04,220 - INFO - step: 1270,loss: 0.124 accuracy: 0.938\n",
      "2017-08-13 22:47:04,284 - INFO - step: 1271,loss: 0.118 accuracy: 0.984\n",
      "2017-08-13 22:47:04,347 - INFO - step: 1272,loss: 0.121 accuracy: 0.984\n",
      "2017-08-13 22:47:04,409 - INFO - step: 1273,loss: 0.118 accuracy: 0.953\n",
      "2017-08-13 22:47:04,472 - INFO - step: 1274,loss: 0.147 accuracy: 0.969\n",
      "2017-08-13 22:47:04,535 - INFO - step: 1275,loss: 0.099 accuracy: 0.953\n",
      "2017-08-13 22:47:04,598 - INFO - step: 1276,loss: 0.198 accuracy: 0.922\n",
      "2017-08-13 22:47:04,660 - INFO - step: 1277,loss: 0.159 accuracy: 0.922\n",
      "2017-08-13 22:47:04,723 - INFO - step: 1278,loss: 0.191 accuracy: 0.906\n",
      "2017-08-13 22:47:04,787 - INFO - step: 1279,loss: 0.216 accuracy: 0.922\n",
      "2017-08-13 22:47:04,849 - INFO - step: 1280,loss: 0.149 accuracy: 0.922\n",
      "2017-08-13 22:47:04,912 - INFO - step: 1281,loss: 0.154 accuracy: 0.953\n",
      "2017-08-13 22:47:04,976 - INFO - step: 1282,loss: 0.207 accuracy: 0.891\n",
      "2017-08-13 22:47:05,040 - INFO - step: 1283,loss: 0.116 accuracy: 0.969\n",
      "2017-08-13 22:47:05,103 - INFO - step: 1284,loss: 0.163 accuracy: 0.922\n",
      "2017-08-13 22:47:05,167 - INFO - step: 1285,loss: 0.242 accuracy: 0.906\n",
      "2017-08-13 22:47:05,230 - INFO - step: 1286,loss: 0.215 accuracy: 0.922\n",
      "2017-08-13 22:47:05,294 - INFO - step: 1287,loss: 0.090 accuracy: 1.000\n",
      "2017-08-13 22:47:05,358 - INFO - step: 1288,loss: 0.172 accuracy: 0.938\n",
      "2017-08-13 22:47:05,421 - INFO - step: 1289,loss: 0.126 accuracy: 0.922\n",
      "2017-08-13 22:47:05,484 - INFO - step: 1290,loss: 0.204 accuracy: 0.938\n",
      "2017-08-13 22:47:05,549 - INFO - step: 1291,loss: 0.250 accuracy: 0.875\n",
      "2017-08-13 22:47:05,611 - INFO - step: 1292,loss: 0.188 accuracy: 0.938\n",
      "2017-08-13 22:47:05,673 - INFO - step: 1293,loss: 0.104 accuracy: 0.953\n",
      "2017-08-13 22:47:05,735 - INFO - step: 1294,loss: 0.128 accuracy: 0.969\n",
      "2017-08-13 22:47:05,798 - INFO - step: 1295,loss: 0.165 accuracy: 0.922\n",
      "2017-08-13 22:47:05,821 - INFO - step: 1296,loss: 0.110 accuracy: 1.000\n",
      "2017-08-13 22:47:05,824 - INFO - \train_epoch:\n",
      "2017-08-13 22:47:05,826 - INFO - loss_total: 0.159 accuracy_total: 0.940\n",
      "2017-08-13 22:47:05,902 - INFO - step: 1297,loss: 0.091 accuracy: 0.984\n",
      "2017-08-13 22:47:05,965 - INFO - step: 1298,loss: 0.096 accuracy: 0.984\n",
      "2017-08-13 22:47:06,028 - INFO - step: 1299,loss: 0.087 accuracy: 0.969\n",
      "2017-08-13 22:47:06,092 - INFO - step: 1300,loss: 0.176 accuracy: 0.953\n",
      "2017-08-13 22:47:06,154 - INFO - step: 1301,loss: 0.152 accuracy: 0.938\n",
      "2017-08-13 22:47:06,217 - INFO - step: 1302,loss: 0.172 accuracy: 0.969\n",
      "2017-08-13 22:47:06,280 - INFO - step: 1303,loss: 0.218 accuracy: 0.906\n",
      "2017-08-13 22:47:06,344 - INFO - step: 1304,loss: 0.198 accuracy: 0.938\n",
      "2017-08-13 22:47:06,407 - INFO - step: 1305,loss: 0.202 accuracy: 0.891\n",
      "2017-08-13 22:47:06,470 - INFO - step: 1306,loss: 0.138 accuracy: 0.938\n",
      "2017-08-13 22:47:06,534 - INFO - step: 1307,loss: 0.150 accuracy: 0.938\n",
      "2017-08-13 22:47:06,593 - INFO - step: 1308,loss: 0.161 accuracy: 0.938\n",
      "2017-08-13 22:47:06,660 - INFO - step: 1309,loss: 0.103 accuracy: 0.938\n",
      "2017-08-13 22:47:06,724 - INFO - step: 1310,loss: 0.192 accuracy: 0.891\n",
      "2017-08-13 22:47:06,787 - INFO - step: 1311,loss: 0.388 accuracy: 0.891\n",
      "2017-08-13 22:47:06,849 - INFO - step: 1312,loss: 0.152 accuracy: 0.922\n",
      "2017-08-13 22:47:06,913 - INFO - step: 1313,loss: 0.260 accuracy: 0.922\n",
      "2017-08-13 22:47:06,975 - INFO - step: 1314,loss: 0.232 accuracy: 0.875\n",
      "2017-08-13 22:47:07,038 - INFO - step: 1315,loss: 0.084 accuracy: 0.969\n",
      "2017-08-13 22:47:07,104 - INFO - step: 1316,loss: 0.133 accuracy: 0.938\n",
      "2017-08-13 22:47:07,167 - INFO - step: 1317,loss: 0.108 accuracy: 0.969\n",
      "2017-08-13 22:47:07,229 - INFO - step: 1318,loss: 0.154 accuracy: 0.953\n",
      "2017-08-13 22:47:07,292 - INFO - step: 1319,loss: 0.123 accuracy: 0.922\n",
      "2017-08-13 22:47:07,355 - INFO - step: 1320,loss: 0.203 accuracy: 0.875\n",
      "2017-08-13 22:47:07,418 - INFO - step: 1321,loss: 0.148 accuracy: 0.938\n",
      "2017-08-13 22:47:07,481 - INFO - step: 1322,loss: 0.143 accuracy: 0.922\n",
      "2017-08-13 22:47:07,541 - INFO - step: 1323,loss: 0.124 accuracy: 0.969\n",
      "2017-08-13 22:47:07,604 - INFO - step: 1324,loss: 0.155 accuracy: 0.953\n",
      "2017-08-13 22:47:07,668 - INFO - step: 1325,loss: 0.116 accuracy: 0.953\n",
      "2017-08-13 22:47:07,730 - INFO - step: 1326,loss: 0.265 accuracy: 0.891\n",
      "2017-08-13 22:47:07,791 - INFO - step: 1327,loss: 0.139 accuracy: 0.953\n",
      "2017-08-13 22:47:07,853 - INFO - step: 1328,loss: 0.229 accuracy: 0.922\n",
      "2017-08-13 22:47:07,918 - INFO - step: 1329,loss: 0.181 accuracy: 0.953\n",
      "2017-08-13 22:47:07,981 - INFO - step: 1330,loss: 0.240 accuracy: 0.906\n",
      "2017-08-13 22:47:08,044 - INFO - step: 1331,loss: 0.052 accuracy: 1.000\n",
      "2017-08-13 22:47:08,066 - INFO - step: 1332,loss: 0.087 accuracy: 1.000\n",
      "2017-08-13 22:47:08,068 - INFO - \n",
      "Evaluation:\n",
      "2017-08-13 22:47:08,213 - INFO - step: 1332,loss: 0.206 accuracy: 0.912\n",
      "2017-08-13 22:47:08,215 - INFO - \train_epoch:\n",
      "2017-08-13 22:47:08,217 - INFO - loss_total: 0.163 accuracy_total: 0.938\n",
      "2017-08-13 22:47:08,300 - INFO - step: 1333,loss: 0.221 accuracy: 0.938\n",
      "2017-08-13 22:47:08,362 - INFO - step: 1334,loss: 0.104 accuracy: 0.984\n",
      "2017-08-13 22:47:08,426 - INFO - step: 1335,loss: 0.263 accuracy: 0.891\n",
      "2017-08-13 22:47:08,488 - INFO - step: 1336,loss: 0.145 accuracy: 0.953\n",
      "2017-08-13 22:47:08,550 - INFO - step: 1337,loss: 0.097 accuracy: 0.984\n",
      "2017-08-13 22:47:08,613 - INFO - step: 1338,loss: 0.110 accuracy: 0.953\n",
      "2017-08-13 22:47:08,675 - INFO - step: 1339,loss: 0.305 accuracy: 0.938\n",
      "2017-08-13 22:47:08,739 - INFO - step: 1340,loss: 0.125 accuracy: 0.938\n",
      "2017-08-13 22:47:08,802 - INFO - step: 1341,loss: 0.191 accuracy: 0.922\n",
      "2017-08-13 22:47:08,867 - INFO - step: 1342,loss: 0.140 accuracy: 0.938\n",
      "2017-08-13 22:47:08,931 - INFO - step: 1343,loss: 0.220 accuracy: 0.906\n",
      "2017-08-13 22:47:08,993 - INFO - step: 1344,loss: 0.231 accuracy: 0.906\n",
      "2017-08-13 22:47:09,057 - INFO - step: 1345,loss: 0.221 accuracy: 0.938\n",
      "2017-08-13 22:47:09,119 - INFO - step: 1346,loss: 0.101 accuracy: 0.969\n",
      "2017-08-13 22:47:09,182 - INFO - step: 1347,loss: 0.109 accuracy: 0.969\n",
      "2017-08-13 22:47:09,243 - INFO - step: 1348,loss: 0.143 accuracy: 0.953\n",
      "2017-08-13 22:47:09,304 - INFO - step: 1349,loss: 0.166 accuracy: 0.938\n",
      "2017-08-13 22:47:09,368 - INFO - step: 1350,loss: 0.061 accuracy: 0.969\n",
      "2017-08-13 22:47:09,431 - INFO - step: 1351,loss: 0.137 accuracy: 0.953\n",
      "2017-08-13 22:47:09,493 - INFO - step: 1352,loss: 0.163 accuracy: 0.938\n",
      "2017-08-13 22:47:09,555 - INFO - step: 1353,loss: 0.151 accuracy: 0.953\n",
      "2017-08-13 22:47:09,617 - INFO - step: 1354,loss: 0.313 accuracy: 0.891\n",
      "2017-08-13 22:47:09,679 - INFO - step: 1355,loss: 0.101 accuracy: 0.969\n",
      "2017-08-13 22:47:09,742 - INFO - step: 1356,loss: 0.206 accuracy: 0.906\n",
      "2017-08-13 22:47:09,805 - INFO - step: 1357,loss: 0.109 accuracy: 0.938\n",
      "2017-08-13 22:47:09,867 - INFO - step: 1358,loss: 0.091 accuracy: 0.984\n",
      "2017-08-13 22:47:09,929 - INFO - step: 1359,loss: 0.189 accuracy: 0.922\n",
      "2017-08-13 22:47:09,992 - INFO - step: 1360,loss: 0.168 accuracy: 0.891\n",
      "2017-08-13 22:47:10,056 - INFO - step: 1361,loss: 0.070 accuracy: 1.000\n",
      "2017-08-13 22:47:10,120 - INFO - step: 1362,loss: 0.180 accuracy: 0.922\n",
      "2017-08-13 22:47:10,182 - INFO - step: 1363,loss: 0.155 accuracy: 0.938\n",
      "2017-08-13 22:47:10,246 - INFO - step: 1364,loss: 0.111 accuracy: 0.953\n",
      "2017-08-13 22:47:10,311 - INFO - step: 1365,loss: 0.194 accuracy: 0.922\n",
      "2017-08-13 22:47:10,374 - INFO - step: 1366,loss: 0.163 accuracy: 0.953\n",
      "2017-08-13 22:47:10,437 - INFO - step: 1367,loss: 0.193 accuracy: 0.891\n",
      "2017-08-13 22:47:10,457 - INFO - step: 1368,loss: 0.357 accuracy: 0.857\n",
      "2017-08-13 22:47:10,459 - INFO - \train_epoch:\n",
      "2017-08-13 22:47:10,461 - INFO - loss_total: 0.167 accuracy_total: 0.938\n",
      "2017-08-13 22:47:10,535 - INFO - step: 1369,loss: 0.266 accuracy: 0.906\n",
      "2017-08-13 22:47:10,599 - INFO - step: 1370,loss: 0.122 accuracy: 0.953\n",
      "2017-08-13 22:47:10,661 - INFO - step: 1371,loss: 0.172 accuracy: 0.891\n",
      "2017-08-13 22:47:10,723 - INFO - step: 1372,loss: 0.146 accuracy: 0.953\n",
      "2017-08-13 22:47:10,786 - INFO - step: 1373,loss: 0.217 accuracy: 0.922\n",
      "2017-08-13 22:47:10,854 - INFO - step: 1374,loss: 0.150 accuracy: 0.938\n",
      "2017-08-13 22:47:10,918 - INFO - step: 1375,loss: 0.079 accuracy: 0.984\n",
      "2017-08-13 22:47:10,980 - INFO - step: 1376,loss: 0.244 accuracy: 0.938\n",
      "2017-08-13 22:47:11,043 - INFO - step: 1377,loss: 0.106 accuracy: 0.938\n",
      "2017-08-13 22:47:11,107 - INFO - step: 1378,loss: 0.061 accuracy: 1.000\n",
      "2017-08-13 22:47:11,171 - INFO - step: 1379,loss: 0.113 accuracy: 0.969\n",
      "2017-08-13 22:47:11,235 - INFO - step: 1380,loss: 0.059 accuracy: 1.000\n",
      "2017-08-13 22:47:11,299 - INFO - step: 1381,loss: 0.131 accuracy: 0.922\n",
      "2017-08-13 22:47:11,365 - INFO - step: 1382,loss: 0.087 accuracy: 0.969\n",
      "2017-08-13 22:47:11,427 - INFO - step: 1383,loss: 0.086 accuracy: 0.984\n",
      "2017-08-13 22:47:11,491 - INFO - step: 1384,loss: 0.173 accuracy: 0.938\n",
      "2017-08-13 22:47:11,552 - INFO - step: 1385,loss: 0.098 accuracy: 0.969\n",
      "2017-08-13 22:47:11,615 - INFO - step: 1386,loss: 0.146 accuracy: 0.969\n",
      "2017-08-13 22:47:11,677 - INFO - step: 1387,loss: 0.274 accuracy: 0.938\n",
      "2017-08-13 22:47:11,759 - INFO - step: 1388,loss: 0.129 accuracy: 0.938\n",
      "2017-08-13 22:47:11,823 - INFO - step: 1389,loss: 0.159 accuracy: 0.922\n",
      "2017-08-13 22:47:11,886 - INFO - step: 1390,loss: 0.138 accuracy: 0.922\n",
      "2017-08-13 22:47:11,948 - INFO - step: 1391,loss: 0.106 accuracy: 0.953\n",
      "2017-08-13 22:47:12,012 - INFO - step: 1392,loss: 0.236 accuracy: 0.891\n",
      "2017-08-13 22:47:12,075 - INFO - step: 1393,loss: 0.092 accuracy: 0.969\n",
      "2017-08-13 22:47:12,138 - INFO - step: 1394,loss: 0.120 accuracy: 0.922\n",
      "2017-08-13 22:47:12,203 - INFO - step: 1395,loss: 0.169 accuracy: 0.953\n",
      "2017-08-13 22:47:12,266 - INFO - step: 1396,loss: 0.144 accuracy: 0.938\n",
      "2017-08-13 22:47:12,329 - INFO - step: 1397,loss: 0.176 accuracy: 0.938\n",
      "2017-08-13 22:47:12,391 - INFO - step: 1398,loss: 0.125 accuracy: 0.922\n",
      "2017-08-13 22:47:12,453 - INFO - step: 1399,loss: 0.208 accuracy: 0.891\n",
      "2017-08-13 22:47:12,517 - INFO - step: 1400,loss: 0.168 accuracy: 0.953\n",
      "2017-08-13 22:47:12,580 - INFO - step: 1401,loss: 0.159 accuracy: 0.906\n",
      "2017-08-13 22:47:12,643 - INFO - step: 1402,loss: 0.205 accuracy: 0.953\n",
      "2017-08-13 22:47:12,707 - INFO - step: 1403,loss: 0.200 accuracy: 0.906\n",
      "2017-08-13 22:47:12,731 - INFO - step: 1404,loss: 0.574 accuracy: 0.857\n",
      "2017-08-13 22:47:12,734 - INFO - \n",
      "Evaluation:\n",
      "2017-08-13 22:47:12,793 - INFO - step: 1404,loss: 0.214 accuracy: 0.916\n",
      "2017-08-13 22:47:12,901 - INFO - Saving model to /home/zx/cuckoo_1000/session_save/checkpoints/model-1404 at step 38.\n",
      "2017-08-13 22:47:12,904 - INFO - \train_epoch:\n",
      "2017-08-13 22:47:12,905 - INFO - loss_total: 0.162 accuracy_total: 0.939\n",
      "2017-08-13 22:47:12,975 - INFO - step: 1405,loss: 0.133 accuracy: 0.969\n",
      "2017-08-13 22:47:13,034 - INFO - step: 1406,loss: 0.111 accuracy: 0.953\n",
      "2017-08-13 22:47:13,098 - INFO - step: 1407,loss: 0.208 accuracy: 0.906\n",
      "2017-08-13 22:47:13,160 - INFO - step: 1408,loss: 0.149 accuracy: 0.953\n",
      "2017-08-13 22:47:13,224 - INFO - step: 1409,loss: 0.192 accuracy: 0.922\n",
      "2017-08-13 22:47:13,286 - INFO - step: 1410,loss: 0.091 accuracy: 0.984\n",
      "2017-08-13 22:47:13,351 - INFO - step: 1411,loss: 0.104 accuracy: 0.969\n",
      "2017-08-13 22:47:13,414 - INFO - step: 1412,loss: 0.128 accuracy: 0.938\n",
      "2017-08-13 22:47:13,479 - INFO - step: 1413,loss: 0.167 accuracy: 0.906\n",
      "2017-08-13 22:47:13,543 - INFO - step: 1414,loss: 0.256 accuracy: 0.906\n",
      "2017-08-13 22:47:13,608 - INFO - step: 1415,loss: 0.183 accuracy: 0.938\n",
      "2017-08-13 22:47:13,673 - INFO - step: 1416,loss: 0.106 accuracy: 0.969\n",
      "2017-08-13 22:47:13,735 - INFO - step: 1417,loss: 0.069 accuracy: 0.969\n",
      "2017-08-13 22:47:13,798 - INFO - step: 1418,loss: 0.098 accuracy: 0.969\n",
      "2017-08-13 22:47:13,860 - INFO - step: 1419,loss: 0.133 accuracy: 0.938\n",
      "2017-08-13 22:47:13,924 - INFO - step: 1420,loss: 0.163 accuracy: 0.922\n",
      "2017-08-13 22:47:13,986 - INFO - step: 1421,loss: 0.118 accuracy: 0.953\n",
      "2017-08-13 22:47:14,048 - INFO - step: 1422,loss: 0.107 accuracy: 0.969\n",
      "2017-08-13 22:47:14,112 - INFO - step: 1423,loss: 0.237 accuracy: 0.906\n",
      "2017-08-13 22:47:14,177 - INFO - step: 1424,loss: 0.132 accuracy: 0.922\n",
      "2017-08-13 22:47:14,241 - INFO - step: 1425,loss: 0.260 accuracy: 0.906\n",
      "2017-08-13 22:47:14,304 - INFO - step: 1426,loss: 0.163 accuracy: 0.953\n",
      "2017-08-13 22:47:14,370 - INFO - step: 1427,loss: 0.186 accuracy: 0.953\n",
      "2017-08-13 22:47:14,434 - INFO - step: 1428,loss: 0.177 accuracy: 0.922\n",
      "2017-08-13 22:47:14,496 - INFO - step: 1429,loss: 0.116 accuracy: 0.984\n",
      "2017-08-13 22:47:14,558 - INFO - step: 1430,loss: 0.191 accuracy: 0.953\n",
      "2017-08-13 22:47:14,622 - INFO - step: 1431,loss: 0.235 accuracy: 0.906\n",
      "2017-08-13 22:47:14,684 - INFO - step: 1432,loss: 0.333 accuracy: 0.891\n",
      "2017-08-13 22:47:14,749 - INFO - step: 1433,loss: 0.164 accuracy: 0.953\n",
      "2017-08-13 22:47:14,812 - INFO - step: 1434,loss: 0.188 accuracy: 0.938\n",
      "2017-08-13 22:47:14,874 - INFO - step: 1435,loss: 0.150 accuracy: 0.922\n",
      "2017-08-13 22:47:14,938 - INFO - step: 1436,loss: 0.096 accuracy: 0.953\n",
      "2017-08-13 22:47:15,001 - INFO - step: 1437,loss: 0.128 accuracy: 0.984\n",
      "2017-08-13 22:47:15,064 - INFO - step: 1438,loss: 0.222 accuracy: 0.922\n",
      "2017-08-13 22:47:15,126 - INFO - step: 1439,loss: 0.105 accuracy: 0.953\n",
      "2017-08-13 22:47:15,149 - INFO - step: 1440,loss: 0.177 accuracy: 0.857\n",
      "2017-08-13 22:47:15,152 - INFO - \train_epoch:\n",
      "2017-08-13 22:47:15,153 - INFO - loss_total: 0.160 accuracy_total: 0.939\n",
      "2017-08-13 22:47:15,225 - INFO - step: 1441,loss: 0.202 accuracy: 0.906\n",
      "2017-08-13 22:47:15,289 - INFO - step: 1442,loss: 0.146 accuracy: 0.922\n",
      "2017-08-13 22:47:15,354 - INFO - step: 1443,loss: 0.130 accuracy: 0.984\n",
      "2017-08-13 22:47:15,419 - INFO - step: 1444,loss: 0.091 accuracy: 0.984\n",
      "2017-08-13 22:47:15,482 - INFO - step: 1445,loss: 0.186 accuracy: 0.938\n",
      "2017-08-13 22:47:15,544 - INFO - step: 1446,loss: 0.068 accuracy: 0.984\n",
      "2017-08-13 22:47:15,608 - INFO - step: 1447,loss: 0.242 accuracy: 0.891\n",
      "2017-08-13 22:47:15,671 - INFO - step: 1448,loss: 0.147 accuracy: 0.953\n",
      "2017-08-13 22:47:15,735 - INFO - step: 1449,loss: 0.151 accuracy: 0.938\n",
      "2017-08-13 22:47:15,798 - INFO - step: 1450,loss: 0.107 accuracy: 0.953\n",
      "2017-08-13 22:47:15,860 - INFO - step: 1451,loss: 0.229 accuracy: 0.906\n",
      "2017-08-13 22:47:15,924 - INFO - step: 1452,loss: 0.171 accuracy: 0.922\n",
      "2017-08-13 22:47:15,987 - INFO - step: 1453,loss: 0.278 accuracy: 0.875\n",
      "2017-08-13 22:47:16,051 - INFO - step: 1454,loss: 0.129 accuracy: 0.938\n",
      "2017-08-13 22:47:16,117 - INFO - step: 1455,loss: 0.231 accuracy: 0.875\n",
      "2017-08-13 22:47:16,182 - INFO - step: 1456,loss: 0.133 accuracy: 0.922\n",
      "2017-08-13 22:47:16,246 - INFO - step: 1457,loss: 0.193 accuracy: 0.953\n",
      "2017-08-13 22:47:16,310 - INFO - step: 1458,loss: 0.085 accuracy: 0.984\n",
      "2017-08-13 22:47:16,378 - INFO - step: 1459,loss: 0.099 accuracy: 0.969\n",
      "2017-08-13 22:47:16,450 - INFO - step: 1460,loss: 0.105 accuracy: 0.969\n",
      "2017-08-13 22:47:16,515 - INFO - step: 1461,loss: 0.172 accuracy: 0.922\n",
      "2017-08-13 22:47:16,578 - INFO - step: 1462,loss: 0.165 accuracy: 0.922\n",
      "2017-08-13 22:47:16,642 - INFO - step: 1463,loss: 0.153 accuracy: 0.922\n",
      "2017-08-13 22:47:16,706 - INFO - step: 1464,loss: 0.139 accuracy: 0.953\n",
      "2017-08-13 22:47:16,769 - INFO - step: 1465,loss: 0.170 accuracy: 0.938\n",
      "2017-08-13 22:47:16,835 - INFO - step: 1466,loss: 0.230 accuracy: 0.938\n",
      "2017-08-13 22:47:16,898 - INFO - step: 1467,loss: 0.106 accuracy: 0.938\n",
      "2017-08-13 22:47:16,962 - INFO - step: 1468,loss: 0.061 accuracy: 1.000\n",
      "2017-08-13 22:47:17,025 - INFO - step: 1469,loss: 0.227 accuracy: 0.906\n",
      "2017-08-13 22:47:17,089 - INFO - step: 1470,loss: 0.153 accuracy: 0.953\n",
      "2017-08-13 22:47:17,154 - INFO - step: 1471,loss: 0.135 accuracy: 0.969\n",
      "2017-08-13 22:47:17,217 - INFO - step: 1472,loss: 0.145 accuracy: 0.953\n",
      "2017-08-13 22:47:17,282 - INFO - step: 1473,loss: 0.115 accuracy: 0.969\n",
      "2017-08-13 22:47:17,346 - INFO - step: 1474,loss: 0.110 accuracy: 0.953\n",
      "2017-08-13 22:47:17,410 - INFO - step: 1475,loss: 0.165 accuracy: 0.938\n",
      "2017-08-13 22:47:17,432 - INFO - step: 1476,loss: 0.341 accuracy: 0.857\n",
      "2017-08-13 22:47:17,435 - INFO - \n",
      "Evaluation:\n",
      "2017-08-13 22:47:17,517 - INFO - step: 1476,loss: 0.214 accuracy: 0.912\n",
      "2017-08-13 22:47:17,518 - INFO - \train_epoch:\n",
      "2017-08-13 22:47:17,521 - INFO - loss_total: 0.159 accuracy_total: 0.939\n",
      "2017-08-13 22:47:17,604 - INFO - step: 1477,loss: 0.093 accuracy: 0.969\n",
      "2017-08-13 22:47:17,668 - INFO - step: 1478,loss: 0.209 accuracy: 0.891\n",
      "2017-08-13 22:47:17,730 - INFO - step: 1479,loss: 0.139 accuracy: 0.938\n",
      "2017-08-13 22:47:17,794 - INFO - step: 1480,loss: 0.160 accuracy: 0.906\n",
      "2017-08-13 22:47:17,858 - INFO - step: 1481,loss: 0.143 accuracy: 0.938\n",
      "2017-08-13 22:47:17,923 - INFO - step: 1482,loss: 0.118 accuracy: 0.969\n",
      "2017-08-13 22:47:17,985 - INFO - step: 1483,loss: 0.125 accuracy: 0.984\n",
      "2017-08-13 22:47:18,048 - INFO - step: 1484,loss: 0.137 accuracy: 0.922\n",
      "2017-08-13 22:47:18,110 - INFO - step: 1485,loss: 0.100 accuracy: 0.984\n",
      "2017-08-13 22:47:18,177 - INFO - step: 1486,loss: 0.138 accuracy: 0.922\n",
      "2017-08-13 22:47:18,240 - INFO - step: 1487,loss: 0.093 accuracy: 0.969\n",
      "2017-08-13 22:47:18,304 - INFO - step: 1488,loss: 0.170 accuracy: 0.953\n",
      "2017-08-13 22:47:18,367 - INFO - step: 1489,loss: 0.259 accuracy: 0.906\n",
      "2017-08-13 22:47:18,434 - INFO - step: 1490,loss: 0.105 accuracy: 0.969\n",
      "2017-08-13 22:47:18,503 - INFO - step: 1491,loss: 0.091 accuracy: 0.953\n",
      "2017-08-13 22:47:18,566 - INFO - step: 1492,loss: 0.179 accuracy: 0.906\n",
      "2017-08-13 22:47:18,630 - INFO - step: 1493,loss: 0.198 accuracy: 0.906\n",
      "2017-08-13 22:47:18,693 - INFO - step: 1494,loss: 0.148 accuracy: 0.938\n",
      "2017-08-13 22:47:18,756 - INFO - step: 1495,loss: 0.043 accuracy: 0.984\n",
      "2017-08-13 22:47:18,820 - INFO - step: 1496,loss: 0.218 accuracy: 0.922\n",
      "2017-08-13 22:47:18,884 - INFO - step: 1497,loss: 0.141 accuracy: 0.938\n",
      "2017-08-13 22:47:18,946 - INFO - step: 1498,loss: 0.111 accuracy: 0.953\n",
      "2017-08-13 22:47:19,010 - INFO - step: 1499,loss: 0.078 accuracy: 0.984\n",
      "2017-08-13 22:47:19,072 - INFO - step: 1500,loss: 0.175 accuracy: 0.953\n",
      "2017-08-13 22:47:19,133 - INFO - step: 1501,loss: 0.101 accuracy: 0.953\n",
      "2017-08-13 22:47:19,197 - INFO - step: 1502,loss: 0.145 accuracy: 0.938\n",
      "2017-08-13 22:47:19,260 - INFO - step: 1503,loss: 0.210 accuracy: 0.922\n",
      "2017-08-13 22:47:19,323 - INFO - step: 1504,loss: 0.215 accuracy: 0.891\n",
      "2017-08-13 22:47:19,385 - INFO - step: 1505,loss: 0.094 accuracy: 0.969\n",
      "2017-08-13 22:47:19,449 - INFO - step: 1506,loss: 0.110 accuracy: 0.969\n",
      "2017-08-13 22:47:19,512 - INFO - step: 1507,loss: 0.097 accuracy: 0.969\n",
      "2017-08-13 22:47:19,575 - INFO - step: 1508,loss: 0.084 accuracy: 0.984\n",
      "2017-08-13 22:47:19,638 - INFO - step: 1509,loss: 0.206 accuracy: 0.922\n",
      "2017-08-13 22:47:19,701 - INFO - step: 1510,loss: 0.126 accuracy: 0.953\n",
      "2017-08-13 22:47:19,766 - INFO - step: 1511,loss: 0.143 accuracy: 0.938\n",
      "2017-08-13 22:47:19,790 - INFO - step: 1512,loss: 0.444 accuracy: 0.714\n",
      "2017-08-13 22:47:19,793 - INFO - \train_epoch:\n",
      "2017-08-13 22:47:19,794 - INFO - loss_total: 0.148 accuracy_total: 0.938\n",
      "2017-08-13 22:47:19,877 - INFO - step: 1513,loss: 0.129 accuracy: 0.969\n",
      "2017-08-13 22:47:19,941 - INFO - step: 1514,loss: 0.152 accuracy: 0.938\n",
      "2017-08-13 22:47:20,003 - INFO - step: 1515,loss: 0.157 accuracy: 0.922\n",
      "2017-08-13 22:47:20,067 - INFO - step: 1516,loss: 0.112 accuracy: 0.953\n",
      "2017-08-13 22:47:20,130 - INFO - step: 1517,loss: 0.093 accuracy: 0.984\n",
      "2017-08-13 22:47:20,194 - INFO - step: 1518,loss: 0.342 accuracy: 0.891\n",
      "2017-08-13 22:47:20,258 - INFO - step: 1519,loss: 0.201 accuracy: 0.922\n",
      "2017-08-13 22:47:20,321 - INFO - step: 1520,loss: 0.139 accuracy: 0.938\n",
      "2017-08-13 22:47:20,384 - INFO - step: 1521,loss: 0.161 accuracy: 0.938\n",
      "2017-08-13 22:47:20,449 - INFO - step: 1522,loss: 0.267 accuracy: 0.875\n",
      "2017-08-13 22:47:20,513 - INFO - step: 1523,loss: 0.154 accuracy: 0.938\n",
      "2017-08-13 22:47:20,585 - INFO - step: 1524,loss: 0.104 accuracy: 0.984\n",
      "2017-08-13 22:47:20,647 - INFO - step: 1525,loss: 0.157 accuracy: 0.906\n",
      "2017-08-13 22:47:20,710 - INFO - step: 1526,loss: 0.212 accuracy: 0.922\n",
      "2017-08-13 22:47:20,773 - INFO - step: 1527,loss: 0.098 accuracy: 1.000\n",
      "2017-08-13 22:47:20,836 - INFO - step: 1528,loss: 0.109 accuracy: 0.969\n",
      "2017-08-13 22:47:20,900 - INFO - step: 1529,loss: 0.102 accuracy: 0.969\n",
      "2017-08-13 22:47:20,964 - INFO - step: 1530,loss: 0.074 accuracy: 0.969\n",
      "2017-08-13 22:47:21,027 - INFO - step: 1531,loss: 0.099 accuracy: 0.969\n",
      "2017-08-13 22:47:21,093 - INFO - step: 1532,loss: 0.185 accuracy: 0.922\n",
      "2017-08-13 22:47:21,155 - INFO - step: 1533,loss: 0.103 accuracy: 0.969\n",
      "2017-08-13 22:47:21,220 - INFO - step: 1534,loss: 0.172 accuracy: 0.906\n",
      "2017-08-13 22:47:21,282 - INFO - step: 1535,loss: 0.190 accuracy: 0.906\n",
      "2017-08-13 22:47:21,347 - INFO - step: 1536,loss: 0.269 accuracy: 0.953\n",
      "2017-08-13 22:47:21,411 - INFO - step: 1537,loss: 0.153 accuracy: 0.938\n",
      "2017-08-13 22:47:21,474 - INFO - step: 1538,loss: 0.117 accuracy: 0.953\n",
      "2017-08-13 22:47:21,538 - INFO - step: 1539,loss: 0.131 accuracy: 0.938\n",
      "2017-08-13 22:47:21,601 - INFO - step: 1540,loss: 0.178 accuracy: 0.938\n",
      "2017-08-13 22:47:21,665 - INFO - step: 1541,loss: 0.164 accuracy: 0.938\n",
      "2017-08-13 22:47:21,728 - INFO - step: 1542,loss: 0.244 accuracy: 0.906\n",
      "2017-08-13 22:47:21,792 - INFO - step: 1543,loss: 0.207 accuracy: 0.922\n",
      "2017-08-13 22:47:21,856 - INFO - step: 1544,loss: 0.126 accuracy: 0.969\n",
      "2017-08-13 22:47:21,920 - INFO - step: 1545,loss: 0.107 accuracy: 0.938\n",
      "2017-08-13 22:47:21,983 - INFO - step: 1546,loss: 0.176 accuracy: 0.922\n",
      "2017-08-13 22:47:22,046 - INFO - step: 1547,loss: 0.078 accuracy: 0.969\n",
      "2017-08-13 22:47:22,069 - INFO - step: 1548,loss: 0.103 accuracy: 1.000\n",
      "2017-08-13 22:47:22,071 - INFO - \n",
      "Evaluation:\n",
      "2017-08-13 22:47:22,132 - INFO - step: 1548,loss: 0.203 accuracy: 0.912\n",
      "2017-08-13 22:47:22,133 - INFO - \train_epoch:\n",
      "2017-08-13 22:47:22,135 - INFO - loss_total: 0.155 accuracy_total: 0.943\n",
      "2017-08-13 22:47:22,217 - INFO - step: 1549,loss: 0.207 accuracy: 0.953\n",
      "2017-08-13 22:47:22,285 - INFO - step: 1550,loss: 0.106 accuracy: 0.969\n",
      "2017-08-13 22:47:22,348 - INFO - step: 1551,loss: 0.185 accuracy: 0.953\n",
      "2017-08-13 22:47:22,409 - INFO - step: 1552,loss: 0.090 accuracy: 0.984\n",
      "2017-08-13 22:47:22,471 - INFO - step: 1553,loss: 0.114 accuracy: 0.953\n",
      "2017-08-13 22:47:22,535 - INFO - step: 1554,loss: 0.137 accuracy: 0.953\n",
      "2017-08-13 22:47:22,596 - INFO - step: 1555,loss: 0.144 accuracy: 0.938\n",
      "2017-08-13 22:47:22,660 - INFO - step: 1556,loss: 0.163 accuracy: 0.922\n",
      "2017-08-13 22:47:22,724 - INFO - step: 1557,loss: 0.161 accuracy: 0.922\n",
      "2017-08-13 22:47:22,786 - INFO - step: 1558,loss: 0.106 accuracy: 0.953\n",
      "2017-08-13 22:47:22,852 - INFO - step: 1559,loss: 0.180 accuracy: 0.906\n",
      "2017-08-13 22:47:22,917 - INFO - step: 1560,loss: 0.164 accuracy: 0.922\n",
      "2017-08-13 22:47:22,981 - INFO - step: 1561,loss: 0.103 accuracy: 0.969\n",
      "2017-08-13 22:47:23,044 - INFO - step: 1562,loss: 0.091 accuracy: 0.969\n",
      "2017-08-13 22:47:23,106 - INFO - step: 1563,loss: 0.090 accuracy: 0.969\n",
      "2017-08-13 22:47:23,171 - INFO - step: 1564,loss: 0.097 accuracy: 0.969\n",
      "2017-08-13 22:47:23,234 - INFO - step: 1565,loss: 0.149 accuracy: 0.953\n",
      "2017-08-13 22:47:23,298 - INFO - step: 1566,loss: 0.113 accuracy: 0.953\n",
      "2017-08-13 22:47:23,361 - INFO - step: 1567,loss: 0.072 accuracy: 1.000\n",
      "2017-08-13 22:47:23,425 - INFO - step: 1568,loss: 0.065 accuracy: 0.969\n",
      "2017-08-13 22:47:23,488 - INFO - step: 1569,loss: 0.108 accuracy: 0.969\n",
      "2017-08-13 22:47:23,553 - INFO - step: 1570,loss: 0.035 accuracy: 1.000\n",
      "2017-08-13 22:47:23,618 - INFO - step: 1571,loss: 0.099 accuracy: 0.938\n",
      "2017-08-13 22:47:23,681 - INFO - step: 1572,loss: 0.099 accuracy: 0.953\n",
      "2017-08-13 22:47:23,745 - INFO - step: 1573,loss: 0.086 accuracy: 0.969\n",
      "2017-08-13 22:47:23,808 - INFO - step: 1574,loss: 0.252 accuracy: 0.875\n",
      "2017-08-13 22:47:23,871 - INFO - step: 1575,loss: 0.237 accuracy: 0.891\n",
      "2017-08-13 22:47:23,935 - INFO - step: 1576,loss: 0.266 accuracy: 0.891\n",
      "2017-08-13 22:47:23,998 - INFO - step: 1577,loss: 0.110 accuracy: 0.953\n",
      "2017-08-13 22:47:24,064 - INFO - step: 1578,loss: 0.242 accuracy: 0.891\n",
      "2017-08-13 22:47:24,126 - INFO - step: 1579,loss: 0.102 accuracy: 0.953\n",
      "2017-08-13 22:47:24,193 - INFO - step: 1580,loss: 0.121 accuracy: 0.953\n",
      "2017-08-13 22:47:24,256 - INFO - step: 1581,loss: 0.145 accuracy: 0.953\n",
      "2017-08-13 22:47:24,319 - INFO - step: 1582,loss: 0.212 accuracy: 0.906\n",
      "2017-08-13 22:47:24,386 - INFO - step: 1583,loss: 0.099 accuracy: 0.969\n",
      "2017-08-13 22:47:24,410 - INFO - step: 1584,loss: 0.244 accuracy: 0.857\n",
      "2017-08-13 22:47:24,412 - INFO - \train_epoch:\n",
      "2017-08-13 22:47:24,414 - INFO - loss_total: 0.139 accuracy_total: 0.944\n",
      "2017-08-13 22:47:24,496 - INFO - step: 1585,loss: 0.164 accuracy: 0.906\n",
      "2017-08-13 22:47:24,561 - INFO - step: 1586,loss: 0.168 accuracy: 0.922\n",
      "2017-08-13 22:47:24,625 - INFO - step: 1587,loss: 0.077 accuracy: 0.969\n",
      "2017-08-13 22:47:24,683 - INFO - step: 1588,loss: 0.090 accuracy: 0.984\n",
      "2017-08-13 22:47:24,744 - INFO - step: 1589,loss: 0.144 accuracy: 0.969\n",
      "2017-08-13 22:47:24,806 - INFO - step: 1590,loss: 0.136 accuracy: 0.969\n",
      "2017-08-13 22:47:24,869 - INFO - step: 1591,loss: 0.106 accuracy: 0.984\n",
      "2017-08-13 22:47:24,933 - INFO - step: 1592,loss: 0.107 accuracy: 0.938\n",
      "2017-08-13 22:47:24,994 - INFO - step: 1593,loss: 0.112 accuracy: 0.969\n",
      "2017-08-13 22:47:25,059 - INFO - step: 1594,loss: 0.210 accuracy: 0.906\n",
      "2017-08-13 22:47:25,123 - INFO - step: 1595,loss: 0.126 accuracy: 0.969\n",
      "2017-08-13 22:47:25,185 - INFO - step: 1596,loss: 0.118 accuracy: 0.984\n",
      "2017-08-13 22:47:25,249 - INFO - step: 1597,loss: 0.139 accuracy: 0.906\n",
      "2017-08-13 22:47:25,312 - INFO - step: 1598,loss: 0.095 accuracy: 0.953\n",
      "2017-08-13 22:47:25,376 - INFO - step: 1599,loss: 0.096 accuracy: 0.984\n",
      "2017-08-13 22:47:25,439 - INFO - step: 1600,loss: 0.129 accuracy: 0.938\n",
      "2017-08-13 22:47:25,500 - INFO - step: 1601,loss: 0.114 accuracy: 0.984\n",
      "2017-08-13 22:47:25,564 - INFO - step: 1602,loss: 0.129 accuracy: 0.953\n",
      "2017-08-13 22:47:25,628 - INFO - step: 1603,loss: 0.266 accuracy: 0.906\n",
      "2017-08-13 22:47:25,692 - INFO - step: 1604,loss: 0.125 accuracy: 0.953\n",
      "2017-08-13 22:47:25,756 - INFO - step: 1605,loss: 0.109 accuracy: 0.969\n",
      "2017-08-13 22:47:25,819 - INFO - step: 1606,loss: 0.143 accuracy: 0.969\n",
      "2017-08-13 22:47:25,883 - INFO - step: 1607,loss: 0.190 accuracy: 0.938\n",
      "2017-08-13 22:47:25,946 - INFO - step: 1608,loss: 0.172 accuracy: 0.953\n",
      "2017-08-13 22:47:26,008 - INFO - step: 1609,loss: 0.268 accuracy: 0.938\n",
      "2017-08-13 22:47:26,073 - INFO - step: 1610,loss: 0.066 accuracy: 0.969\n",
      "2017-08-13 22:47:26,134 - INFO - step: 1611,loss: 0.090 accuracy: 0.969\n",
      "2017-08-13 22:47:26,197 - INFO - step: 1612,loss: 0.138 accuracy: 0.938\n",
      "2017-08-13 22:47:26,269 - INFO - step: 1613,loss: 0.251 accuracy: 0.891\n",
      "2017-08-13 22:47:26,331 - INFO - step: 1614,loss: 0.146 accuracy: 0.953\n",
      "2017-08-13 22:47:26,394 - INFO - step: 1615,loss: 0.099 accuracy: 1.000\n",
      "2017-08-13 22:47:26,458 - INFO - step: 1616,loss: 0.195 accuracy: 0.938\n",
      "2017-08-13 22:47:26,534 - INFO - step: 1617,loss: 0.133 accuracy: 0.969\n",
      "2017-08-13 22:47:26,595 - INFO - step: 1618,loss: 0.113 accuracy: 0.938\n",
      "2017-08-13 22:47:26,658 - INFO - step: 1619,loss: 0.170 accuracy: 0.953\n",
      "2017-08-13 22:47:26,681 - INFO - step: 1620,loss: 0.024 accuracy: 1.000\n",
      "2017-08-13 22:47:26,683 - INFO - \n",
      "Evaluation:\n",
      "2017-08-13 22:47:26,744 - INFO - step: 1620,loss: 0.200 accuracy: 0.904\n",
      "2017-08-13 22:47:26,746 - INFO - \train_epoch:\n",
      "2017-08-13 22:47:26,747 - INFO - loss_total: 0.138 accuracy_total: 0.954\n",
      "2017-08-13 22:47:26,829 - INFO - step: 1621,loss: 0.151 accuracy: 0.922\n",
      "2017-08-13 22:47:26,893 - INFO - step: 1622,loss: 0.119 accuracy: 0.953\n",
      "2017-08-13 22:47:26,956 - INFO - step: 1623,loss: 0.105 accuracy: 0.969\n",
      "2017-08-13 22:47:27,019 - INFO - step: 1624,loss: 0.165 accuracy: 0.938\n",
      "2017-08-13 22:47:27,082 - INFO - step: 1625,loss: 0.094 accuracy: 0.984\n",
      "2017-08-13 22:47:27,146 - INFO - step: 1626,loss: 0.085 accuracy: 0.953\n",
      "2017-08-13 22:47:27,210 - INFO - step: 1627,loss: 0.236 accuracy: 0.875\n",
      "2017-08-13 22:47:27,273 - INFO - step: 1628,loss: 0.297 accuracy: 0.844\n",
      "2017-08-13 22:47:27,336 - INFO - step: 1629,loss: 0.156 accuracy: 0.922\n",
      "2017-08-13 22:47:27,399 - INFO - step: 1630,loss: 0.120 accuracy: 0.938\n",
      "2017-08-13 22:47:27,461 - INFO - step: 1631,loss: 0.094 accuracy: 0.969\n",
      "2017-08-13 22:47:27,525 - INFO - step: 1632,loss: 0.130 accuracy: 0.953\n",
      "2017-08-13 22:47:27,594 - INFO - step: 1633,loss: 0.099 accuracy: 0.969\n",
      "2017-08-13 22:47:27,657 - INFO - step: 1634,loss: 0.161 accuracy: 0.906\n",
      "2017-08-13 22:47:27,721 - INFO - step: 1635,loss: 0.209 accuracy: 0.922\n",
      "2017-08-13 22:47:27,784 - INFO - step: 1636,loss: 0.163 accuracy: 0.922\n",
      "2017-08-13 22:47:27,848 - INFO - step: 1637,loss: 0.064 accuracy: 0.984\n",
      "2017-08-13 22:47:27,910 - INFO - step: 1638,loss: 0.086 accuracy: 0.984\n",
      "2017-08-13 22:47:27,972 - INFO - step: 1639,loss: 0.111 accuracy: 0.938\n",
      "2017-08-13 22:47:28,036 - INFO - step: 1640,loss: 0.102 accuracy: 0.969\n",
      "2017-08-13 22:47:28,099 - INFO - step: 1641,loss: 0.083 accuracy: 0.984\n",
      "2017-08-13 22:47:28,163 - INFO - step: 1642,loss: 0.155 accuracy: 0.938\n",
      "2017-08-13 22:47:28,224 - INFO - step: 1643,loss: 0.154 accuracy: 0.938\n",
      "2017-08-13 22:47:28,289 - INFO - step: 1644,loss: 0.155 accuracy: 0.938\n",
      "2017-08-13 22:47:28,352 - INFO - step: 1645,loss: 0.147 accuracy: 0.953\n",
      "2017-08-13 22:47:28,415 - INFO - step: 1646,loss: 0.130 accuracy: 0.953\n",
      "2017-08-13 22:47:28,478 - INFO - step: 1647,loss: 0.170 accuracy: 0.922\n",
      "2017-08-13 22:47:28,542 - INFO - step: 1648,loss: 0.111 accuracy: 0.953\n",
      "2017-08-13 22:47:28,606 - INFO - step: 1649,loss: 0.084 accuracy: 0.984\n",
      "2017-08-13 22:47:28,670 - INFO - step: 1650,loss: 0.144 accuracy: 0.953\n",
      "2017-08-13 22:47:28,733 - INFO - step: 1651,loss: 0.136 accuracy: 0.938\n",
      "2017-08-13 22:47:28,794 - INFO - step: 1652,loss: 0.133 accuracy: 0.953\n",
      "2017-08-13 22:47:28,859 - INFO - step: 1653,loss: 0.102 accuracy: 0.969\n",
      "2017-08-13 22:47:28,921 - INFO - step: 1654,loss: 0.095 accuracy: 0.969\n",
      "2017-08-13 22:47:28,984 - INFO - step: 1655,loss: 0.152 accuracy: 0.938\n",
      "2017-08-13 22:47:29,006 - INFO - step: 1656,loss: 0.275 accuracy: 0.857\n",
      "2017-08-13 22:47:29,010 - INFO - \train_epoch:\n",
      "2017-08-13 22:47:29,012 - INFO - loss_total: 0.138 accuracy_total: 0.943\n",
      "2017-08-13 22:47:29,094 - INFO - step: 1657,loss: 0.096 accuracy: 0.969\n",
      "2017-08-13 22:47:29,158 - INFO - step: 1658,loss: 0.060 accuracy: 1.000\n",
      "2017-08-13 22:47:29,221 - INFO - step: 1659,loss: 0.083 accuracy: 0.969\n",
      "2017-08-13 22:47:29,284 - INFO - step: 1660,loss: 0.220 accuracy: 0.906\n",
      "2017-08-13 22:47:29,345 - INFO - step: 1661,loss: 0.114 accuracy: 0.938\n",
      "2017-08-13 22:47:29,407 - INFO - step: 1662,loss: 0.076 accuracy: 0.969\n",
      "2017-08-13 22:47:29,474 - INFO - step: 1663,loss: 0.224 accuracy: 0.891\n",
      "2017-08-13 22:47:29,541 - INFO - step: 1664,loss: 0.216 accuracy: 0.922\n",
      "2017-08-13 22:47:29,604 - INFO - step: 1665,loss: 0.131 accuracy: 0.938\n",
      "2017-08-13 22:47:29,667 - INFO - step: 1666,loss: 0.076 accuracy: 0.969\n",
      "2017-08-13 22:47:29,729 - INFO - step: 1667,loss: 0.161 accuracy: 0.953\n",
      "2017-08-13 22:47:29,790 - INFO - step: 1668,loss: 0.187 accuracy: 0.922\n",
      "2017-08-13 22:47:29,851 - INFO - step: 1669,loss: 0.165 accuracy: 0.938\n",
      "2017-08-13 22:47:29,912 - INFO - step: 1670,loss: 0.204 accuracy: 0.922\n",
      "2017-08-13 22:47:29,976 - INFO - step: 1671,loss: 0.179 accuracy: 0.922\n",
      "2017-08-13 22:47:30,039 - INFO - step: 1672,loss: 0.132 accuracy: 0.953\n",
      "2017-08-13 22:47:30,104 - INFO - step: 1673,loss: 0.072 accuracy: 0.969\n",
      "2017-08-13 22:47:30,169 - INFO - step: 1674,loss: 0.148 accuracy: 0.938\n",
      "2017-08-13 22:47:30,232 - INFO - step: 1675,loss: 0.236 accuracy: 0.859\n",
      "2017-08-13 22:47:30,295 - INFO - step: 1676,loss: 0.070 accuracy: 0.984\n",
      "2017-08-13 22:47:30,358 - INFO - step: 1677,loss: 0.149 accuracy: 0.953\n",
      "2017-08-13 22:47:30,422 - INFO - step: 1678,loss: 0.066 accuracy: 0.984\n",
      "2017-08-13 22:47:30,485 - INFO - step: 1679,loss: 0.185 accuracy: 0.922\n",
      "2017-08-13 22:47:30,549 - INFO - step: 1680,loss: 0.111 accuracy: 0.969\n",
      "2017-08-13 22:47:30,613 - INFO - step: 1681,loss: 0.082 accuracy: 0.969\n",
      "2017-08-13 22:47:30,676 - INFO - step: 1682,loss: 0.092 accuracy: 0.984\n",
      "2017-08-13 22:47:30,739 - INFO - step: 1683,loss: 0.078 accuracy: 0.969\n",
      "2017-08-13 22:47:30,803 - INFO - step: 1684,loss: 0.084 accuracy: 0.953\n",
      "2017-08-13 22:47:30,866 - INFO - step: 1685,loss: 0.208 accuracy: 0.906\n",
      "2017-08-13 22:47:30,928 - INFO - step: 1686,loss: 0.080 accuracy: 1.000\n",
      "2017-08-13 22:47:30,991 - INFO - step: 1687,loss: 0.143 accuracy: 0.938\n",
      "2017-08-13 22:47:31,053 - INFO - step: 1688,loss: 0.182 accuracy: 0.938\n",
      "2017-08-13 22:47:31,116 - INFO - step: 1689,loss: 0.117 accuracy: 0.922\n",
      "2017-08-13 22:47:31,179 - INFO - step: 1690,loss: 0.159 accuracy: 0.906\n",
      "2017-08-13 22:47:31,242 - INFO - step: 1691,loss: 0.310 accuracy: 0.938\n",
      "2017-08-13 22:47:31,264 - INFO - step: 1692,loss: 0.049 accuracy: 1.000\n",
      "2017-08-13 22:47:31,266 - INFO - \n",
      "Evaluation:\n",
      "2017-08-13 22:47:31,325 - INFO - step: 1692,loss: 0.202 accuracy: 0.920\n",
      "2017-08-13 22:47:31,421 - INFO - Saving model to /home/zx/cuckoo_1000/session_save/checkpoints/model-1692 at step 46.\n",
      "2017-08-13 22:47:31,424 - INFO - \train_epoch:\n",
      "2017-08-13 22:47:31,424 - INFO - loss_total: 0.137 accuracy_total: 0.947\n",
      "2017-08-13 22:47:31,497 - INFO - step: 1693,loss: 0.167 accuracy: 0.938\n",
      "2017-08-13 22:47:31,560 - INFO - step: 1694,loss: 0.083 accuracy: 0.938\n",
      "2017-08-13 22:47:31,622 - INFO - step: 1695,loss: 0.104 accuracy: 0.953\n",
      "2017-08-13 22:47:31,684 - INFO - step: 1696,loss: 0.054 accuracy: 1.000\n",
      "2017-08-13 22:47:31,747 - INFO - step: 1697,loss: 0.131 accuracy: 0.938\n",
      "2017-08-13 22:47:31,809 - INFO - step: 1698,loss: 0.079 accuracy: 0.969\n",
      "2017-08-13 22:47:31,870 - INFO - step: 1699,loss: 0.113 accuracy: 0.953\n",
      "2017-08-13 22:47:31,933 - INFO - step: 1700,loss: 0.176 accuracy: 0.906\n",
      "2017-08-13 22:47:31,996 - INFO - step: 1701,loss: 0.171 accuracy: 0.953\n",
      "2017-08-13 22:47:32,060 - INFO - step: 1702,loss: 0.070 accuracy: 1.000\n",
      "2017-08-13 22:47:32,124 - INFO - step: 1703,loss: 0.095 accuracy: 0.969\n",
      "2017-08-13 22:47:32,189 - INFO - step: 1704,loss: 0.162 accuracy: 0.953\n",
      "2017-08-13 22:47:32,254 - INFO - step: 1705,loss: 0.099 accuracy: 0.938\n",
      "2017-08-13 22:47:32,318 - INFO - step: 1706,loss: 0.270 accuracy: 0.891\n",
      "2017-08-13 22:47:32,381 - INFO - step: 1707,loss: 0.160 accuracy: 0.938\n",
      "2017-08-13 22:47:32,447 - INFO - step: 1708,loss: 0.129 accuracy: 0.938\n",
      "2017-08-13 22:47:32,509 - INFO - step: 1709,loss: 0.148 accuracy: 0.938\n",
      "2017-08-13 22:47:32,572 - INFO - step: 1710,loss: 0.183 accuracy: 0.922\n",
      "2017-08-13 22:47:32,633 - INFO - step: 1711,loss: 0.198 accuracy: 0.922\n",
      "2017-08-13 22:47:32,695 - INFO - step: 1712,loss: 0.093 accuracy: 0.969\n",
      "2017-08-13 22:47:32,758 - INFO - step: 1713,loss: 0.082 accuracy: 0.969\n",
      "2017-08-13 22:47:32,819 - INFO - step: 1714,loss: 0.156 accuracy: 0.922\n",
      "2017-08-13 22:47:32,882 - INFO - step: 1715,loss: 0.172 accuracy: 0.938\n",
      "2017-08-13 22:47:32,944 - INFO - step: 1716,loss: 0.104 accuracy: 0.953\n",
      "2017-08-13 22:47:33,008 - INFO - step: 1717,loss: 0.059 accuracy: 0.984\n",
      "2017-08-13 22:47:33,072 - INFO - step: 1718,loss: 0.160 accuracy: 0.938\n",
      "2017-08-13 22:47:33,152 - INFO - step: 1719,loss: 0.077 accuracy: 0.984\n",
      "2017-08-13 22:47:33,215 - INFO - step: 1720,loss: 0.118 accuracy: 0.969\n",
      "2017-08-13 22:47:33,278 - INFO - step: 1721,loss: 0.118 accuracy: 0.938\n",
      "2017-08-13 22:47:33,340 - INFO - step: 1722,loss: 0.150 accuracy: 0.922\n",
      "2017-08-13 22:47:33,402 - INFO - step: 1723,loss: 0.118 accuracy: 0.969\n",
      "2017-08-13 22:47:33,464 - INFO - step: 1724,loss: 0.108 accuracy: 0.953\n",
      "2017-08-13 22:47:33,527 - INFO - step: 1725,loss: 0.097 accuracy: 0.969\n",
      "2017-08-13 22:47:33,591 - INFO - step: 1726,loss: 0.138 accuracy: 0.938\n",
      "2017-08-13 22:47:33,655 - INFO - step: 1727,loss: 0.097 accuracy: 0.969\n",
      "2017-08-13 22:47:33,680 - INFO - step: 1728,loss: 0.098 accuracy: 1.000\n",
      "2017-08-13 22:47:33,682 - INFO - \train_epoch:\n",
      "2017-08-13 22:47:33,684 - INFO - loss_total: 0.126 accuracy_total: 0.951\n",
      "2017-08-13 22:47:33,768 - INFO - step: 1729,loss: 0.146 accuracy: 0.953\n",
      "2017-08-13 22:47:33,830 - INFO - step: 1730,loss: 0.157 accuracy: 0.938\n",
      "2017-08-13 22:47:33,892 - INFO - step: 1731,loss: 0.132 accuracy: 0.938\n",
      "2017-08-13 22:47:33,955 - INFO - step: 1732,loss: 0.046 accuracy: 0.984\n",
      "2017-08-13 22:47:34,019 - INFO - step: 1733,loss: 0.227 accuracy: 0.922\n",
      "2017-08-13 22:47:34,083 - INFO - step: 1734,loss: 0.099 accuracy: 0.984\n",
      "2017-08-13 22:47:34,147 - INFO - step: 1735,loss: 0.144 accuracy: 0.938\n",
      "2017-08-13 22:47:34,236 - INFO - step: 1736,loss: 0.044 accuracy: 1.000\n",
      "2017-08-13 22:47:34,300 - INFO - step: 1737,loss: 0.165 accuracy: 0.938\n",
      "2017-08-13 22:47:34,363 - INFO - step: 1738,loss: 0.112 accuracy: 0.969\n",
      "2017-08-13 22:47:34,427 - INFO - step: 1739,loss: 0.104 accuracy: 0.953\n",
      "2017-08-13 22:47:34,490 - INFO - step: 1740,loss: 0.138 accuracy: 0.906\n",
      "2017-08-13 22:47:34,557 - INFO - step: 1741,loss: 0.059 accuracy: 0.969\n",
      "2017-08-13 22:47:34,623 - INFO - step: 1742,loss: 0.113 accuracy: 0.969\n",
      "2017-08-13 22:47:34,688 - INFO - step: 1743,loss: 0.144 accuracy: 0.922\n",
      "2017-08-13 22:47:34,753 - INFO - step: 1744,loss: 0.156 accuracy: 0.938\n",
      "2017-08-13 22:47:34,816 - INFO - step: 1745,loss: 0.157 accuracy: 0.922\n",
      "2017-08-13 22:47:34,878 - INFO - step: 1746,loss: 0.092 accuracy: 0.969\n",
      "2017-08-13 22:47:34,939 - INFO - step: 1747,loss: 0.135 accuracy: 0.922\n",
      "2017-08-13 22:47:35,004 - INFO - step: 1748,loss: 0.108 accuracy: 0.953\n",
      "2017-08-13 22:47:35,068 - INFO - step: 1749,loss: 0.145 accuracy: 0.953\n",
      "2017-08-13 22:47:35,132 - INFO - step: 1750,loss: 0.082 accuracy: 0.969\n",
      "2017-08-13 22:47:35,194 - INFO - step: 1751,loss: 0.179 accuracy: 0.906\n",
      "2017-08-13 22:47:35,256 - INFO - step: 1752,loss: 0.147 accuracy: 0.938\n",
      "2017-08-13 22:47:35,321 - INFO - step: 1753,loss: 0.320 accuracy: 0.891\n",
      "2017-08-13 22:47:35,384 - INFO - step: 1754,loss: 0.144 accuracy: 0.922\n",
      "2017-08-13 22:47:35,448 - INFO - step: 1755,loss: 0.092 accuracy: 0.953\n",
      "2017-08-13 22:47:35,510 - INFO - step: 1756,loss: 0.123 accuracy: 0.953\n",
      "2017-08-13 22:47:35,576 - INFO - step: 1757,loss: 0.111 accuracy: 0.953\n",
      "2017-08-13 22:47:35,639 - INFO - step: 1758,loss: 0.226 accuracy: 0.969\n",
      "2017-08-13 22:47:35,703 - INFO - step: 1759,loss: 0.171 accuracy: 0.938\n",
      "2017-08-13 22:47:35,768 - INFO - step: 1760,loss: 0.130 accuracy: 0.953\n",
      "2017-08-13 22:47:35,831 - INFO - step: 1761,loss: 0.163 accuracy: 0.969\n",
      "2017-08-13 22:47:35,896 - INFO - step: 1762,loss: 0.079 accuracy: 0.984\n",
      "2017-08-13 22:47:35,960 - INFO - step: 1763,loss: 0.104 accuracy: 0.984\n",
      "2017-08-13 22:47:35,984 - INFO - step: 1764,loss: 0.145 accuracy: 1.000\n",
      "2017-08-13 22:47:35,987 - INFO - \n",
      "Evaluation:\n",
      "2017-08-13 22:47:36,047 - INFO - step: 1764,loss: 0.210 accuracy: 0.924\n",
      "2017-08-13 22:47:36,157 - INFO - Saving model to /home/zx/cuckoo_1000/session_save/checkpoints/model-1764 at step 48.\n",
      "2017-08-13 22:47:36,160 - INFO - \train_epoch:\n",
      "2017-08-13 22:47:36,161 - INFO - loss_total: 0.134 accuracy_total: 0.951\n",
      "2017-08-13 22:47:36,230 - INFO - step: 1765,loss: 0.159 accuracy: 0.953\n",
      "2017-08-13 22:47:36,293 - INFO - step: 1766,loss: 0.126 accuracy: 0.938\n",
      "2017-08-13 22:47:36,357 - INFO - step: 1767,loss: 0.095 accuracy: 0.969\n",
      "2017-08-13 22:47:36,421 - INFO - step: 1768,loss: 0.146 accuracy: 0.938\n",
      "2017-08-13 22:47:36,480 - INFO - step: 1769,loss: 0.092 accuracy: 0.938\n",
      "2017-08-13 22:47:36,541 - INFO - step: 1770,loss: 0.217 accuracy: 0.906\n",
      "2017-08-13 22:47:36,605 - INFO - step: 1771,loss: 0.107 accuracy: 0.953\n",
      "2017-08-13 22:47:36,669 - INFO - step: 1772,loss: 0.211 accuracy: 0.906\n",
      "2017-08-13 22:47:36,734 - INFO - step: 1773,loss: 0.117 accuracy: 0.969\n",
      "2017-08-13 22:47:36,799 - INFO - step: 1774,loss: 0.100 accuracy: 0.953\n",
      "2017-08-13 22:47:36,863 - INFO - step: 1775,loss: 0.116 accuracy: 0.938\n",
      "2017-08-13 22:47:36,922 - INFO - step: 1776,loss: 0.092 accuracy: 0.953\n",
      "2017-08-13 22:47:36,984 - INFO - step: 1777,loss: 0.216 accuracy: 0.891\n",
      "2017-08-13 22:47:37,038 - INFO - step: 1778,loss: 0.184 accuracy: 0.922\n",
      "2017-08-13 22:47:37,101 - INFO - step: 1779,loss: 0.071 accuracy: 0.984\n",
      "2017-08-13 22:47:37,162 - INFO - step: 1780,loss: 0.194 accuracy: 0.969\n",
      "2017-08-13 22:47:37,225 - INFO - step: 1781,loss: 0.118 accuracy: 0.969\n",
      "2017-08-13 22:47:37,287 - INFO - step: 1782,loss: 0.163 accuracy: 0.938\n",
      "2017-08-13 22:47:37,350 - INFO - step: 1783,loss: 0.130 accuracy: 0.938\n",
      "2017-08-13 22:47:37,413 - INFO - step: 1784,loss: 0.117 accuracy: 0.953\n",
      "2017-08-13 22:47:37,474 - INFO - step: 1785,loss: 0.089 accuracy: 0.969\n",
      "2017-08-13 22:47:37,539 - INFO - step: 1786,loss: 0.115 accuracy: 0.953\n",
      "2017-08-13 22:47:37,603 - INFO - step: 1787,loss: 0.143 accuracy: 0.953\n",
      "2017-08-13 22:47:37,667 - INFO - step: 1788,loss: 0.196 accuracy: 0.922\n",
      "2017-08-13 22:47:37,734 - INFO - step: 1789,loss: 0.100 accuracy: 0.969\n",
      "2017-08-13 22:47:37,798 - INFO - step: 1790,loss: 0.062 accuracy: 0.984\n",
      "2017-08-13 22:47:37,861 - INFO - step: 1791,loss: 0.248 accuracy: 0.906\n",
      "2017-08-13 22:47:37,924 - INFO - step: 1792,loss: 0.133 accuracy: 0.922\n",
      "2017-08-13 22:47:37,985 - INFO - step: 1793,loss: 0.079 accuracy: 0.984\n",
      "2017-08-13 22:47:38,047 - INFO - step: 1794,loss: 0.123 accuracy: 0.953\n",
      "2017-08-13 22:47:38,109 - INFO - step: 1795,loss: 0.049 accuracy: 1.000\n",
      "2017-08-13 22:47:38,173 - INFO - step: 1796,loss: 0.129 accuracy: 0.938\n",
      "2017-08-13 22:47:38,237 - INFO - step: 1797,loss: 0.130 accuracy: 0.953\n",
      "2017-08-13 22:47:38,301 - INFO - step: 1798,loss: 0.124 accuracy: 0.922\n",
      "2017-08-13 22:47:38,367 - INFO - step: 1799,loss: 0.072 accuracy: 0.969\n",
      "2017-08-13 22:47:38,392 - INFO - step: 1800,loss: 0.039 accuracy: 1.000\n",
      "2017-08-13 22:47:38,394 - INFO - \train_epoch:\n",
      "2017-08-13 22:47:38,396 - INFO - loss_total: 0.128 accuracy_total: 0.949\n",
      "2017-08-13 22:47:38,478 - INFO - step: 1801,loss: 0.065 accuracy: 0.969\n",
      "2017-08-13 22:47:38,543 - INFO - step: 1802,loss: 0.143 accuracy: 0.906\n",
      "2017-08-13 22:47:38,605 - INFO - step: 1803,loss: 0.101 accuracy: 0.953\n",
      "2017-08-13 22:47:38,668 - INFO - step: 1804,loss: 0.095 accuracy: 0.953\n",
      "2017-08-13 22:47:38,732 - INFO - step: 1805,loss: 0.119 accuracy: 0.953\n",
      "2017-08-13 22:47:38,795 - INFO - step: 1806,loss: 0.065 accuracy: 0.969\n",
      "2017-08-13 22:47:38,857 - INFO - step: 1807,loss: 0.083 accuracy: 1.000\n",
      "2017-08-13 22:47:38,922 - INFO - step: 1808,loss: 0.178 accuracy: 0.938\n",
      "2017-08-13 22:47:38,987 - INFO - step: 1809,loss: 0.088 accuracy: 0.953\n",
      "2017-08-13 22:47:39,051 - INFO - step: 1810,loss: 0.101 accuracy: 0.969\n",
      "2017-08-13 22:47:39,115 - INFO - step: 1811,loss: 0.079 accuracy: 0.953\n",
      "2017-08-13 22:47:39,179 - INFO - step: 1812,loss: 0.073 accuracy: 0.984\n",
      "2017-08-13 22:47:39,243 - INFO - step: 1813,loss: 0.076 accuracy: 0.953\n",
      "2017-08-13 22:47:39,305 - INFO - step: 1814,loss: 0.160 accuracy: 0.922\n",
      "2017-08-13 22:47:39,368 - INFO - step: 1815,loss: 0.295 accuracy: 0.922\n",
      "2017-08-13 22:47:39,434 - INFO - step: 1816,loss: 0.205 accuracy: 0.922\n",
      "2017-08-13 22:47:39,500 - INFO - step: 1817,loss: 0.116 accuracy: 0.953\n",
      "2017-08-13 22:47:39,562 - INFO - step: 1818,loss: 0.125 accuracy: 0.953\n",
      "2017-08-13 22:47:39,625 - INFO - step: 1819,loss: 0.075 accuracy: 0.969\n",
      "2017-08-13 22:47:39,691 - INFO - step: 1820,loss: 0.229 accuracy: 0.891\n",
      "2017-08-13 22:47:39,753 - INFO - step: 1821,loss: 0.149 accuracy: 0.953\n",
      "2017-08-13 22:47:39,820 - INFO - step: 1822,loss: 0.169 accuracy: 0.953\n",
      "2017-08-13 22:47:39,886 - INFO - step: 1823,loss: 0.131 accuracy: 0.938\n",
      "2017-08-13 22:47:39,948 - INFO - step: 1824,loss: 0.107 accuracy: 0.969\n",
      "2017-08-13 22:47:40,010 - INFO - step: 1825,loss: 0.162 accuracy: 0.922\n",
      "2017-08-13 22:47:40,072 - INFO - step: 1826,loss: 0.139 accuracy: 0.922\n",
      "2017-08-13 22:47:40,135 - INFO - step: 1827,loss: 0.190 accuracy: 0.938\n",
      "2017-08-13 22:47:40,197 - INFO - step: 1828,loss: 0.094 accuracy: 0.984\n",
      "2017-08-13 22:47:40,259 - INFO - step: 1829,loss: 0.069 accuracy: 1.000\n",
      "2017-08-13 22:47:40,321 - INFO - step: 1830,loss: 0.079 accuracy: 0.953\n",
      "2017-08-13 22:47:40,384 - INFO - step: 1831,loss: 0.132 accuracy: 0.938\n",
      "2017-08-13 22:47:40,447 - INFO - step: 1832,loss: 0.115 accuracy: 0.969\n",
      "2017-08-13 22:47:40,510 - INFO - step: 1833,loss: 0.083 accuracy: 0.984\n",
      "2017-08-13 22:47:40,579 - INFO - step: 1834,loss: 0.204 accuracy: 0.938\n",
      "2017-08-13 22:47:40,643 - INFO - step: 1835,loss: 0.075 accuracy: 0.984\n",
      "2017-08-13 22:47:40,668 - INFO - step: 1836,loss: 0.018 accuracy: 1.000\n",
      "2017-08-13 22:47:40,670 - INFO - \n",
      "Evaluation:\n",
      "2017-08-13 22:47:40,750 - INFO - step: 1836,loss: 0.201 accuracy: 0.924\n",
      "2017-08-13 22:47:40,751 - INFO - \train_epoch:\n",
      "2017-08-13 22:47:40,753 - INFO - loss_total: 0.122 accuracy_total: 0.954\n",
      "2017-08-13 22:47:40,835 - INFO - step: 1837,loss: 0.185 accuracy: 0.938\n",
      "2017-08-13 22:47:40,898 - INFO - step: 1838,loss: 0.064 accuracy: 0.969\n",
      "2017-08-13 22:47:40,960 - INFO - step: 1839,loss: 0.276 accuracy: 0.953\n",
      "2017-08-13 22:47:41,023 - INFO - step: 1840,loss: 0.104 accuracy: 0.953\n",
      "2017-08-13 22:47:41,086 - INFO - step: 1841,loss: 0.158 accuracy: 0.969\n",
      "2017-08-13 22:47:41,148 - INFO - step: 1842,loss: 0.066 accuracy: 0.969\n",
      "2017-08-13 22:47:41,211 - INFO - step: 1843,loss: 0.081 accuracy: 0.969\n",
      "2017-08-13 22:47:41,274 - INFO - step: 1844,loss: 0.139 accuracy: 0.969\n",
      "2017-08-13 22:47:41,335 - INFO - step: 1845,loss: 0.100 accuracy: 0.969\n",
      "2017-08-13 22:47:41,398 - INFO - step: 1846,loss: 0.193 accuracy: 0.969\n",
      "2017-08-13 22:47:41,463 - INFO - step: 1847,loss: 0.095 accuracy: 0.969\n",
      "2017-08-13 22:47:41,527 - INFO - step: 1848,loss: 0.102 accuracy: 0.969\n",
      "2017-08-13 22:47:41,590 - INFO - step: 1849,loss: 0.124 accuracy: 0.953\n",
      "2017-08-13 22:47:41,669 - INFO - step: 1850,loss: 0.073 accuracy: 0.984\n",
      "2017-08-13 22:47:41,732 - INFO - step: 1851,loss: 0.139 accuracy: 0.969\n",
      "2017-08-13 22:47:41,798 - INFO - step: 1852,loss: 0.099 accuracy: 0.969\n",
      "2017-08-13 22:47:41,860 - INFO - step: 1853,loss: 0.149 accuracy: 0.938\n",
      "2017-08-13 22:47:41,923 - INFO - step: 1854,loss: 0.094 accuracy: 0.938\n",
      "2017-08-13 22:47:41,986 - INFO - step: 1855,loss: 0.132 accuracy: 0.922\n",
      "2017-08-13 22:47:42,050 - INFO - step: 1856,loss: 0.207 accuracy: 0.922\n",
      "2017-08-13 22:47:42,114 - INFO - step: 1857,loss: 0.113 accuracy: 0.953\n",
      "2017-08-13 22:47:42,176 - INFO - step: 1858,loss: 0.139 accuracy: 0.953\n",
      "2017-08-13 22:47:42,242 - INFO - step: 1859,loss: 0.167 accuracy: 0.938\n",
      "2017-08-13 22:47:42,305 - INFO - step: 1860,loss: 0.199 accuracy: 0.891\n",
      "2017-08-13 22:47:42,368 - INFO - step: 1861,loss: 0.082 accuracy: 0.969\n",
      "2017-08-13 22:47:42,431 - INFO - step: 1862,loss: 0.167 accuracy: 0.922\n",
      "2017-08-13 22:47:42,496 - INFO - step: 1863,loss: 0.132 accuracy: 0.938\n",
      "2017-08-13 22:47:42,561 - INFO - step: 1864,loss: 0.146 accuracy: 0.953\n",
      "2017-08-13 22:47:42,625 - INFO - step: 1865,loss: 0.116 accuracy: 0.953\n",
      "2017-08-13 22:47:42,689 - INFO - step: 1866,loss: 0.126 accuracy: 0.953\n",
      "2017-08-13 22:47:42,752 - INFO - step: 1867,loss: 0.130 accuracy: 0.953\n",
      "2017-08-13 22:47:42,809 - INFO - step: 1868,loss: 0.098 accuracy: 0.953\n",
      "2017-08-13 22:47:42,871 - INFO - step: 1869,loss: 0.092 accuracy: 0.969\n",
      "2017-08-13 22:47:42,934 - INFO - step: 1870,loss: 0.108 accuracy: 0.953\n",
      "2017-08-13 22:47:42,997 - INFO - step: 1871,loss: 0.105 accuracy: 0.953\n",
      "2017-08-13 22:47:43,020 - INFO - step: 1872,loss: 0.001 accuracy: 1.000\n",
      "2017-08-13 22:47:43,023 - INFO - \train_epoch:\n",
      "2017-08-13 22:47:43,024 - INFO - loss_total: 0.125 accuracy_total: 0.954\n",
      "2017-08-13 22:47:43,097 - INFO - step: 1873,loss: 0.116 accuracy: 0.953\n",
      "2017-08-13 22:47:43,161 - INFO - step: 1874,loss: 0.125 accuracy: 0.938\n",
      "2017-08-13 22:47:43,224 - INFO - step: 1875,loss: 0.099 accuracy: 0.953\n",
      "2017-08-13 22:47:43,289 - INFO - step: 1876,loss: 0.081 accuracy: 0.984\n",
      "2017-08-13 22:47:43,354 - INFO - step: 1877,loss: 0.082 accuracy: 0.984\n",
      "2017-08-13 22:47:43,417 - INFO - step: 1878,loss: 0.031 accuracy: 1.000\n",
      "2017-08-13 22:47:43,480 - INFO - step: 1879,loss: 0.053 accuracy: 0.984\n",
      "2017-08-13 22:47:43,542 - INFO - step: 1880,loss: 0.088 accuracy: 0.969\n",
      "2017-08-13 22:47:43,606 - INFO - step: 1881,loss: 0.074 accuracy: 0.969\n",
      "2017-08-13 22:47:43,670 - INFO - step: 1882,loss: 0.181 accuracy: 0.938\n",
      "2017-08-13 22:47:43,741 - INFO - step: 1883,loss: 0.078 accuracy: 0.969\n",
      "2017-08-13 22:47:43,805 - INFO - step: 1884,loss: 0.164 accuracy: 0.953\n",
      "2017-08-13 22:47:43,868 - INFO - step: 1885,loss: 0.124 accuracy: 0.938\n",
      "2017-08-13 22:47:43,932 - INFO - step: 1886,loss: 0.055 accuracy: 0.984\n",
      "2017-08-13 22:47:43,995 - INFO - step: 1887,loss: 0.180 accuracy: 0.938\n",
      "2017-08-13 22:47:44,058 - INFO - step: 1888,loss: 0.158 accuracy: 0.953\n",
      "2017-08-13 22:47:44,120 - INFO - step: 1889,loss: 0.091 accuracy: 0.969\n",
      "2017-08-13 22:47:44,181 - INFO - step: 1890,loss: 0.172 accuracy: 0.906\n",
      "2017-08-13 22:47:44,246 - INFO - step: 1891,loss: 0.153 accuracy: 0.938\n",
      "2017-08-13 22:47:44,311 - INFO - step: 1892,loss: 0.113 accuracy: 0.953\n",
      "2017-08-13 22:47:44,373 - INFO - step: 1893,loss: 0.139 accuracy: 0.938\n",
      "2017-08-13 22:47:44,438 - INFO - step: 1894,loss: 0.080 accuracy: 0.984\n",
      "2017-08-13 22:47:44,494 - INFO - step: 1895,loss: 0.099 accuracy: 0.938\n",
      "2017-08-13 22:47:44,558 - INFO - step: 1896,loss: 0.133 accuracy: 0.969\n",
      "2017-08-13 22:47:44,622 - INFO - step: 1897,loss: 0.170 accuracy: 0.953\n",
      "2017-08-13 22:47:44,685 - INFO - step: 1898,loss: 0.115 accuracy: 0.969\n",
      "2017-08-13 22:47:44,748 - INFO - step: 1899,loss: 0.155 accuracy: 0.969\n",
      "2017-08-13 22:47:44,812 - INFO - step: 1900,loss: 0.135 accuracy: 0.953\n",
      "2017-08-13 22:47:44,875 - INFO - step: 1901,loss: 0.265 accuracy: 0.891\n",
      "2017-08-13 22:47:44,940 - INFO - step: 1902,loss: 0.073 accuracy: 0.969\n",
      "2017-08-13 22:47:45,004 - INFO - step: 1903,loss: 0.164 accuracy: 0.922\n",
      "2017-08-13 22:47:45,085 - INFO - step: 1904,loss: 0.134 accuracy: 0.969\n",
      "2017-08-13 22:47:45,149 - INFO - step: 1905,loss: 0.055 accuracy: 0.984\n",
      "2017-08-13 22:47:45,213 - INFO - step: 1906,loss: 0.161 accuracy: 0.953\n",
      "2017-08-13 22:47:45,274 - INFO - step: 1907,loss: 0.096 accuracy: 0.953\n",
      "2017-08-13 22:47:45,297 - INFO - step: 1908,loss: 0.008 accuracy: 1.000\n",
      "2017-08-13 22:47:45,299 - INFO - \n",
      "Evaluation:\n",
      "2017-08-13 22:47:45,361 - INFO - step: 1908,loss: 0.209 accuracy: 0.924\n",
      "2017-08-13 22:47:45,458 - INFO - Saving model to /home/zx/cuckoo_1000/session_save/checkpoints/model-1908 at step 52.\n",
      "2017-08-13 22:47:45,460 - INFO - \train_epoch:\n",
      "2017-08-13 22:47:45,461 - INFO - loss_total: 0.117 accuracy_total: 0.958\n",
      "2017-08-13 22:47:45,538 - INFO - step: 1909,loss: 0.065 accuracy: 0.984\n",
      "2017-08-13 22:47:45,601 - INFO - step: 1910,loss: 0.162 accuracy: 0.953\n",
      "2017-08-13 22:47:45,665 - INFO - step: 1911,loss: 0.112 accuracy: 0.969\n",
      "2017-08-13 22:47:45,729 - INFO - step: 1912,loss: 0.078 accuracy: 0.969\n",
      "2017-08-13 22:47:45,791 - INFO - step: 1913,loss: 0.105 accuracy: 0.922\n",
      "2017-08-13 22:47:45,856 - INFO - step: 1914,loss: 0.165 accuracy: 0.953\n",
      "2017-08-13 22:47:45,921 - INFO - step: 1915,loss: 0.200 accuracy: 0.953\n",
      "2017-08-13 22:47:45,983 - INFO - step: 1916,loss: 0.154 accuracy: 0.938\n",
      "2017-08-13 22:47:46,046 - INFO - step: 1917,loss: 0.084 accuracy: 0.984\n",
      "2017-08-13 22:47:46,108 - INFO - step: 1918,loss: 0.121 accuracy: 0.938\n",
      "2017-08-13 22:47:46,172 - INFO - step: 1919,loss: 0.058 accuracy: 0.984\n",
      "2017-08-13 22:47:46,236 - INFO - step: 1920,loss: 0.141 accuracy: 0.938\n",
      "2017-08-13 22:47:46,298 - INFO - step: 1921,loss: 0.123 accuracy: 0.969\n",
      "2017-08-13 22:47:46,363 - INFO - step: 1922,loss: 0.066 accuracy: 0.969\n",
      "2017-08-13 22:47:46,426 - INFO - step: 1923,loss: 0.265 accuracy: 0.891\n",
      "2017-08-13 22:47:46,491 - INFO - step: 1924,loss: 0.099 accuracy: 0.969\n",
      "2017-08-13 22:47:46,555 - INFO - step: 1925,loss: 0.129 accuracy: 0.969\n",
      "2017-08-13 22:47:46,618 - INFO - step: 1926,loss: 0.062 accuracy: 0.984\n",
      "2017-08-13 22:47:46,703 - INFO - step: 1927,loss: 0.092 accuracy: 0.969\n",
      "2017-08-13 22:47:46,766 - INFO - step: 1928,loss: 0.173 accuracy: 0.922\n",
      "2017-08-13 22:47:46,831 - INFO - step: 1929,loss: 0.087 accuracy: 0.953\n",
      "2017-08-13 22:47:46,895 - INFO - step: 1930,loss: 0.170 accuracy: 0.906\n",
      "2017-08-13 22:47:46,959 - INFO - step: 1931,loss: 0.047 accuracy: 0.984\n",
      "2017-08-13 22:47:47,020 - INFO - step: 1932,loss: 0.164 accuracy: 0.953\n",
      "2017-08-13 22:47:47,083 - INFO - step: 1933,loss: 0.129 accuracy: 0.938\n",
      "2017-08-13 22:47:47,147 - INFO - step: 1934,loss: 0.069 accuracy: 0.984\n",
      "2017-08-13 22:47:47,211 - INFO - step: 1935,loss: 0.078 accuracy: 0.969\n",
      "2017-08-13 22:47:47,275 - INFO - step: 1936,loss: 0.070 accuracy: 0.984\n",
      "2017-08-13 22:47:47,341 - INFO - step: 1937,loss: 0.088 accuracy: 0.984\n",
      "2017-08-13 22:47:47,403 - INFO - step: 1938,loss: 0.049 accuracy: 0.984\n",
      "2017-08-13 22:47:47,465 - INFO - step: 1939,loss: 0.090 accuracy: 0.969\n",
      "2017-08-13 22:47:47,529 - INFO - step: 1940,loss: 0.178 accuracy: 0.953\n",
      "2017-08-13 22:47:47,592 - INFO - step: 1941,loss: 0.051 accuracy: 1.000\n",
      "2017-08-13 22:47:47,654 - INFO - step: 1942,loss: 0.224 accuracy: 0.891\n",
      "2017-08-13 22:47:47,718 - INFO - step: 1943,loss: 0.090 accuracy: 0.969\n",
      "2017-08-13 22:47:47,738 - INFO - step: 1944,loss: 0.152 accuracy: 1.000\n",
      "2017-08-13 22:47:47,741 - INFO - \train_epoch:\n",
      "2017-08-13 22:47:47,742 - INFO - loss_total: 0.116 accuracy_total: 0.960\n",
      "2017-08-13 22:47:47,827 - INFO - step: 1945,loss: 0.094 accuracy: 0.969\n",
      "2017-08-13 22:47:47,890 - INFO - step: 1946,loss: 0.233 accuracy: 0.906\n",
      "2017-08-13 22:47:47,952 - INFO - step: 1947,loss: 0.057 accuracy: 0.969\n",
      "2017-08-13 22:47:48,015 - INFO - step: 1948,loss: 0.122 accuracy: 0.922\n",
      "2017-08-13 22:47:48,083 - INFO - step: 1949,loss: 0.096 accuracy: 0.969\n",
      "2017-08-13 22:47:48,147 - INFO - step: 1950,loss: 0.096 accuracy: 0.969\n",
      "2017-08-13 22:47:48,209 - INFO - step: 1951,loss: 0.104 accuracy: 0.969\n",
      "2017-08-13 22:47:48,270 - INFO - step: 1952,loss: 0.071 accuracy: 0.969\n",
      "2017-08-13 22:47:48,333 - INFO - step: 1953,loss: 0.084 accuracy: 0.969\n",
      "2017-08-13 22:47:48,397 - INFO - step: 1954,loss: 0.084 accuracy: 0.969\n",
      "2017-08-13 22:47:48,461 - INFO - step: 1955,loss: 0.063 accuracy: 0.984\n",
      "2017-08-13 22:47:48,525 - INFO - step: 1956,loss: 0.080 accuracy: 0.969\n",
      "2017-08-13 22:47:48,586 - INFO - step: 1957,loss: 0.149 accuracy: 0.938\n",
      "2017-08-13 22:47:48,650 - INFO - step: 1958,loss: 0.061 accuracy: 0.984\n",
      "2017-08-13 22:47:48,712 - INFO - step: 1959,loss: 0.123 accuracy: 0.938\n",
      "2017-08-13 22:47:48,777 - INFO - step: 1960,loss: 0.185 accuracy: 0.938\n",
      "2017-08-13 22:47:48,840 - INFO - step: 1961,loss: 0.138 accuracy: 0.953\n",
      "2017-08-13 22:47:48,903 - INFO - step: 1962,loss: 0.095 accuracy: 0.984\n",
      "2017-08-13 22:47:48,966 - INFO - step: 1963,loss: 0.141 accuracy: 0.938\n",
      "2017-08-13 22:47:49,030 - INFO - step: 1964,loss: 0.034 accuracy: 1.000\n",
      "2017-08-13 22:47:49,093 - INFO - step: 1965,loss: 0.089 accuracy: 0.969\n",
      "2017-08-13 22:47:49,157 - INFO - step: 1966,loss: 0.047 accuracy: 0.984\n",
      "2017-08-13 22:47:49,220 - INFO - step: 1967,loss: 0.164 accuracy: 0.969\n",
      "2017-08-13 22:47:49,283 - INFO - step: 1968,loss: 0.075 accuracy: 0.969\n",
      "2017-08-13 22:47:49,347 - INFO - step: 1969,loss: 0.053 accuracy: 0.984\n",
      "2017-08-13 22:47:49,408 - INFO - step: 1970,loss: 0.258 accuracy: 0.906\n",
      "2017-08-13 22:47:49,472 - INFO - step: 1971,loss: 0.060 accuracy: 0.984\n",
      "2017-08-13 22:47:49,536 - INFO - step: 1972,loss: 0.108 accuracy: 0.953\n",
      "2017-08-13 22:47:49,600 - INFO - step: 1973,loss: 0.123 accuracy: 0.984\n",
      "2017-08-13 22:47:49,664 - INFO - step: 1974,loss: 0.289 accuracy: 0.938\n",
      "2017-08-13 22:47:49,727 - INFO - step: 1975,loss: 0.069 accuracy: 0.969\n",
      "2017-08-13 22:47:49,789 - INFO - step: 1976,loss: 0.205 accuracy: 0.922\n",
      "2017-08-13 22:47:49,853 - INFO - step: 1977,loss: 0.103 accuracy: 0.953\n",
      "2017-08-13 22:47:49,918 - INFO - step: 1978,loss: 0.214 accuracy: 0.953\n",
      "2017-08-13 22:47:49,984 - INFO - step: 1979,loss: 0.265 accuracy: 0.875\n",
      "2017-08-13 22:47:50,007 - INFO - step: 1980,loss: 0.007 accuracy: 1.000\n",
      "2017-08-13 22:47:50,010 - INFO - \n",
      "Evaluation:\n",
      "2017-08-13 22:47:50,077 - INFO - step: 1980,loss: 0.206 accuracy: 0.920\n",
      "2017-08-13 22:47:50,078 - INFO - \train_epoch:\n",
      "2017-08-13 22:47:50,080 - INFO - loss_total: 0.118 accuracy_total: 0.959\n",
      "2017-08-13 22:47:50,162 - INFO - step: 1981,loss: 0.140 accuracy: 0.953\n",
      "2017-08-13 22:47:50,225 - INFO - step: 1982,loss: 0.073 accuracy: 0.984\n",
      "2017-08-13 22:47:50,289 - INFO - step: 1983,loss: 0.061 accuracy: 0.969\n",
      "2017-08-13 22:47:50,351 - INFO - step: 1984,loss: 0.088 accuracy: 0.984\n",
      "2017-08-13 22:47:50,414 - INFO - step: 1985,loss: 0.122 accuracy: 0.938\n",
      "2017-08-13 22:47:50,478 - INFO - step: 1986,loss: 0.059 accuracy: 0.984\n",
      "2017-08-13 22:47:50,540 - INFO - step: 1987,loss: 0.098 accuracy: 0.969\n",
      "2017-08-13 22:47:50,603 - INFO - step: 1988,loss: 0.114 accuracy: 0.938\n",
      "2017-08-13 22:47:50,666 - INFO - step: 1989,loss: 0.106 accuracy: 0.953\n",
      "2017-08-13 22:47:50,729 - INFO - step: 1990,loss: 0.137 accuracy: 0.922\n",
      "2017-08-13 22:47:50,791 - INFO - step: 1991,loss: 0.104 accuracy: 0.969\n",
      "2017-08-13 22:47:50,857 - INFO - step: 1992,loss: 0.264 accuracy: 0.922\n",
      "2017-08-13 22:47:50,920 - INFO - step: 1993,loss: 0.078 accuracy: 0.969\n",
      "2017-08-13 22:47:50,983 - INFO - step: 1994,loss: 0.130 accuracy: 0.953\n",
      "2017-08-13 22:47:51,047 - INFO - step: 1995,loss: 0.269 accuracy: 0.938\n",
      "2017-08-13 22:47:51,113 - INFO - step: 1996,loss: 0.104 accuracy: 0.953\n",
      "2017-08-13 22:47:51,176 - INFO - step: 1997,loss: 0.098 accuracy: 0.969\n",
      "2017-08-13 22:47:51,243 - INFO - step: 1998,loss: 0.110 accuracy: 0.953\n",
      "2017-08-13 22:47:51,306 - INFO - step: 1999,loss: 0.167 accuracy: 0.922\n",
      "2017-08-13 22:47:51,368 - INFO - step: 2000,loss: 0.068 accuracy: 0.984\n",
      "2017-08-13 22:47:51,432 - INFO - step: 2001,loss: 0.044 accuracy: 0.984\n",
      "2017-08-13 22:47:51,497 - INFO - step: 2002,loss: 0.159 accuracy: 0.938\n",
      "2017-08-13 22:47:51,558 - INFO - step: 2003,loss: 0.104 accuracy: 0.984\n",
      "2017-08-13 22:47:51,621 - INFO - step: 2004,loss: 0.068 accuracy: 0.984\n",
      "2017-08-13 22:47:51,688 - INFO - step: 2005,loss: 0.102 accuracy: 0.953\n",
      "2017-08-13 22:47:51,752 - INFO - step: 2006,loss: 0.099 accuracy: 0.953\n",
      "2017-08-13 22:47:51,815 - INFO - step: 2007,loss: 0.131 accuracy: 0.953\n",
      "2017-08-13 22:47:51,878 - INFO - step: 2008,loss: 0.075 accuracy: 0.969\n",
      "2017-08-13 22:47:51,940 - INFO - step: 2009,loss: 0.109 accuracy: 0.969\n",
      "2017-08-13 22:47:52,005 - INFO - step: 2010,loss: 0.178 accuracy: 0.938\n",
      "2017-08-13 22:47:52,068 - INFO - step: 2011,loss: 0.147 accuracy: 0.922\n",
      "2017-08-13 22:47:52,130 - INFO - step: 2012,loss: 0.127 accuracy: 0.953\n",
      "2017-08-13 22:47:52,190 - INFO - step: 2013,loss: 0.112 accuracy: 0.953\n",
      "2017-08-13 22:47:52,253 - INFO - step: 2014,loss: 0.074 accuracy: 0.969\n",
      "2017-08-13 22:47:52,316 - INFO - step: 2015,loss: 0.055 accuracy: 0.984\n",
      "2017-08-13 22:47:52,339 - INFO - step: 2016,loss: 0.065 accuracy: 1.000\n",
      "2017-08-13 22:47:52,341 - INFO - \train_epoch:\n",
      "2017-08-13 22:47:52,342 - INFO - loss_total: 0.112 accuracy_total: 0.959\n",
      "2017-08-13 22:47:52,422 - INFO - step: 2017,loss: 0.133 accuracy: 0.906\n",
      "2017-08-13 22:47:52,485 - INFO - step: 2018,loss: 0.161 accuracy: 0.891\n",
      "2017-08-13 22:47:52,548 - INFO - step: 2019,loss: 0.104 accuracy: 0.938\n",
      "2017-08-13 22:47:52,610 - INFO - step: 2020,loss: 0.118 accuracy: 0.938\n",
      "2017-08-13 22:47:52,677 - INFO - step: 2021,loss: 0.084 accuracy: 0.953\n",
      "2017-08-13 22:47:52,741 - INFO - step: 2022,loss: 0.083 accuracy: 0.953\n",
      "2017-08-13 22:47:52,805 - INFO - step: 2023,loss: 0.079 accuracy: 0.984\n",
      "2017-08-13 22:47:52,866 - INFO - step: 2024,loss: 0.175 accuracy: 0.953\n",
      "2017-08-13 22:47:52,930 - INFO - step: 2025,loss: 0.144 accuracy: 0.953\n",
      "2017-08-13 22:47:52,994 - INFO - step: 2026,loss: 0.100 accuracy: 0.984\n",
      "2017-08-13 22:47:53,057 - INFO - step: 2027,loss: 0.085 accuracy: 0.969\n",
      "2017-08-13 22:47:53,119 - INFO - step: 2028,loss: 0.073 accuracy: 0.969\n",
      "2017-08-13 22:47:53,182 - INFO - step: 2029,loss: 0.140 accuracy: 0.938\n",
      "2017-08-13 22:47:53,245 - INFO - step: 2030,loss: 0.048 accuracy: 0.984\n",
      "2017-08-13 22:47:53,307 - INFO - step: 2031,loss: 0.110 accuracy: 0.969\n",
      "2017-08-13 22:47:53,370 - INFO - step: 2032,loss: 0.083 accuracy: 0.953\n",
      "2017-08-13 22:47:53,433 - INFO - step: 2033,loss: 0.104 accuracy: 0.953\n",
      "2017-08-13 22:47:53,498 - INFO - step: 2034,loss: 0.021 accuracy: 1.000\n",
      "2017-08-13 22:47:53,567 - INFO - step: 2035,loss: 0.179 accuracy: 0.938\n",
      "2017-08-13 22:47:53,632 - INFO - step: 2036,loss: 0.094 accuracy: 0.969\n",
      "2017-08-13 22:47:53,696 - INFO - step: 2037,loss: 0.094 accuracy: 0.953\n",
      "2017-08-13 22:47:53,759 - INFO - step: 2038,loss: 0.096 accuracy: 0.969\n",
      "2017-08-13 22:47:53,823 - INFO - step: 2039,loss: 0.127 accuracy: 0.922\n",
      "2017-08-13 22:47:53,888 - INFO - step: 2040,loss: 0.097 accuracy: 0.953\n",
      "2017-08-13 22:47:53,951 - INFO - step: 2041,loss: 0.079 accuracy: 0.984\n",
      "2017-08-13 22:47:54,015 - INFO - step: 2042,loss: 0.147 accuracy: 0.938\n",
      "2017-08-13 22:47:54,078 - INFO - step: 2043,loss: 0.107 accuracy: 0.953\n",
      "2017-08-13 22:47:54,142 - INFO - step: 2044,loss: 0.114 accuracy: 0.969\n",
      "2017-08-13 22:47:54,208 - INFO - step: 2045,loss: 0.146 accuracy: 0.906\n",
      "2017-08-13 22:47:54,270 - INFO - step: 2046,loss: 0.086 accuracy: 0.969\n",
      "2017-08-13 22:47:54,332 - INFO - step: 2047,loss: 0.076 accuracy: 0.969\n",
      "2017-08-13 22:47:54,394 - INFO - step: 2048,loss: 0.306 accuracy: 0.922\n",
      "2017-08-13 22:47:54,455 - INFO - step: 2049,loss: 0.089 accuracy: 0.984\n",
      "2017-08-13 22:47:54,518 - INFO - step: 2050,loss: 0.036 accuracy: 1.000\n",
      "2017-08-13 22:47:54,581 - INFO - step: 2051,loss: 0.156 accuracy: 0.938\n",
      "2017-08-13 22:47:54,603 - INFO - step: 2052,loss: 0.194 accuracy: 0.857\n",
      "2017-08-13 22:47:54,606 - INFO - \n",
      "Evaluation:\n",
      "2017-08-13 22:47:54,668 - INFO - step: 2052,loss: 0.209 accuracy: 0.924\n",
      "2017-08-13 22:47:54,772 - INFO - Saving model to /home/zx/cuckoo_1000/session_save/checkpoints/model-2052 at step 56.\n",
      "2017-08-13 22:47:54,775 - INFO - \train_epoch:\n",
      "2017-08-13 22:47:54,776 - INFO - loss_total: 0.113 accuracy_total: 0.952\n",
      "2017-08-13 22:47:54,852 - INFO - step: 2053,loss: 0.157 accuracy: 0.906\n",
      "2017-08-13 22:47:54,916 - INFO - step: 2054,loss: 0.112 accuracy: 0.938\n",
      "2017-08-13 22:47:54,982 - INFO - step: 2055,loss: 0.163 accuracy: 0.922\n",
      "2017-08-13 22:47:55,046 - INFO - step: 2056,loss: 0.083 accuracy: 0.984\n",
      "2017-08-13 22:47:55,111 - INFO - step: 2057,loss: 0.084 accuracy: 0.969\n",
      "2017-08-13 22:47:55,174 - INFO - step: 2058,loss: 0.209 accuracy: 0.938\n",
      "2017-08-13 22:47:55,236 - INFO - step: 2059,loss: 0.189 accuracy: 0.922\n",
      "2017-08-13 22:47:55,301 - INFO - step: 2060,loss: 0.141 accuracy: 0.969\n",
      "2017-08-13 22:47:55,364 - INFO - step: 2061,loss: 0.074 accuracy: 0.953\n",
      "2017-08-13 22:47:55,429 - INFO - step: 2062,loss: 0.024 accuracy: 1.000\n",
      "2017-08-13 22:47:55,493 - INFO - step: 2063,loss: 0.084 accuracy: 0.969\n",
      "2017-08-13 22:47:55,557 - INFO - step: 2064,loss: 0.133 accuracy: 0.953\n",
      "2017-08-13 22:47:55,621 - INFO - step: 2065,loss: 0.097 accuracy: 0.938\n",
      "2017-08-13 22:47:55,686 - INFO - step: 2066,loss: 0.094 accuracy: 0.969\n",
      "2017-08-13 22:47:55,749 - INFO - step: 2067,loss: 0.058 accuracy: 0.984\n",
      "2017-08-13 22:47:55,813 - INFO - step: 2068,loss: 0.087 accuracy: 0.969\n",
      "2017-08-13 22:47:55,876 - INFO - step: 2069,loss: 0.141 accuracy: 0.953\n",
      "2017-08-13 22:47:55,938 - INFO - step: 2070,loss: 0.088 accuracy: 0.953\n",
      "2017-08-13 22:47:55,999 - INFO - step: 2071,loss: 0.091 accuracy: 0.969\n",
      "2017-08-13 22:47:56,062 - INFO - step: 2072,loss: 0.116 accuracy: 0.938\n",
      "2017-08-13 22:47:56,127 - INFO - step: 2073,loss: 0.121 accuracy: 0.922\n",
      "2017-08-13 22:47:56,192 - INFO - step: 2074,loss: 0.196 accuracy: 0.953\n",
      "2017-08-13 22:47:56,254 - INFO - step: 2075,loss: 0.105 accuracy: 0.938\n",
      "2017-08-13 22:47:56,317 - INFO - step: 2076,loss: 0.101 accuracy: 0.953\n",
      "2017-08-13 22:47:56,381 - INFO - step: 2077,loss: 0.102 accuracy: 0.969\n",
      "2017-08-13 22:47:56,443 - INFO - step: 2078,loss: 0.159 accuracy: 0.906\n",
      "2017-08-13 22:47:56,507 - INFO - step: 2079,loss: 0.261 accuracy: 0.922\n",
      "2017-08-13 22:47:56,570 - INFO - step: 2080,loss: 0.075 accuracy: 0.984\n",
      "2017-08-13 22:47:56,634 - INFO - step: 2081,loss: 0.186 accuracy: 0.922\n",
      "2017-08-13 22:47:56,713 - INFO - step: 2082,loss: 0.128 accuracy: 0.953\n",
      "2017-08-13 22:47:56,777 - INFO - step: 2083,loss: 0.139 accuracy: 0.938\n",
      "2017-08-13 22:47:56,841 - INFO - step: 2084,loss: 0.137 accuracy: 0.953\n",
      "2017-08-13 22:47:56,904 - INFO - step: 2085,loss: 0.091 accuracy: 0.953\n",
      "2017-08-13 22:47:56,966 - INFO - step: 2086,loss: 0.054 accuracy: 1.000\n",
      "2017-08-13 22:47:57,030 - INFO - step: 2087,loss: 0.079 accuracy: 0.969\n",
      "2017-08-13 22:47:57,052 - INFO - step: 2088,loss: 0.380 accuracy: 0.714\n",
      "2017-08-13 22:47:57,054 - INFO - \train_epoch:\n",
      "2017-08-13 22:47:57,056 - INFO - loss_total: 0.126 accuracy_total: 0.946\n",
      "2017-08-13 22:47:57,139 - INFO - step: 2089,loss: 0.056 accuracy: 0.984\n",
      "2017-08-13 22:47:57,203 - INFO - step: 2090,loss: 0.143 accuracy: 0.938\n",
      "2017-08-13 22:47:57,267 - INFO - step: 2091,loss: 0.047 accuracy: 1.000\n",
      "2017-08-13 22:47:57,330 - INFO - step: 2092,loss: 0.278 accuracy: 0.875\n",
      "2017-08-13 22:47:57,394 - INFO - step: 2093,loss: 0.089 accuracy: 0.984\n",
      "2017-08-13 22:47:57,458 - INFO - step: 2094,loss: 0.155 accuracy: 0.969\n",
      "2017-08-13 22:47:57,520 - INFO - step: 2095,loss: 0.095 accuracy: 0.984\n",
      "2017-08-13 22:47:57,581 - INFO - step: 2096,loss: 0.114 accuracy: 0.938\n",
      "2017-08-13 22:47:57,644 - INFO - step: 2097,loss: 0.101 accuracy: 0.953\n",
      "2017-08-13 22:47:57,707 - INFO - step: 2098,loss: 0.120 accuracy: 0.953\n",
      "2017-08-13 22:47:57,769 - INFO - step: 2099,loss: 0.105 accuracy: 0.938\n",
      "2017-08-13 22:47:57,833 - INFO - step: 2100,loss: 0.082 accuracy: 0.984\n",
      "2017-08-13 22:47:57,898 - INFO - step: 2101,loss: 0.120 accuracy: 0.953\n",
      "2017-08-13 22:47:57,960 - INFO - step: 2102,loss: 0.121 accuracy: 0.969\n",
      "2017-08-13 22:47:58,026 - INFO - step: 2103,loss: 0.110 accuracy: 0.984\n",
      "2017-08-13 22:47:58,088 - INFO - step: 2104,loss: 0.069 accuracy: 0.969\n",
      "2017-08-13 22:47:58,153 - INFO - step: 2105,loss: 0.061 accuracy: 0.984\n",
      "2017-08-13 22:47:58,215 - INFO - step: 2106,loss: 0.103 accuracy: 0.953\n",
      "2017-08-13 22:47:58,278 - INFO - step: 2107,loss: 0.028 accuracy: 1.000\n",
      "2017-08-13 22:47:58,340 - INFO - step: 2108,loss: 0.159 accuracy: 0.984\n",
      "2017-08-13 22:47:58,405 - INFO - step: 2109,loss: 0.142 accuracy: 0.922\n",
      "2017-08-13 22:47:58,466 - INFO - step: 2110,loss: 0.099 accuracy: 0.969\n",
      "2017-08-13 22:47:58,527 - INFO - step: 2111,loss: 0.121 accuracy: 0.969\n",
      "2017-08-13 22:47:58,590 - INFO - step: 2112,loss: 0.138 accuracy: 0.938\n",
      "2017-08-13 22:47:58,654 - INFO - step: 2113,loss: 0.103 accuracy: 0.938\n",
      "2017-08-13 22:47:58,717 - INFO - step: 2114,loss: 0.195 accuracy: 0.938\n",
      "2017-08-13 22:47:58,777 - INFO - step: 2115,loss: 0.173 accuracy: 0.922\n",
      "2017-08-13 22:47:58,841 - INFO - step: 2116,loss: 0.075 accuracy: 0.984\n",
      "2017-08-13 22:47:58,903 - INFO - step: 2117,loss: 0.102 accuracy: 0.969\n",
      "2017-08-13 22:47:58,966 - INFO - step: 2118,loss: 0.144 accuracy: 0.938\n",
      "2017-08-13 22:47:59,029 - INFO - step: 2119,loss: 0.131 accuracy: 0.938\n",
      "2017-08-13 22:47:59,092 - INFO - step: 2120,loss: 0.082 accuracy: 0.969\n",
      "2017-08-13 22:47:59,155 - INFO - step: 2121,loss: 0.057 accuracy: 0.984\n",
      "2017-08-13 22:47:59,219 - INFO - step: 2122,loss: 0.093 accuracy: 0.953\n",
      "2017-08-13 22:47:59,282 - INFO - step: 2123,loss: 0.068 accuracy: 0.984\n",
      "2017-08-13 22:47:59,305 - INFO - step: 2124,loss: 0.022 accuracy: 1.000\n",
      "2017-08-13 22:47:59,307 - INFO - \n",
      "Evaluation:\n",
      "2017-08-13 22:47:59,368 - INFO - step: 2124,loss: 0.222 accuracy: 0.920\n",
      "2017-08-13 22:47:59,369 - INFO - \train_epoch:\n",
      "2017-08-13 22:47:59,371 - INFO - loss_total: 0.108 accuracy_total: 0.961\n",
      "2017-08-13 22:47:59,444 - INFO - step: 2125,loss: 0.081 accuracy: 0.984\n",
      "2017-08-13 22:47:59,507 - INFO - step: 2126,loss: 0.093 accuracy: 0.984\n",
      "2017-08-13 22:47:59,571 - INFO - step: 2127,loss: 0.083 accuracy: 0.984\n",
      "2017-08-13 22:47:59,637 - INFO - step: 2128,loss: 0.088 accuracy: 0.969\n",
      "2017-08-13 22:47:59,700 - INFO - step: 2129,loss: 0.072 accuracy: 0.969\n",
      "2017-08-13 22:47:59,761 - INFO - step: 2130,loss: 0.111 accuracy: 0.969\n",
      "2017-08-13 22:47:59,825 - INFO - step: 2131,loss: 0.101 accuracy: 0.953\n",
      "2017-08-13 22:47:59,889 - INFO - step: 2132,loss: 0.121 accuracy: 0.938\n",
      "2017-08-13 22:47:59,952 - INFO - step: 2133,loss: 0.183 accuracy: 0.969\n",
      "2017-08-13 22:48:00,018 - INFO - step: 2134,loss: 0.250 accuracy: 0.922\n",
      "2017-08-13 22:48:00,081 - INFO - step: 2135,loss: 0.092 accuracy: 0.953\n",
      "2017-08-13 22:48:00,145 - INFO - step: 2136,loss: 0.146 accuracy: 0.953\n",
      "2017-08-13 22:48:00,208 - INFO - step: 2137,loss: 0.165 accuracy: 0.906\n",
      "2017-08-13 22:48:00,272 - INFO - step: 2138,loss: 0.042 accuracy: 1.000\n",
      "2017-08-13 22:48:00,336 - INFO - step: 2139,loss: 0.054 accuracy: 0.984\n",
      "2017-08-13 22:48:00,398 - INFO - step: 2140,loss: 0.241 accuracy: 0.891\n",
      "2017-08-13 22:48:00,460 - INFO - step: 2141,loss: 0.046 accuracy: 1.000\n",
      "2017-08-13 22:48:00,522 - INFO - step: 2142,loss: 0.133 accuracy: 0.938\n",
      "2017-08-13 22:48:00,585 - INFO - step: 2143,loss: 0.159 accuracy: 0.938\n",
      "2017-08-13 22:48:00,648 - INFO - step: 2144,loss: 0.061 accuracy: 0.969\n",
      "2017-08-13 22:48:00,709 - INFO - step: 2145,loss: 0.071 accuracy: 0.969\n",
      "2017-08-13 22:48:00,771 - INFO - step: 2146,loss: 0.082 accuracy: 0.969\n",
      "2017-08-13 22:48:00,834 - INFO - step: 2147,loss: 0.130 accuracy: 0.953\n",
      "2017-08-13 22:48:00,897 - INFO - step: 2148,loss: 0.084 accuracy: 0.953\n",
      "2017-08-13 22:48:00,960 - INFO - step: 2149,loss: 0.084 accuracy: 0.984\n",
      "2017-08-13 22:48:01,022 - INFO - step: 2150,loss: 0.118 accuracy: 0.969\n",
      "2017-08-13 22:48:01,087 - INFO - step: 2151,loss: 0.097 accuracy: 0.969\n",
      "2017-08-13 22:48:01,150 - INFO - step: 2152,loss: 0.053 accuracy: 1.000\n",
      "2017-08-13 22:48:01,213 - INFO - step: 2153,loss: 0.052 accuracy: 1.000\n",
      "2017-08-13 22:48:01,277 - INFO - step: 2154,loss: 0.057 accuracy: 0.984\n",
      "2017-08-13 22:48:01,339 - INFO - step: 2155,loss: 0.079 accuracy: 0.984\n",
      "2017-08-13 22:48:01,398 - INFO - step: 2156,loss: 0.151 accuracy: 0.938\n",
      "2017-08-13 22:48:01,460 - INFO - step: 2157,loss: 0.115 accuracy: 0.953\n",
      "2017-08-13 22:48:01,527 - INFO - step: 2158,loss: 0.156 accuracy: 0.938\n",
      "2017-08-13 22:48:01,590 - INFO - step: 2159,loss: 0.151 accuracy: 0.938\n",
      "2017-08-13 22:48:01,614 - INFO - step: 2160,loss: 0.048 accuracy: 1.000\n",
      "2017-08-13 22:48:01,616 - INFO - \train_epoch:\n",
      "2017-08-13 22:48:01,618 - INFO - loss_total: 0.107 accuracy_total: 0.963\n",
      "2017-08-13 22:48:01,687 - INFO - step: 2161,loss: 0.058 accuracy: 0.984\n",
      "2017-08-13 22:48:01,760 - INFO - step: 2162,loss: 0.125 accuracy: 0.938\n",
      "2017-08-13 22:48:01,824 - INFO - step: 2163,loss: 0.108 accuracy: 0.969\n",
      "2017-08-13 22:48:01,887 - INFO - step: 2164,loss: 0.068 accuracy: 0.969\n",
      "2017-08-13 22:48:01,949 - INFO - step: 2165,loss: 0.077 accuracy: 0.969\n",
      "2017-08-13 22:48:02,011 - INFO - step: 2166,loss: 0.088 accuracy: 0.953\n",
      "2017-08-13 22:48:02,075 - INFO - step: 2167,loss: 0.188 accuracy: 0.953\n",
      "2017-08-13 22:48:02,138 - INFO - step: 2168,loss: 0.067 accuracy: 0.984\n",
      "2017-08-13 22:48:02,201 - INFO - step: 2169,loss: 0.067 accuracy: 0.969\n",
      "2017-08-13 22:48:02,262 - INFO - step: 2170,loss: 0.107 accuracy: 0.922\n",
      "2017-08-13 22:48:02,323 - INFO - step: 2171,loss: 0.175 accuracy: 0.938\n",
      "2017-08-13 22:48:02,390 - INFO - step: 2172,loss: 0.111 accuracy: 0.953\n",
      "2017-08-13 22:48:02,453 - INFO - step: 2173,loss: 0.052 accuracy: 1.000\n",
      "2017-08-13 22:48:02,515 - INFO - step: 2174,loss: 0.118 accuracy: 0.953\n",
      "2017-08-13 22:48:02,578 - INFO - step: 2175,loss: 0.171 accuracy: 0.953\n",
      "2017-08-13 22:48:02,642 - INFO - step: 2176,loss: 0.197 accuracy: 0.891\n",
      "2017-08-13 22:48:02,706 - INFO - step: 2177,loss: 0.151 accuracy: 0.938\n",
      "2017-08-13 22:48:02,770 - INFO - step: 2178,loss: 0.077 accuracy: 0.953\n",
      "2017-08-13 22:48:02,831 - INFO - step: 2179,loss: 0.079 accuracy: 0.969\n",
      "2017-08-13 22:48:02,893 - INFO - step: 2180,loss: 0.063 accuracy: 0.984\n",
      "2017-08-13 22:48:02,959 - INFO - step: 2181,loss: 0.188 accuracy: 0.953\n",
      "2017-08-13 22:48:03,041 - INFO - step: 2182,loss: 0.145 accuracy: 0.922\n",
      "2017-08-13 22:48:03,103 - INFO - step: 2183,loss: 0.026 accuracy: 1.000\n",
      "2017-08-13 22:48:03,163 - INFO - step: 2184,loss: 0.106 accuracy: 0.922\n",
      "2017-08-13 22:48:03,227 - INFO - step: 2185,loss: 0.159 accuracy: 0.969\n",
      "2017-08-13 22:48:03,291 - INFO - step: 2186,loss: 0.050 accuracy: 0.984\n",
      "2017-08-13 22:48:03,354 - INFO - step: 2187,loss: 0.130 accuracy: 0.953\n",
      "2017-08-13 22:48:03,419 - INFO - step: 2188,loss: 0.205 accuracy: 0.891\n",
      "2017-08-13 22:48:03,482 - INFO - step: 2189,loss: 0.074 accuracy: 0.969\n",
      "2017-08-13 22:48:03,544 - INFO - step: 2190,loss: 0.077 accuracy: 0.969\n",
      "2017-08-13 22:48:03,613 - INFO - step: 2191,loss: 0.066 accuracy: 0.969\n",
      "2017-08-13 22:48:03,679 - INFO - step: 2192,loss: 0.071 accuracy: 0.984\n",
      "2017-08-13 22:48:03,742 - INFO - step: 2193,loss: 0.070 accuracy: 0.969\n",
      "2017-08-13 22:48:03,804 - INFO - step: 2194,loss: 0.234 accuracy: 0.922\n",
      "2017-08-13 22:48:03,867 - INFO - step: 2195,loss: 0.116 accuracy: 0.953\n",
      "2017-08-13 22:48:03,890 - INFO - step: 2196,loss: 0.013 accuracy: 1.000\n",
      "2017-08-13 22:48:03,892 - INFO - \n",
      "Evaluation:\n",
      "2017-08-13 22:48:03,969 - INFO - step: 2196,loss: 0.235 accuracy: 0.908\n",
      "2017-08-13 22:48:03,971 - INFO - \train_epoch:\n",
      "2017-08-13 22:48:03,972 - INFO - loss_total: 0.108 accuracy_total: 0.957\n",
      "2017-08-13 22:48:04,042 - INFO - step: 2197,loss: 0.101 accuracy: 0.953\n",
      "2017-08-13 22:48:04,112 - INFO - step: 2198,loss: 0.150 accuracy: 0.938\n",
      "2017-08-13 22:48:04,174 - INFO - step: 2199,loss: 0.064 accuracy: 0.984\n",
      "2017-08-13 22:48:04,237 - INFO - step: 2200,loss: 0.114 accuracy: 0.953\n",
      "2017-08-13 22:48:04,307 - INFO - step: 2201,loss: 0.162 accuracy: 0.922\n",
      "2017-08-13 22:48:04,370 - INFO - step: 2202,loss: 0.116 accuracy: 0.953\n",
      "2017-08-13 22:48:04,433 - INFO - step: 2203,loss: 0.051 accuracy: 1.000\n",
      "2017-08-13 22:48:04,494 - INFO - step: 2204,loss: 0.099 accuracy: 0.953\n",
      "2017-08-13 22:48:04,557 - INFO - step: 2205,loss: 0.078 accuracy: 0.969\n",
      "2017-08-13 22:48:04,617 - INFO - step: 2206,loss: 0.112 accuracy: 0.953\n",
      "2017-08-13 22:48:04,678 - INFO - step: 2207,loss: 0.127 accuracy: 0.969\n",
      "2017-08-13 22:48:04,742 - INFO - step: 2208,loss: 0.205 accuracy: 0.938\n",
      "2017-08-13 22:48:04,804 - INFO - step: 2209,loss: 0.130 accuracy: 0.953\n",
      "2017-08-13 22:48:04,868 - INFO - step: 2210,loss: 0.117 accuracy: 0.969\n",
      "2017-08-13 22:48:04,931 - INFO - step: 2211,loss: 0.108 accuracy: 0.953\n",
      "2017-08-13 22:48:04,995 - INFO - step: 2212,loss: 0.270 accuracy: 0.875\n",
      "2017-08-13 22:48:05,060 - INFO - step: 2213,loss: 0.099 accuracy: 0.984\n",
      "2017-08-13 22:48:05,122 - INFO - step: 2214,loss: 0.092 accuracy: 0.953\n",
      "2017-08-13 22:48:05,186 - INFO - step: 2215,loss: 0.082 accuracy: 0.969\n",
      "2017-08-13 22:48:05,249 - INFO - step: 2216,loss: 0.032 accuracy: 1.000\n",
      "2017-08-13 22:48:05,311 - INFO - step: 2217,loss: 0.079 accuracy: 0.953\n",
      "2017-08-13 22:48:05,374 - INFO - step: 2218,loss: 0.064 accuracy: 0.969\n",
      "2017-08-13 22:48:05,437 - INFO - step: 2219,loss: 0.117 accuracy: 0.969\n",
      "2017-08-13 22:48:05,501 - INFO - step: 2220,loss: 0.039 accuracy: 1.000\n",
      "2017-08-13 22:48:05,565 - INFO - step: 2221,loss: 0.083 accuracy: 0.969\n",
      "2017-08-13 22:48:05,628 - INFO - step: 2222,loss: 0.132 accuracy: 0.969\n",
      "2017-08-13 22:48:05,690 - INFO - step: 2223,loss: 0.076 accuracy: 0.969\n",
      "2017-08-13 22:48:05,753 - INFO - step: 2224,loss: 0.201 accuracy: 0.938\n",
      "2017-08-13 22:48:05,816 - INFO - step: 2225,loss: 0.112 accuracy: 0.953\n",
      "2017-08-13 22:48:05,881 - INFO - step: 2226,loss: 0.098 accuracy: 0.969\n",
      "2017-08-13 22:48:05,944 - INFO - step: 2227,loss: 0.070 accuracy: 1.000\n",
      "2017-08-13 22:48:06,007 - INFO - step: 2228,loss: 0.145 accuracy: 0.953\n",
      "2017-08-13 22:48:06,072 - INFO - step: 2229,loss: 0.088 accuracy: 0.984\n",
      "2017-08-13 22:48:06,137 - INFO - step: 2230,loss: 0.157 accuracy: 0.938\n",
      "2017-08-13 22:48:06,199 - INFO - step: 2231,loss: 0.068 accuracy: 0.984\n",
      "2017-08-13 22:48:06,221 - INFO - step: 2232,loss: 0.038 accuracy: 1.000\n",
      "2017-08-13 22:48:06,224 - INFO - \train_epoch:\n",
      "2017-08-13 22:48:06,226 - INFO - loss_total: 0.108 accuracy_total: 0.963\n",
      "2017-08-13 22:48:06,311 - INFO - step: 2233,loss: 0.151 accuracy: 0.922\n",
      "2017-08-13 22:48:06,375 - INFO - step: 2234,loss: 0.236 accuracy: 0.922\n",
      "2017-08-13 22:48:06,440 - INFO - step: 2235,loss: 0.156 accuracy: 0.953\n",
      "2017-08-13 22:48:06,502 - INFO - step: 2236,loss: 0.039 accuracy: 1.000\n",
      "2017-08-13 22:48:06,564 - INFO - step: 2237,loss: 0.069 accuracy: 0.984\n",
      "2017-08-13 22:48:06,626 - INFO - step: 2238,loss: 0.106 accuracy: 0.953\n",
      "2017-08-13 22:48:06,687 - INFO - step: 2239,loss: 0.049 accuracy: 0.984\n",
      "2017-08-13 22:48:06,749 - INFO - step: 2240,loss: 0.079 accuracy: 0.984\n",
      "2017-08-13 22:48:06,814 - INFO - step: 2241,loss: 0.085 accuracy: 0.969\n",
      "2017-08-13 22:48:06,877 - INFO - step: 2242,loss: 0.024 accuracy: 1.000\n",
      "2017-08-13 22:48:06,941 - INFO - step: 2243,loss: 0.255 accuracy: 0.938\n",
      "2017-08-13 22:48:07,004 - INFO - step: 2244,loss: 0.072 accuracy: 0.969\n",
      "2017-08-13 22:48:07,066 - INFO - step: 2245,loss: 0.164 accuracy: 0.953\n",
      "2017-08-13 22:48:07,129 - INFO - step: 2246,loss: 0.105 accuracy: 0.953\n",
      "2017-08-13 22:48:07,193 - INFO - step: 2247,loss: 0.091 accuracy: 0.969\n",
      "2017-08-13 22:48:07,254 - INFO - step: 2248,loss: 0.050 accuracy: 0.984\n",
      "2017-08-13 22:48:07,317 - INFO - step: 2249,loss: 0.098 accuracy: 0.938\n",
      "2017-08-13 22:48:07,379 - INFO - step: 2250,loss: 0.049 accuracy: 0.984\n",
      "2017-08-13 22:48:07,442 - INFO - step: 2251,loss: 0.127 accuracy: 0.969\n",
      "2017-08-13 22:48:07,505 - INFO - step: 2252,loss: 0.144 accuracy: 0.969\n",
      "2017-08-13 22:48:07,569 - INFO - step: 2253,loss: 0.202 accuracy: 0.922\n",
      "2017-08-13 22:48:07,632 - INFO - step: 2254,loss: 0.102 accuracy: 0.953\n",
      "2017-08-13 22:48:07,695 - INFO - step: 2255,loss: 0.066 accuracy: 0.969\n",
      "2017-08-13 22:48:07,759 - INFO - step: 2256,loss: 0.110 accuracy: 0.953\n",
      "2017-08-13 22:48:07,823 - INFO - step: 2257,loss: 0.049 accuracy: 1.000\n",
      "2017-08-13 22:48:07,886 - INFO - step: 2258,loss: 0.113 accuracy: 0.984\n",
      "2017-08-13 22:48:07,948 - INFO - step: 2259,loss: 0.122 accuracy: 0.969\n",
      "2017-08-13 22:48:08,013 - INFO - step: 2260,loss: 0.066 accuracy: 0.969\n",
      "2017-08-13 22:48:08,075 - INFO - step: 2261,loss: 0.135 accuracy: 0.938\n",
      "2017-08-13 22:48:08,138 - INFO - step: 2262,loss: 0.086 accuracy: 0.969\n",
      "2017-08-13 22:48:08,202 - INFO - step: 2263,loss: 0.122 accuracy: 0.938\n",
      "2017-08-13 22:48:08,264 - INFO - step: 2264,loss: 0.110 accuracy: 0.922\n",
      "2017-08-13 22:48:08,329 - INFO - step: 2265,loss: 0.047 accuracy: 0.984\n",
      "2017-08-13 22:48:08,392 - INFO - step: 2266,loss: 0.115 accuracy: 0.953\n",
      "2017-08-13 22:48:08,456 - INFO - step: 2267,loss: 0.094 accuracy: 0.953\n",
      "2017-08-13 22:48:08,478 - INFO - step: 2268,loss: 0.068 accuracy: 1.000\n",
      "2017-08-13 22:48:08,480 - INFO - \n",
      "Evaluation:\n",
      "2017-08-13 22:48:08,540 - INFO - step: 2268,loss: 0.224 accuracy: 0.916\n",
      "2017-08-13 22:48:08,541 - INFO - \train_epoch:\n",
      "2017-08-13 22:48:08,543 - INFO - loss_total: 0.104 accuracy_total: 0.963\n",
      "2017-08-13 22:48:08,626 - INFO - step: 2269,loss: 0.088 accuracy: 0.969\n",
      "2017-08-13 22:48:08,692 - INFO - step: 2270,loss: 0.146 accuracy: 0.969\n",
      "2017-08-13 22:48:08,756 - INFO - step: 2271,loss: 0.080 accuracy: 0.953\n",
      "2017-08-13 22:48:08,820 - INFO - step: 2272,loss: 0.064 accuracy: 0.969\n",
      "2017-08-13 22:48:08,883 - INFO - step: 2273,loss: 0.107 accuracy: 0.953\n",
      "2017-08-13 22:48:08,947 - INFO - step: 2274,loss: 0.179 accuracy: 0.938\n",
      "2017-08-13 22:48:09,009 - INFO - step: 2275,loss: 0.110 accuracy: 0.969\n",
      "2017-08-13 22:48:09,072 - INFO - step: 2276,loss: 0.106 accuracy: 0.953\n",
      "2017-08-13 22:48:09,139 - INFO - step: 2277,loss: 0.062 accuracy: 0.969\n",
      "2017-08-13 22:48:09,205 - INFO - step: 2278,loss: 0.118 accuracy: 0.953\n",
      "2017-08-13 22:48:09,270 - INFO - step: 2279,loss: 0.067 accuracy: 0.969\n",
      "2017-08-13 22:48:09,333 - INFO - step: 2280,loss: 0.210 accuracy: 0.953\n",
      "2017-08-13 22:48:09,395 - INFO - step: 2281,loss: 0.093 accuracy: 0.984\n",
      "2017-08-13 22:48:09,460 - INFO - step: 2282,loss: 0.100 accuracy: 0.953\n",
      "2017-08-13 22:48:09,524 - INFO - step: 2283,loss: 0.111 accuracy: 0.969\n",
      "2017-08-13 22:48:09,586 - INFO - step: 2284,loss: 0.074 accuracy: 0.953\n",
      "2017-08-13 22:48:09,649 - INFO - step: 2285,loss: 0.067 accuracy: 0.969\n",
      "2017-08-13 22:48:09,712 - INFO - step: 2286,loss: 0.127 accuracy: 0.953\n",
      "2017-08-13 22:48:09,776 - INFO - step: 2287,loss: 0.113 accuracy: 0.938\n",
      "2017-08-13 22:48:09,838 - INFO - step: 2288,loss: 0.096 accuracy: 0.953\n",
      "2017-08-13 22:48:09,901 - INFO - step: 2289,loss: 0.105 accuracy: 0.953\n",
      "2017-08-13 22:48:09,964 - INFO - step: 2290,loss: 0.072 accuracy: 1.000\n",
      "2017-08-13 22:48:10,031 - INFO - step: 2291,loss: 0.097 accuracy: 0.953\n",
      "2017-08-13 22:48:10,093 - INFO - step: 2292,loss: 0.149 accuracy: 0.969\n",
      "2017-08-13 22:48:10,155 - INFO - step: 2293,loss: 0.180 accuracy: 0.953\n",
      "2017-08-13 22:48:10,220 - INFO - step: 2294,loss: 0.111 accuracy: 0.938\n",
      "2017-08-13 22:48:10,284 - INFO - step: 2295,loss: 0.089 accuracy: 0.969\n",
      "2017-08-13 22:48:10,349 - INFO - step: 2296,loss: 0.032 accuracy: 1.000\n",
      "2017-08-13 22:48:10,413 - INFO - step: 2297,loss: 0.138 accuracy: 0.922\n",
      "2017-08-13 22:48:10,475 - INFO - step: 2298,loss: 0.139 accuracy: 0.938\n",
      "2017-08-13 22:48:10,537 - INFO - step: 2299,loss: 0.107 accuracy: 0.969\n",
      "2017-08-13 22:48:10,600 - INFO - step: 2300,loss: 0.074 accuracy: 0.969\n",
      "2017-08-13 22:48:10,664 - INFO - step: 2301,loss: 0.153 accuracy: 0.953\n",
      "2017-08-13 22:48:10,728 - INFO - step: 2302,loss: 0.223 accuracy: 0.938\n",
      "2017-08-13 22:48:10,792 - INFO - step: 2303,loss: 0.075 accuracy: 0.984\n",
      "2017-08-13 22:48:10,813 - INFO - step: 2304,loss: 0.046 accuracy: 1.000\n",
      "2017-08-13 22:48:10,816 - INFO - \train_epoch:\n",
      "2017-08-13 22:48:10,817 - INFO - loss_total: 0.109 accuracy_total: 0.961\n",
      "2017-08-13 22:48:10,901 - INFO - step: 2305,loss: 0.195 accuracy: 0.922\n",
      "2017-08-13 22:48:10,964 - INFO - step: 2306,loss: 0.119 accuracy: 0.938\n",
      "2017-08-13 22:48:11,030 - INFO - step: 2307,loss: 0.070 accuracy: 0.938\n",
      "2017-08-13 22:48:11,096 - INFO - step: 2308,loss: 0.048 accuracy: 0.984\n",
      "2017-08-13 22:48:11,159 - INFO - step: 2309,loss: 0.089 accuracy: 0.984\n",
      "2017-08-13 22:48:11,223 - INFO - step: 2310,loss: 0.049 accuracy: 0.984\n",
      "2017-08-13 22:48:11,287 - INFO - step: 2311,loss: 0.099 accuracy: 0.953\n",
      "2017-08-13 22:48:11,351 - INFO - step: 2312,loss: 0.048 accuracy: 0.984\n",
      "2017-08-13 22:48:11,419 - INFO - step: 2313,loss: 0.067 accuracy: 0.984\n",
      "2017-08-13 22:48:11,484 - INFO - step: 2314,loss: 0.092 accuracy: 0.953\n",
      "2017-08-13 22:48:11,549 - INFO - step: 2315,loss: 0.055 accuracy: 1.000\n",
      "2017-08-13 22:48:11,613 - INFO - step: 2316,loss: 0.050 accuracy: 0.969\n",
      "2017-08-13 22:48:11,676 - INFO - step: 2317,loss: 0.139 accuracy: 0.938\n",
      "2017-08-13 22:48:11,740 - INFO - step: 2318,loss: 0.055 accuracy: 0.984\n",
      "2017-08-13 22:48:11,803 - INFO - step: 2319,loss: 0.106 accuracy: 0.969\n",
      "2017-08-13 22:48:11,866 - INFO - step: 2320,loss: 0.175 accuracy: 0.938\n",
      "2017-08-13 22:48:11,930 - INFO - step: 2321,loss: 0.128 accuracy: 0.953\n",
      "2017-08-13 22:48:11,994 - INFO - step: 2322,loss: 0.089 accuracy: 0.984\n",
      "2017-08-13 22:48:12,057 - INFO - step: 2323,loss: 0.109 accuracy: 0.969\n",
      "2017-08-13 22:48:12,120 - INFO - step: 2324,loss: 0.049 accuracy: 1.000\n",
      "2017-08-13 22:48:12,181 - INFO - step: 2325,loss: 0.098 accuracy: 0.969\n",
      "2017-08-13 22:48:12,243 - INFO - step: 2326,loss: 0.167 accuracy: 0.938\n",
      "2017-08-13 22:48:12,305 - INFO - step: 2327,loss: 0.061 accuracy: 0.969\n",
      "2017-08-13 22:48:12,370 - INFO - step: 2328,loss: 0.104 accuracy: 0.969\n",
      "2017-08-13 22:48:12,432 - INFO - step: 2329,loss: 0.132 accuracy: 0.922\n",
      "2017-08-13 22:48:12,496 - INFO - step: 2330,loss: 0.085 accuracy: 0.953\n",
      "2017-08-13 22:48:12,560 - INFO - step: 2331,loss: 0.112 accuracy: 0.953\n",
      "2017-08-13 22:48:12,626 - INFO - step: 2332,loss: 0.159 accuracy: 0.938\n",
      "2017-08-13 22:48:12,689 - INFO - step: 2333,loss: 0.050 accuracy: 0.984\n",
      "2017-08-13 22:48:12,756 - INFO - step: 2334,loss: 0.081 accuracy: 0.969\n",
      "2017-08-13 22:48:12,819 - INFO - step: 2335,loss: 0.162 accuracy: 0.938\n",
      "2017-08-13 22:48:12,882 - INFO - step: 2336,loss: 0.168 accuracy: 0.953\n",
      "2017-08-13 22:48:12,946 - INFO - step: 2337,loss: 0.078 accuracy: 0.969\n",
      "2017-08-13 22:48:13,010 - INFO - step: 2338,loss: 0.096 accuracy: 0.953\n",
      "2017-08-13 22:48:13,072 - INFO - step: 2339,loss: 0.164 accuracy: 0.922\n",
      "2017-08-13 22:48:13,095 - INFO - step: 2340,loss: 0.196 accuracy: 0.857\n",
      "2017-08-13 22:48:13,097 - INFO - \n",
      "Evaluation:\n",
      "2017-08-13 22:48:13,180 - INFO - step: 2340,loss: 0.231 accuracy: 0.912\n",
      "2017-08-13 22:48:13,182 - INFO - \train_epoch:\n",
      "2017-08-13 22:48:13,183 - INFO - loss_total: 0.104 accuracy_total: 0.958\n",
      "2017-08-13 22:48:13,266 - INFO - step: 2341,loss: 0.202 accuracy: 0.984\n",
      "2017-08-13 22:48:13,330 - INFO - step: 2342,loss: 0.135 accuracy: 0.938\n",
      "2017-08-13 22:48:13,393 - INFO - step: 2343,loss: 0.070 accuracy: 0.969\n",
      "2017-08-13 22:48:13,455 - INFO - step: 2344,loss: 0.112 accuracy: 0.953\n",
      "2017-08-13 22:48:13,519 - INFO - step: 2345,loss: 0.119 accuracy: 0.922\n",
      "2017-08-13 22:48:13,583 - INFO - step: 2346,loss: 0.136 accuracy: 0.938\n",
      "2017-08-13 22:48:13,649 - INFO - step: 2347,loss: 0.099 accuracy: 0.953\n",
      "2017-08-13 22:48:13,713 - INFO - step: 2348,loss: 0.064 accuracy: 0.969\n",
      "2017-08-13 22:48:13,776 - INFO - step: 2349,loss: 0.146 accuracy: 0.938\n",
      "2017-08-13 22:48:13,839 - INFO - step: 2350,loss: 0.074 accuracy: 0.984\n",
      "2017-08-13 22:48:13,902 - INFO - step: 2351,loss: 0.042 accuracy: 1.000\n",
      "2017-08-13 22:48:13,966 - INFO - step: 2352,loss: 0.082 accuracy: 0.953\n",
      "2017-08-13 22:48:14,029 - INFO - step: 2353,loss: 0.055 accuracy: 0.969\n",
      "2017-08-13 22:48:14,093 - INFO - step: 2354,loss: 0.103 accuracy: 0.969\n",
      "2017-08-13 22:48:14,157 - INFO - step: 2355,loss: 0.065 accuracy: 0.984\n",
      "2017-08-13 22:48:14,221 - INFO - step: 2356,loss: 0.077 accuracy: 0.984\n",
      "2017-08-13 22:48:14,286 - INFO - step: 2357,loss: 0.273 accuracy: 0.891\n",
      "2017-08-13 22:48:14,350 - INFO - step: 2358,loss: 0.129 accuracy: 0.953\n",
      "2017-08-13 22:48:14,413 - INFO - step: 2359,loss: 0.076 accuracy: 0.969\n",
      "2017-08-13 22:48:14,476 - INFO - step: 2360,loss: 0.097 accuracy: 0.969\n",
      "2017-08-13 22:48:14,538 - INFO - step: 2361,loss: 0.094 accuracy: 0.984\n",
      "2017-08-13 22:48:14,601 - INFO - step: 2362,loss: 0.033 accuracy: 1.000\n",
      "2017-08-13 22:48:14,664 - INFO - step: 2363,loss: 0.100 accuracy: 0.938\n",
      "2017-08-13 22:48:14,727 - INFO - step: 2364,loss: 0.107 accuracy: 0.969\n",
      "2017-08-13 22:48:14,789 - INFO - step: 2365,loss: 0.136 accuracy: 0.922\n",
      "2017-08-13 22:48:14,850 - INFO - step: 2366,loss: 0.133 accuracy: 0.953\n",
      "2017-08-13 22:48:14,913 - INFO - step: 2367,loss: 0.083 accuracy: 0.984\n",
      "2017-08-13 22:48:14,977 - INFO - step: 2368,loss: 0.069 accuracy: 0.969\n",
      "2017-08-13 22:48:15,040 - INFO - step: 2369,loss: 0.081 accuracy: 0.953\n",
      "2017-08-13 22:48:15,103 - INFO - step: 2370,loss: 0.047 accuracy: 0.969\n",
      "2017-08-13 22:48:15,167 - INFO - step: 2371,loss: 0.070 accuracy: 0.969\n",
      "2017-08-13 22:48:15,230 - INFO - step: 2372,loss: 0.074 accuracy: 0.969\n",
      "2017-08-13 22:48:15,294 - INFO - step: 2373,loss: 0.099 accuracy: 0.938\n",
      "2017-08-13 22:48:15,357 - INFO - step: 2374,loss: 0.096 accuracy: 0.938\n",
      "2017-08-13 22:48:15,421 - INFO - step: 2375,loss: 0.066 accuracy: 0.969\n",
      "2017-08-13 22:48:15,444 - INFO - step: 2376,loss: 0.391 accuracy: 0.857\n",
      "2017-08-13 22:48:15,447 - INFO - \train_epoch:\n",
      "2017-08-13 22:48:15,448 - INFO - loss_total: 0.107 accuracy_total: 0.957\n",
      "2017-08-13 22:48:15,522 - INFO - step: 2377,loss: 0.112 accuracy: 0.938\n",
      "2017-08-13 22:48:15,587 - INFO - step: 2378,loss: 0.083 accuracy: 0.984\n",
      "2017-08-13 22:48:15,650 - INFO - step: 2379,loss: 0.056 accuracy: 0.984\n",
      "2017-08-13 22:48:15,712 - INFO - step: 2380,loss: 0.107 accuracy: 0.922\n",
      "2017-08-13 22:48:15,775 - INFO - step: 2381,loss: 0.050 accuracy: 0.984\n",
      "2017-08-13 22:48:15,838 - INFO - step: 2382,loss: 0.128 accuracy: 0.969\n",
      "2017-08-13 22:48:15,901 - INFO - step: 2383,loss: 0.076 accuracy: 0.969\n",
      "2017-08-13 22:48:15,966 - INFO - step: 2384,loss: 0.081 accuracy: 0.969\n",
      "2017-08-13 22:48:16,028 - INFO - step: 2385,loss: 0.105 accuracy: 0.938\n",
      "2017-08-13 22:48:16,092 - INFO - step: 2386,loss: 0.075 accuracy: 0.953\n",
      "2017-08-13 22:48:16,155 - INFO - step: 2387,loss: 0.058 accuracy: 0.984\n",
      "2017-08-13 22:48:16,222 - INFO - step: 2388,loss: 0.080 accuracy: 0.969\n",
      "2017-08-13 22:48:16,286 - INFO - step: 2389,loss: 0.116 accuracy: 0.938\n",
      "2017-08-13 22:48:16,349 - INFO - step: 2390,loss: 0.096 accuracy: 0.953\n",
      "2017-08-13 22:48:16,413 - INFO - step: 2391,loss: 0.152 accuracy: 0.938\n",
      "2017-08-13 22:48:16,476 - INFO - step: 2392,loss: 0.065 accuracy: 0.953\n",
      "2017-08-13 22:48:16,539 - INFO - step: 2393,loss: 0.050 accuracy: 1.000\n",
      "2017-08-13 22:48:16,603 - INFO - step: 2394,loss: 0.191 accuracy: 0.922\n",
      "2017-08-13 22:48:16,667 - INFO - step: 2395,loss: 0.097 accuracy: 0.969\n",
      "2017-08-13 22:48:16,731 - INFO - step: 2396,loss: 0.115 accuracy: 0.953\n",
      "2017-08-13 22:48:16,794 - INFO - step: 2397,loss: 0.034 accuracy: 1.000\n",
      "2017-08-13 22:48:16,858 - INFO - step: 2398,loss: 0.093 accuracy: 0.953\n",
      "2017-08-13 22:48:16,923 - INFO - step: 2399,loss: 0.075 accuracy: 0.984\n",
      "2017-08-13 22:48:16,986 - INFO - step: 2400,loss: 0.116 accuracy: 0.938\n",
      "2017-08-13 22:48:17,052 - INFO - step: 2401,loss: 0.170 accuracy: 0.969\n",
      "2017-08-13 22:48:17,115 - INFO - step: 2402,loss: 0.076 accuracy: 0.969\n",
      "2017-08-13 22:48:17,179 - INFO - step: 2403,loss: 0.180 accuracy: 0.922\n",
      "2017-08-13 22:48:17,241 - INFO - step: 2404,loss: 0.129 accuracy: 0.953\n",
      "2017-08-13 22:48:17,304 - INFO - step: 2405,loss: 0.104 accuracy: 0.953\n",
      "2017-08-13 22:48:17,367 - INFO - step: 2406,loss: 0.162 accuracy: 0.969\n",
      "2017-08-13 22:48:17,429 - INFO - step: 2407,loss: 0.119 accuracy: 0.922\n",
      "2017-08-13 22:48:17,492 - INFO - step: 2408,loss: 0.126 accuracy: 0.938\n",
      "2017-08-13 22:48:17,555 - INFO - step: 2409,loss: 0.067 accuracy: 0.969\n",
      "2017-08-13 22:48:17,618 - INFO - step: 2410,loss: 0.140 accuracy: 0.922\n",
      "2017-08-13 22:48:17,682 - INFO - step: 2411,loss: 0.141 accuracy: 0.938\n",
      "2017-08-13 22:48:17,704 - INFO - step: 2412,loss: 0.218 accuracy: 0.857\n",
      "2017-08-13 22:48:17,707 - INFO - \n",
      "Evaluation:\n",
      "2017-08-13 22:48:17,769 - INFO - step: 2412,loss: 0.227 accuracy: 0.924\n",
      "2017-08-13 22:48:17,771 - INFO - \train_epoch:\n",
      "2017-08-13 22:48:17,772 - INFO - loss_total: 0.107 accuracy_total: 0.954\n",
      "2017-08-13 22:48:17,854 - INFO - step: 2413,loss: 0.100 accuracy: 0.953\n",
      "2017-08-13 22:48:17,918 - INFO - step: 2414,loss: 0.046 accuracy: 0.984\n",
      "2017-08-13 22:48:17,979 - INFO - step: 2415,loss: 0.049 accuracy: 0.969\n",
      "2017-08-13 22:48:18,044 - INFO - step: 2416,loss: 0.176 accuracy: 0.906\n",
      "2017-08-13 22:48:18,106 - INFO - step: 2417,loss: 0.102 accuracy: 0.938\n",
      "2017-08-13 22:48:18,171 - INFO - step: 2418,loss: 0.086 accuracy: 0.969\n",
      "2017-08-13 22:48:18,234 - INFO - step: 2419,loss: 0.144 accuracy: 0.938\n",
      "2017-08-13 22:48:18,296 - INFO - step: 2420,loss: 0.051 accuracy: 0.984\n",
      "2017-08-13 22:48:18,360 - INFO - step: 2421,loss: 0.198 accuracy: 0.922\n",
      "2017-08-13 22:48:18,424 - INFO - step: 2422,loss: 0.050 accuracy: 0.984\n",
      "2017-08-13 22:48:18,485 - INFO - step: 2423,loss: 0.033 accuracy: 1.000\n",
      "2017-08-13 22:48:18,548 - INFO - step: 2424,loss: 0.094 accuracy: 0.969\n",
      "2017-08-13 22:48:18,615 - INFO - step: 2425,loss: 0.068 accuracy: 0.984\n",
      "2017-08-13 22:48:18,678 - INFO - step: 2426,loss: 0.184 accuracy: 0.969\n",
      "2017-08-13 22:48:18,742 - INFO - step: 2427,loss: 0.088 accuracy: 0.984\n",
      "2017-08-13 22:48:18,804 - INFO - step: 2428,loss: 0.155 accuracy: 0.953\n",
      "2017-08-13 22:48:18,868 - INFO - step: 2429,loss: 0.145 accuracy: 0.938\n",
      "2017-08-13 22:48:18,931 - INFO - step: 2430,loss: 0.058 accuracy: 0.969\n",
      "2017-08-13 22:48:18,994 - INFO - step: 2431,loss: 0.117 accuracy: 0.938\n",
      "2017-08-13 22:48:19,056 - INFO - step: 2432,loss: 0.057 accuracy: 0.984\n",
      "2017-08-13 22:48:19,120 - INFO - step: 2433,loss: 0.071 accuracy: 0.984\n",
      "2017-08-13 22:48:19,182 - INFO - step: 2434,loss: 0.140 accuracy: 0.953\n",
      "2017-08-13 22:48:19,245 - INFO - step: 2435,loss: 0.083 accuracy: 0.969\n",
      "2017-08-13 22:48:19,308 - INFO - step: 2436,loss: 0.124 accuracy: 0.938\n",
      "2017-08-13 22:48:19,373 - INFO - step: 2437,loss: 0.060 accuracy: 0.969\n",
      "2017-08-13 22:48:19,436 - INFO - step: 2438,loss: 0.036 accuracy: 1.000\n",
      "2017-08-13 22:48:19,500 - INFO - step: 2439,loss: 0.161 accuracy: 0.938\n",
      "2017-08-13 22:48:19,567 - INFO - step: 2440,loss: 0.100 accuracy: 0.953\n",
      "2017-08-13 22:48:19,631 - INFO - step: 2441,loss: 0.077 accuracy: 0.969\n",
      "2017-08-13 22:48:19,693 - INFO - step: 2442,loss: 0.116 accuracy: 0.984\n",
      "2017-08-13 22:48:19,753 - INFO - step: 2443,loss: 0.377 accuracy: 0.859\n",
      "2017-08-13 22:48:19,816 - INFO - step: 2444,loss: 0.044 accuracy: 1.000\n",
      "2017-08-13 22:48:19,877 - INFO - step: 2445,loss: 0.133 accuracy: 0.922\n",
      "2017-08-13 22:48:19,940 - INFO - step: 2446,loss: 0.069 accuracy: 0.984\n",
      "2017-08-13 22:48:20,004 - INFO - step: 2447,loss: 0.034 accuracy: 1.000\n",
      "2017-08-13 22:48:20,026 - INFO - step: 2448,loss: 0.291 accuracy: 0.714\n",
      "2017-08-13 22:48:20,029 - INFO - \train_epoch:\n",
      "2017-08-13 22:48:20,031 - INFO - loss_total: 0.109 accuracy_total: 0.955\n",
      "2017-08-13 22:48:20,114 - INFO - step: 2449,loss: 0.175 accuracy: 0.922\n",
      "2017-08-13 22:48:20,178 - INFO - step: 2450,loss: 0.067 accuracy: 0.984\n",
      "2017-08-13 22:48:20,241 - INFO - step: 2451,loss: 0.098 accuracy: 0.969\n",
      "2017-08-13 22:48:20,304 - INFO - step: 2452,loss: 0.089 accuracy: 0.969\n",
      "2017-08-13 22:48:20,367 - INFO - step: 2453,loss: 0.066 accuracy: 0.984\n",
      "2017-08-13 22:48:20,431 - INFO - step: 2454,loss: 0.057 accuracy: 0.984\n",
      "2017-08-13 22:48:20,494 - INFO - step: 2455,loss: 0.097 accuracy: 0.969\n",
      "2017-08-13 22:48:20,558 - INFO - step: 2456,loss: 0.077 accuracy: 0.984\n",
      "2017-08-13 22:48:20,622 - INFO - step: 2457,loss: 0.024 accuracy: 1.000\n",
      "2017-08-13 22:48:20,685 - INFO - step: 2458,loss: 0.103 accuracy: 0.969\n",
      "2017-08-13 22:48:20,748 - INFO - step: 2459,loss: 0.056 accuracy: 1.000\n",
      "2017-08-13 22:48:20,809 - INFO - step: 2460,loss: 0.075 accuracy: 0.984\n",
      "2017-08-13 22:48:20,872 - INFO - step: 2461,loss: 0.076 accuracy: 0.969\n",
      "2017-08-13 22:48:20,935 - INFO - step: 2462,loss: 0.069 accuracy: 0.984\n",
      "2017-08-13 22:48:21,010 - INFO - step: 2463,loss: 0.071 accuracy: 0.969\n",
      "2017-08-13 22:48:21,079 - INFO - step: 2464,loss: 0.082 accuracy: 0.984\n",
      "2017-08-13 22:48:21,142 - INFO - step: 2465,loss: 0.217 accuracy: 0.922\n",
      "2017-08-13 22:48:21,205 - INFO - step: 2466,loss: 0.153 accuracy: 0.969\n",
      "2017-08-13 22:48:21,268 - INFO - step: 2467,loss: 0.155 accuracy: 0.922\n",
      "2017-08-13 22:48:21,330 - INFO - step: 2468,loss: 0.086 accuracy: 0.984\n",
      "2017-08-13 22:48:21,394 - INFO - step: 2469,loss: 0.077 accuracy: 0.969\n",
      "2017-08-13 22:48:21,457 - INFO - step: 2470,loss: 0.087 accuracy: 0.953\n",
      "2017-08-13 22:48:21,519 - INFO - step: 2471,loss: 0.057 accuracy: 0.984\n",
      "2017-08-13 22:48:21,582 - INFO - step: 2472,loss: 0.076 accuracy: 0.953\n",
      "2017-08-13 22:48:21,645 - INFO - step: 2473,loss: 0.065 accuracy: 1.000\n",
      "2017-08-13 22:48:21,709 - INFO - step: 2474,loss: 0.064 accuracy: 0.969\n",
      "2017-08-13 22:48:21,770 - INFO - step: 2475,loss: 0.112 accuracy: 0.938\n",
      "2017-08-13 22:48:21,833 - INFO - step: 2476,loss: 0.109 accuracy: 0.953\n",
      "2017-08-13 22:48:21,894 - INFO - step: 2477,loss: 0.091 accuracy: 0.969\n",
      "2017-08-13 22:48:21,956 - INFO - step: 2478,loss: 0.055 accuracy: 1.000\n",
      "2017-08-13 22:48:22,020 - INFO - step: 2479,loss: 0.146 accuracy: 0.953\n",
      "2017-08-13 22:48:22,081 - INFO - step: 2480,loss: 0.127 accuracy: 0.969\n",
      "2017-08-13 22:48:22,145 - INFO - step: 2481,loss: 0.058 accuracy: 0.984\n",
      "2017-08-13 22:48:22,207 - INFO - step: 2482,loss: 0.177 accuracy: 0.922\n",
      "2017-08-13 22:48:22,271 - INFO - step: 2483,loss: 0.189 accuracy: 0.891\n",
      "2017-08-13 22:48:22,293 - INFO - step: 2484,loss: 0.083 accuracy: 1.000\n",
      "2017-08-13 22:48:22,296 - INFO - \n",
      "Evaluation:\n",
      "2017-08-13 22:48:22,358 - INFO - step: 2484,loss: 0.226 accuracy: 0.912\n",
      "2017-08-13 22:48:22,360 - INFO - \train_epoch:\n",
      "2017-08-13 22:48:22,361 - INFO - loss_total: 0.096 accuracy_total: 0.967\n",
      "2017-08-13 22:48:22,444 - INFO - step: 2485,loss: 0.067 accuracy: 0.969\n",
      "2017-08-13 22:48:22,507 - INFO - step: 2486,loss: 0.094 accuracy: 0.969\n",
      "2017-08-13 22:48:22,570 - INFO - step: 2487,loss: 0.100 accuracy: 0.938\n",
      "2017-08-13 22:48:22,627 - INFO - step: 2488,loss: 0.140 accuracy: 0.953\n",
      "2017-08-13 22:48:22,693 - INFO - step: 2489,loss: 0.087 accuracy: 0.969\n",
      "2017-08-13 22:48:22,754 - INFO - step: 2490,loss: 0.106 accuracy: 0.953\n",
      "2017-08-13 22:48:22,816 - INFO - step: 2491,loss: 0.071 accuracy: 0.969\n",
      "2017-08-13 22:48:22,879 - INFO - step: 2492,loss: 0.115 accuracy: 0.969\n",
      "2017-08-13 22:48:22,941 - INFO - step: 2493,loss: 0.136 accuracy: 0.953\n",
      "2017-08-13 22:48:23,004 - INFO - step: 2494,loss: 0.081 accuracy: 0.984\n",
      "2017-08-13 22:48:23,067 - INFO - step: 2495,loss: 0.124 accuracy: 0.969\n",
      "2017-08-13 22:48:23,131 - INFO - step: 2496,loss: 0.145 accuracy: 0.938\n",
      "2017-08-13 22:48:23,194 - INFO - step: 2497,loss: 0.043 accuracy: 1.000\n",
      "2017-08-13 22:48:23,257 - INFO - step: 2498,loss: 0.097 accuracy: 0.969\n",
      "2017-08-13 22:48:23,319 - INFO - step: 2499,loss: 0.106 accuracy: 0.953\n",
      "2017-08-13 22:48:23,383 - INFO - step: 2500,loss: 0.123 accuracy: 0.953\n",
      "2017-08-13 22:48:23,444 - INFO - step: 2501,loss: 0.115 accuracy: 0.953\n",
      "2017-08-13 22:48:23,507 - INFO - step: 2502,loss: 0.069 accuracy: 0.984\n",
      "2017-08-13 22:48:23,571 - INFO - step: 2503,loss: 0.102 accuracy: 0.984\n",
      "2017-08-13 22:48:23,635 - INFO - step: 2504,loss: 0.072 accuracy: 0.969\n",
      "2017-08-13 22:48:23,698 - INFO - step: 2505,loss: 0.066 accuracy: 0.969\n",
      "2017-08-13 22:48:23,759 - INFO - step: 2506,loss: 0.119 accuracy: 0.953\n",
      "2017-08-13 22:48:23,823 - INFO - step: 2507,loss: 0.111 accuracy: 0.953\n",
      "2017-08-13 22:48:23,885 - INFO - step: 2508,loss: 0.097 accuracy: 0.969\n",
      "2017-08-13 22:48:23,950 - INFO - step: 2509,loss: 0.046 accuracy: 0.984\n",
      "2017-08-13 22:48:24,011 - INFO - step: 2510,loss: 0.083 accuracy: 0.969\n",
      "2017-08-13 22:48:24,075 - INFO - step: 2511,loss: 0.040 accuracy: 1.000\n",
      "2017-08-13 22:48:24,133 - INFO - step: 2512,loss: 0.205 accuracy: 0.906\n",
      "2017-08-13 22:48:24,195 - INFO - step: 2513,loss: 0.052 accuracy: 1.000\n",
      "2017-08-13 22:48:24,279 - INFO - step: 2514,loss: 0.114 accuracy: 0.938\n",
      "2017-08-13 22:48:24,345 - INFO - step: 2515,loss: 0.044 accuracy: 1.000\n",
      "2017-08-13 22:48:24,408 - INFO - step: 2516,loss: 0.050 accuracy: 0.984\n",
      "2017-08-13 22:48:24,472 - INFO - step: 2517,loss: 0.128 accuracy: 0.922\n",
      "2017-08-13 22:48:24,536 - INFO - step: 2518,loss: 0.063 accuracy: 0.984\n",
      "2017-08-13 22:48:24,603 - INFO - step: 2519,loss: 0.116 accuracy: 0.938\n",
      "2017-08-13 22:48:24,627 - INFO - step: 2520,loss: 0.122 accuracy: 0.857\n",
      "2017-08-13 22:48:24,629 - INFO - \train_epoch:\n",
      "2017-08-13 22:48:24,631 - INFO - loss_total: 0.096 accuracy_total: 0.962\n",
      "2017-08-13 22:48:24,704 - INFO - step: 2521,loss: 0.078 accuracy: 0.969\n",
      "2017-08-13 22:48:24,768 - INFO - step: 2522,loss: 0.063 accuracy: 0.984\n",
      "2017-08-13 22:48:24,830 - INFO - step: 2523,loss: 0.108 accuracy: 0.984\n",
      "2017-08-13 22:48:24,897 - INFO - step: 2524,loss: 0.081 accuracy: 0.969\n",
      "2017-08-13 22:48:24,960 - INFO - step: 2525,loss: 0.036 accuracy: 1.000\n",
      "2017-08-13 22:48:25,024 - INFO - step: 2526,loss: 0.059 accuracy: 0.969\n",
      "2017-08-13 22:48:25,087 - INFO - step: 2527,loss: 0.027 accuracy: 1.000\n",
      "2017-08-13 22:48:25,148 - INFO - step: 2528,loss: 0.055 accuracy: 0.984\n",
      "2017-08-13 22:48:25,211 - INFO - step: 2529,loss: 0.061 accuracy: 0.984\n",
      "2017-08-13 22:48:25,274 - INFO - step: 2530,loss: 0.059 accuracy: 0.984\n",
      "2017-08-13 22:48:25,338 - INFO - step: 2531,loss: 0.097 accuracy: 0.953\n",
      "2017-08-13 22:48:25,401 - INFO - step: 2532,loss: 0.093 accuracy: 0.953\n",
      "2017-08-13 22:48:25,462 - INFO - step: 2533,loss: 0.090 accuracy: 0.969\n",
      "2017-08-13 22:48:25,525 - INFO - step: 2534,loss: 0.130 accuracy: 0.953\n",
      "2017-08-13 22:48:25,587 - INFO - step: 2535,loss: 0.062 accuracy: 0.969\n",
      "2017-08-13 22:48:25,649 - INFO - step: 2536,loss: 0.072 accuracy: 0.953\n",
      "2017-08-13 22:48:25,711 - INFO - step: 2537,loss: 0.101 accuracy: 0.953\n",
      "2017-08-13 22:48:25,777 - INFO - step: 2538,loss: 0.169 accuracy: 0.969\n",
      "2017-08-13 22:48:25,840 - INFO - step: 2539,loss: 0.109 accuracy: 0.938\n",
      "2017-08-13 22:48:25,904 - INFO - step: 2540,loss: 0.098 accuracy: 0.953\n",
      "2017-08-13 22:48:25,966 - INFO - step: 2541,loss: 0.040 accuracy: 1.000\n",
      "2017-08-13 22:48:26,033 - INFO - step: 2542,loss: 0.246 accuracy: 0.891\n",
      "2017-08-13 22:48:26,096 - INFO - step: 2543,loss: 0.105 accuracy: 0.938\n",
      "2017-08-13 22:48:26,160 - INFO - step: 2544,loss: 0.086 accuracy: 0.969\n",
      "2017-08-13 22:48:26,222 - INFO - step: 2545,loss: 0.135 accuracy: 0.938\n",
      "2017-08-13 22:48:26,284 - INFO - step: 2546,loss: 0.056 accuracy: 0.984\n",
      "2017-08-13 22:48:26,346 - INFO - step: 2547,loss: 0.199 accuracy: 0.906\n",
      "2017-08-13 22:48:26,410 - INFO - step: 2548,loss: 0.171 accuracy: 0.922\n",
      "2017-08-13 22:48:26,474 - INFO - step: 2549,loss: 0.067 accuracy: 0.984\n",
      "2017-08-13 22:48:26,535 - INFO - step: 2550,loss: 0.056 accuracy: 0.969\n",
      "2017-08-13 22:48:26,599 - INFO - step: 2551,loss: 0.062 accuracy: 0.984\n",
      "2017-08-13 22:48:26,663 - INFO - step: 2552,loss: 0.193 accuracy: 0.953\n",
      "2017-08-13 22:48:26,726 - INFO - step: 2553,loss: 0.064 accuracy: 0.984\n",
      "2017-08-13 22:48:26,788 - INFO - step: 2554,loss: 0.084 accuracy: 0.969\n",
      "2017-08-13 22:48:26,851 - INFO - step: 2555,loss: 0.046 accuracy: 0.984\n",
      "2017-08-13 22:48:26,874 - INFO - step: 2556,loss: 0.080 accuracy: 1.000\n",
      "2017-08-13 22:48:26,876 - INFO - \n",
      "Evaluation:\n",
      "2017-08-13 22:48:26,957 - INFO - step: 2556,loss: 0.235 accuracy: 0.916\n",
      "2017-08-13 22:48:26,959 - INFO - \train_epoch:\n",
      "2017-08-13 22:48:26,961 - INFO - loss_total: 0.093 accuracy_total: 0.966\n",
      "2017-08-13 22:48:27,031 - INFO - step: 2557,loss: 0.127 accuracy: 0.969\n",
      "2017-08-13 22:48:27,095 - INFO - step: 2558,loss: 0.071 accuracy: 0.969\n",
      "2017-08-13 22:48:27,158 - INFO - step: 2559,loss: 0.127 accuracy: 0.953\n",
      "2017-08-13 22:48:27,219 - INFO - step: 2560,loss: 0.261 accuracy: 0.953\n",
      "2017-08-13 22:48:27,283 - INFO - step: 2561,loss: 0.074 accuracy: 0.969\n",
      "2017-08-13 22:48:27,348 - INFO - step: 2562,loss: 0.048 accuracy: 0.984\n",
      "2017-08-13 22:48:27,411 - INFO - step: 2563,loss: 0.068 accuracy: 0.969\n",
      "2017-08-13 22:48:27,476 - INFO - step: 2564,loss: 0.175 accuracy: 0.938\n",
      "2017-08-13 22:48:27,539 - INFO - step: 2565,loss: 0.164 accuracy: 0.906\n",
      "2017-08-13 22:48:27,603 - INFO - step: 2566,loss: 0.086 accuracy: 0.953\n",
      "2017-08-13 22:48:27,666 - INFO - step: 2567,loss: 0.064 accuracy: 0.984\n",
      "2017-08-13 22:48:27,733 - INFO - step: 2568,loss: 0.061 accuracy: 0.969\n",
      "2017-08-13 22:48:27,798 - INFO - step: 2569,loss: 0.107 accuracy: 0.953\n",
      "2017-08-13 22:48:27,860 - INFO - step: 2570,loss: 0.139 accuracy: 0.953\n",
      "2017-08-13 22:48:27,924 - INFO - step: 2571,loss: 0.135 accuracy: 0.906\n",
      "2017-08-13 22:48:28,003 - INFO - step: 2572,loss: 0.215 accuracy: 0.938\n",
      "2017-08-13 22:48:28,066 - INFO - step: 2573,loss: 0.137 accuracy: 0.938\n",
      "2017-08-13 22:48:28,128 - INFO - step: 2574,loss: 0.085 accuracy: 0.984\n",
      "2017-08-13 22:48:28,191 - INFO - step: 2575,loss: 0.026 accuracy: 1.000\n",
      "2017-08-13 22:48:28,253 - INFO - step: 2576,loss: 0.088 accuracy: 0.984\n",
      "2017-08-13 22:48:28,317 - INFO - step: 2577,loss: 0.130 accuracy: 0.953\n",
      "2017-08-13 22:48:28,380 - INFO - step: 2578,loss: 0.032 accuracy: 1.000\n",
      "2017-08-13 22:48:28,444 - INFO - step: 2579,loss: 0.099 accuracy: 0.969\n",
      "2017-08-13 22:48:28,506 - INFO - step: 2580,loss: 0.093 accuracy: 0.953\n",
      "2017-08-13 22:48:28,571 - INFO - step: 2581,loss: 0.076 accuracy: 0.984\n",
      "2017-08-13 22:48:28,635 - INFO - step: 2582,loss: 0.019 accuracy: 1.000\n",
      "2017-08-13 22:48:28,697 - INFO - step: 2583,loss: 0.112 accuracy: 0.953\n",
      "2017-08-13 22:48:28,761 - INFO - step: 2584,loss: 0.077 accuracy: 0.953\n",
      "2017-08-13 22:48:28,825 - INFO - step: 2585,loss: 0.115 accuracy: 0.969\n",
      "2017-08-13 22:48:28,889 - INFO - step: 2586,loss: 0.058 accuracy: 0.984\n",
      "2017-08-13 22:48:28,952 - INFO - step: 2587,loss: 0.034 accuracy: 1.000\n",
      "2017-08-13 22:48:29,016 - INFO - step: 2588,loss: 0.043 accuracy: 1.000\n",
      "2017-08-13 22:48:29,087 - INFO - step: 2589,loss: 0.124 accuracy: 0.969\n",
      "2017-08-13 22:48:29,151 - INFO - step: 2590,loss: 0.073 accuracy: 0.969\n",
      "2017-08-13 22:48:29,214 - INFO - step: 2591,loss: 0.054 accuracy: 0.984\n",
      "2017-08-13 22:48:29,237 - INFO - step: 2592,loss: 0.011 accuracy: 1.000\n",
      "2017-08-13 22:48:29,239 - INFO - \train_epoch:\n",
      "2017-08-13 22:48:29,241 - INFO - loss_total: 0.095 accuracy_total: 0.967\n",
      "2017-08-13 22:48:29,325 - INFO - step: 2593,loss: 0.090 accuracy: 0.984\n",
      "2017-08-13 22:48:29,391 - INFO - step: 2594,loss: 0.074 accuracy: 0.969\n",
      "2017-08-13 22:48:29,456 - INFO - step: 2595,loss: 0.102 accuracy: 0.953\n",
      "2017-08-13 22:48:29,520 - INFO - step: 2596,loss: 0.057 accuracy: 0.984\n",
      "2017-08-13 22:48:29,583 - INFO - step: 2597,loss: 0.053 accuracy: 0.984\n",
      "2017-08-13 22:48:29,645 - INFO - step: 2598,loss: 0.206 accuracy: 0.969\n",
      "2017-08-13 22:48:29,709 - INFO - step: 2599,loss: 0.105 accuracy: 0.969\n",
      "2017-08-13 22:48:29,772 - INFO - step: 2600,loss: 0.116 accuracy: 0.969\n",
      "2017-08-13 22:48:29,838 - INFO - step: 2601,loss: 0.070 accuracy: 0.984\n",
      "2017-08-13 22:48:29,901 - INFO - step: 2602,loss: 0.087 accuracy: 0.969\n",
      "2017-08-13 22:48:29,964 - INFO - step: 2603,loss: 0.100 accuracy: 0.969\n",
      "2017-08-13 22:48:30,026 - INFO - step: 2604,loss: 0.044 accuracy: 1.000\n",
      "2017-08-13 22:48:30,090 - INFO - step: 2605,loss: 0.048 accuracy: 0.984\n",
      "2017-08-13 22:48:30,152 - INFO - step: 2606,loss: 0.155 accuracy: 0.938\n",
      "2017-08-13 22:48:30,217 - INFO - step: 2607,loss: 0.103 accuracy: 0.969\n",
      "2017-08-13 22:48:30,283 - INFO - step: 2608,loss: 0.086 accuracy: 0.953\n",
      "2017-08-13 22:48:30,347 - INFO - step: 2609,loss: 0.127 accuracy: 0.969\n",
      "2017-08-13 22:48:30,409 - INFO - step: 2610,loss: 0.111 accuracy: 0.953\n",
      "2017-08-13 22:48:30,472 - INFO - step: 2611,loss: 0.083 accuracy: 0.953\n",
      "2017-08-13 22:48:30,534 - INFO - step: 2612,loss: 0.097 accuracy: 0.969\n",
      "2017-08-13 22:48:30,597 - INFO - step: 2613,loss: 0.061 accuracy: 0.984\n",
      "2017-08-13 22:48:30,660 - INFO - step: 2614,loss: 0.123 accuracy: 0.922\n",
      "2017-08-13 22:48:30,722 - INFO - step: 2615,loss: 0.066 accuracy: 0.969\n",
      "2017-08-13 22:48:30,783 - INFO - step: 2616,loss: 0.107 accuracy: 0.953\n",
      "2017-08-13 22:48:30,846 - INFO - step: 2617,loss: 0.110 accuracy: 0.953\n",
      "2017-08-13 22:48:30,907 - INFO - step: 2618,loss: 0.128 accuracy: 0.953\n",
      "2017-08-13 22:48:30,978 - INFO - step: 2619,loss: 0.260 accuracy: 0.906\n",
      "2017-08-13 22:48:31,054 - INFO - step: 2620,loss: 0.110 accuracy: 0.938\n",
      "2017-08-13 22:48:31,117 - INFO - step: 2621,loss: 0.109 accuracy: 0.953\n",
      "2017-08-13 22:48:31,181 - INFO - step: 2622,loss: 0.052 accuracy: 0.984\n",
      "2017-08-13 22:48:31,244 - INFO - step: 2623,loss: 0.126 accuracy: 0.969\n",
      "2017-08-13 22:48:31,306 - INFO - step: 2624,loss: 0.109 accuracy: 0.969\n",
      "2017-08-13 22:48:31,368 - INFO - step: 2625,loss: 0.092 accuracy: 0.984\n",
      "2017-08-13 22:48:31,431 - INFO - step: 2626,loss: 0.066 accuracy: 0.969\n",
      "2017-08-13 22:48:31,494 - INFO - step: 2627,loss: 0.055 accuracy: 1.000\n",
      "2017-08-13 22:48:31,516 - INFO - step: 2628,loss: 0.055 accuracy: 1.000\n",
      "2017-08-13 22:48:31,518 - INFO - \n",
      "Evaluation:\n",
      "2017-08-13 22:48:31,577 - INFO - step: 2628,loss: 0.242 accuracy: 0.912\n",
      "2017-08-13 22:48:31,579 - INFO - \train_epoch:\n",
      "2017-08-13 22:48:31,580 - INFO - loss_total: 0.098 accuracy_total: 0.967\n",
      "2017-08-13 22:48:31,665 - INFO - step: 2629,loss: 0.027 accuracy: 0.984\n",
      "2017-08-13 22:48:31,727 - INFO - step: 2630,loss: 0.032 accuracy: 0.984\n",
      "2017-08-13 22:48:31,790 - INFO - step: 2631,loss: 0.066 accuracy: 0.969\n",
      "2017-08-13 22:48:31,853 - INFO - step: 2632,loss: 0.100 accuracy: 0.953\n",
      "2017-08-13 22:48:31,917 - INFO - step: 2633,loss: 0.038 accuracy: 0.984\n",
      "2017-08-13 22:48:31,982 - INFO - step: 2634,loss: 0.142 accuracy: 0.953\n",
      "2017-08-13 22:48:32,046 - INFO - step: 2635,loss: 0.114 accuracy: 0.938\n",
      "2017-08-13 22:48:32,109 - INFO - step: 2636,loss: 0.136 accuracy: 0.953\n",
      "2017-08-13 22:48:32,172 - INFO - step: 2637,loss: 0.034 accuracy: 1.000\n",
      "2017-08-13 22:48:32,234 - INFO - step: 2638,loss: 0.052 accuracy: 0.984\n",
      "2017-08-13 22:48:32,298 - INFO - step: 2639,loss: 0.161 accuracy: 0.938\n",
      "2017-08-13 22:48:32,362 - INFO - step: 2640,loss: 0.078 accuracy: 0.984\n",
      "2017-08-13 22:48:32,424 - INFO - step: 2641,loss: 0.087 accuracy: 0.969\n",
      "2017-08-13 22:48:32,487 - INFO - step: 2642,loss: 0.094 accuracy: 0.969\n",
      "2017-08-13 22:48:32,553 - INFO - step: 2643,loss: 0.093 accuracy: 0.953\n",
      "2017-08-13 22:48:32,616 - INFO - step: 2644,loss: 0.083 accuracy: 0.969\n",
      "2017-08-13 22:48:32,678 - INFO - step: 2645,loss: 0.063 accuracy: 0.984\n",
      "2017-08-13 22:48:32,741 - INFO - step: 2646,loss: 0.147 accuracy: 0.953\n",
      "2017-08-13 22:48:32,804 - INFO - step: 2647,loss: 0.063 accuracy: 0.984\n",
      "2017-08-13 22:48:32,865 - INFO - step: 2648,loss: 0.117 accuracy: 0.938\n",
      "2017-08-13 22:48:32,932 - INFO - step: 2649,loss: 0.075 accuracy: 0.953\n",
      "2017-08-13 22:48:32,994 - INFO - step: 2650,loss: 0.084 accuracy: 0.969\n",
      "2017-08-13 22:48:33,057 - INFO - step: 2651,loss: 0.045 accuracy: 0.984\n",
      "2017-08-13 22:48:33,124 - INFO - step: 2652,loss: 0.119 accuracy: 0.938\n",
      "2017-08-13 22:48:33,186 - INFO - step: 2653,loss: 0.115 accuracy: 0.969\n",
      "2017-08-13 22:48:33,250 - INFO - step: 2654,loss: 0.070 accuracy: 0.969\n",
      "2017-08-13 22:48:33,313 - INFO - step: 2655,loss: 0.102 accuracy: 0.953\n",
      "2017-08-13 22:48:33,375 - INFO - step: 2656,loss: 0.088 accuracy: 0.969\n",
      "2017-08-13 22:48:33,439 - INFO - step: 2657,loss: 0.071 accuracy: 0.984\n",
      "2017-08-13 22:48:33,501 - INFO - step: 2658,loss: 0.106 accuracy: 0.969\n",
      "2017-08-13 22:48:33,563 - INFO - step: 2659,loss: 0.066 accuracy: 0.969\n",
      "2017-08-13 22:48:33,626 - INFO - step: 2660,loss: 0.127 accuracy: 0.969\n",
      "2017-08-13 22:48:33,686 - INFO - step: 2661,loss: 0.134 accuracy: 0.938\n",
      "2017-08-13 22:48:33,750 - INFO - step: 2662,loss: 0.116 accuracy: 0.938\n",
      "2017-08-13 22:48:33,812 - INFO - step: 2663,loss: 0.096 accuracy: 0.969\n",
      "2017-08-13 22:48:33,837 - INFO - step: 2664,loss: 0.026 accuracy: 1.000\n",
      "2017-08-13 22:48:33,839 - INFO - \train_epoch:\n",
      "2017-08-13 22:48:33,841 - INFO - loss_total: 0.088 accuracy_total: 0.966\n",
      "2017-08-13 22:48:33,914 - INFO - step: 2665,loss: 0.085 accuracy: 0.953\n",
      "2017-08-13 22:48:33,976 - INFO - step: 2666,loss: 0.136 accuracy: 0.938\n",
      "2017-08-13 22:48:34,037 - INFO - step: 2667,loss: 0.142 accuracy: 0.922\n",
      "2017-08-13 22:48:34,101 - INFO - step: 2668,loss: 0.140 accuracy: 0.938\n",
      "2017-08-13 22:48:34,164 - INFO - step: 2669,loss: 0.083 accuracy: 0.984\n",
      "2017-08-13 22:48:34,228 - INFO - step: 2670,loss: 0.069 accuracy: 0.984\n",
      "2017-08-13 22:48:34,290 - INFO - step: 2671,loss: 0.067 accuracy: 0.984\n",
      "2017-08-13 22:48:34,351 - INFO - step: 2672,loss: 0.071 accuracy: 0.984\n",
      "2017-08-13 22:48:34,415 - INFO - step: 2673,loss: 0.111 accuracy: 0.938\n",
      "2017-08-13 22:48:34,477 - INFO - step: 2674,loss: 0.072 accuracy: 0.984\n",
      "2017-08-13 22:48:34,539 - INFO - step: 2675,loss: 0.045 accuracy: 1.000\n",
      "2017-08-13 22:48:34,602 - INFO - step: 2676,loss: 0.116 accuracy: 0.953\n",
      "2017-08-13 22:48:34,665 - INFO - step: 2677,loss: 0.079 accuracy: 0.969\n",
      "2017-08-13 22:48:34,731 - INFO - step: 2678,loss: 0.136 accuracy: 0.953\n",
      "2017-08-13 22:48:34,795 - INFO - step: 2679,loss: 0.043 accuracy: 0.984\n",
      "2017-08-13 22:48:34,857 - INFO - step: 2680,loss: 0.083 accuracy: 0.953\n",
      "2017-08-13 22:48:34,919 - INFO - step: 2681,loss: 0.066 accuracy: 0.984\n",
      "2017-08-13 22:48:34,982 - INFO - step: 2682,loss: 0.039 accuracy: 0.984\n",
      "2017-08-13 22:48:35,045 - INFO - step: 2683,loss: 0.141 accuracy: 0.953\n",
      "2017-08-13 22:48:35,108 - INFO - step: 2684,loss: 0.091 accuracy: 0.953\n",
      "2017-08-13 22:48:35,170 - INFO - step: 2685,loss: 0.108 accuracy: 0.922\n",
      "2017-08-13 22:48:35,233 - INFO - step: 2686,loss: 0.116 accuracy: 0.938\n",
      "2017-08-13 22:48:35,294 - INFO - step: 2687,loss: 0.111 accuracy: 0.953\n",
      "2017-08-13 22:48:35,357 - INFO - step: 2688,loss: 0.061 accuracy: 0.984\n",
      "2017-08-13 22:48:35,420 - INFO - step: 2689,loss: 0.098 accuracy: 0.969\n",
      "2017-08-13 22:48:35,484 - INFO - step: 2690,loss: 0.146 accuracy: 0.922\n",
      "2017-08-13 22:48:35,549 - INFO - step: 2691,loss: 0.084 accuracy: 0.969\n",
      "2017-08-13 22:48:35,611 - INFO - step: 2692,loss: 0.081 accuracy: 0.953\n",
      "2017-08-13 22:48:35,674 - INFO - step: 2693,loss: 0.081 accuracy: 0.969\n",
      "2017-08-13 22:48:35,739 - INFO - step: 2694,loss: 0.093 accuracy: 0.938\n",
      "2017-08-13 22:48:35,803 - INFO - step: 2695,loss: 0.045 accuracy: 0.984\n",
      "2017-08-13 22:48:35,867 - INFO - step: 2696,loss: 0.055 accuracy: 0.984\n",
      "2017-08-13 22:48:35,930 - INFO - step: 2697,loss: 0.061 accuracy: 1.000\n",
      "2017-08-13 22:48:35,993 - INFO - step: 2698,loss: 0.247 accuracy: 0.906\n",
      "2017-08-13 22:48:36,055 - INFO - step: 2699,loss: 0.084 accuracy: 0.953\n",
      "2017-08-13 22:48:36,078 - INFO - step: 2700,loss: 0.205 accuracy: 0.857\n",
      "2017-08-13 22:48:36,081 - INFO - \n",
      "Evaluation:\n",
      "2017-08-13 22:48:36,146 - INFO - step: 2700,loss: 0.253 accuracy: 0.912\n",
      "2017-08-13 22:48:36,147 - INFO - \train_epoch:\n",
      "2017-08-13 22:48:36,149 - INFO - loss_total: 0.097 accuracy_total: 0.958\n",
      "2017-08-13 22:48:36,220 - INFO - step: 2701,loss: 0.066 accuracy: 0.969\n",
      "2017-08-13 22:48:36,283 - INFO - step: 2702,loss: 0.100 accuracy: 0.969\n",
      "2017-08-13 22:48:36,347 - INFO - step: 2703,loss: 0.093 accuracy: 0.953\n",
      "2017-08-13 22:48:36,410 - INFO - step: 2704,loss: 0.070 accuracy: 0.953\n",
      "2017-08-13 22:48:36,474 - INFO - step: 2705,loss: 0.029 accuracy: 0.984\n",
      "2017-08-13 22:48:36,535 - INFO - step: 2706,loss: 0.094 accuracy: 0.969\n",
      "2017-08-13 22:48:36,600 - INFO - step: 2707,loss: 0.216 accuracy: 0.984\n",
      "2017-08-13 22:48:36,663 - INFO - step: 2708,loss: 0.081 accuracy: 0.938\n",
      "2017-08-13 22:48:36,725 - INFO - step: 2709,loss: 0.118 accuracy: 0.953\n",
      "2017-08-13 22:48:36,789 - INFO - step: 2710,loss: 0.064 accuracy: 0.984\n",
      "2017-08-13 22:48:36,852 - INFO - step: 2711,loss: 0.040 accuracy: 0.969\n",
      "2017-08-13 22:48:36,918 - INFO - step: 2712,loss: 0.072 accuracy: 0.984\n",
      "2017-08-13 22:48:36,981 - INFO - step: 2713,loss: 0.156 accuracy: 0.953\n",
      "2017-08-13 22:48:37,044 - INFO - step: 2714,loss: 0.051 accuracy: 0.984\n",
      "2017-08-13 22:48:37,108 - INFO - step: 2715,loss: 0.058 accuracy: 0.984\n",
      "2017-08-13 22:48:37,171 - INFO - step: 2716,loss: 0.147 accuracy: 0.922\n",
      "2017-08-13 22:48:37,233 - INFO - step: 2717,loss: 0.081 accuracy: 0.953\n",
      "2017-08-13 22:48:37,296 - INFO - step: 2718,loss: 0.109 accuracy: 0.938\n",
      "2017-08-13 22:48:37,358 - INFO - step: 2719,loss: 0.043 accuracy: 0.984\n",
      "2017-08-13 22:48:37,423 - INFO - step: 2720,loss: 0.176 accuracy: 0.938\n",
      "2017-08-13 22:48:37,487 - INFO - step: 2721,loss: 0.078 accuracy: 0.984\n",
      "2017-08-13 22:48:37,551 - INFO - step: 2722,loss: 0.055 accuracy: 0.984\n",
      "2017-08-13 22:48:37,615 - INFO - step: 2723,loss: 0.103 accuracy: 0.938\n",
      "2017-08-13 22:48:37,679 - INFO - step: 2724,loss: 0.113 accuracy: 0.922\n",
      "2017-08-13 22:48:37,742 - INFO - step: 2725,loss: 0.068 accuracy: 0.984\n",
      "2017-08-13 22:48:37,804 - INFO - step: 2726,loss: 0.101 accuracy: 0.953\n",
      "2017-08-13 22:48:37,868 - INFO - step: 2727,loss: 0.049 accuracy: 0.984\n",
      "2017-08-13 22:48:37,933 - INFO - step: 2728,loss: 0.095 accuracy: 0.953\n",
      "2017-08-13 22:48:37,997 - INFO - step: 2729,loss: 0.134 accuracy: 0.938\n",
      "2017-08-13 22:48:38,059 - INFO - step: 2730,loss: 0.140 accuracy: 0.953\n",
      "2017-08-13 22:48:38,122 - INFO - step: 2731,loss: 0.082 accuracy: 0.953\n",
      "2017-08-13 22:48:38,185 - INFO - step: 2732,loss: 0.061 accuracy: 0.984\n",
      "2017-08-13 22:48:38,249 - INFO - step: 2733,loss: 0.126 accuracy: 0.969\n",
      "2017-08-13 22:48:38,310 - INFO - step: 2734,loss: 0.075 accuracy: 0.953\n",
      "2017-08-13 22:48:38,374 - INFO - step: 2735,loss: 0.057 accuracy: 0.969\n",
      "2017-08-13 22:48:38,395 - INFO - step: 2736,loss: 0.761 accuracy: 0.714\n",
      "2017-08-13 22:48:38,398 - INFO - \train_epoch:\n",
      "2017-08-13 22:48:38,399 - INFO - loss_total: 0.110 accuracy_total: 0.956\n",
      "2017-08-13 22:48:38,470 - INFO - step: 2737,loss: 0.125 accuracy: 0.953\n",
      "2017-08-13 22:48:38,534 - INFO - step: 2738,loss: 0.083 accuracy: 0.969\n",
      "2017-08-13 22:48:38,597 - INFO - step: 2739,loss: 0.062 accuracy: 0.969\n",
      "2017-08-13 22:48:38,659 - INFO - step: 2740,loss: 0.093 accuracy: 0.984\n",
      "2017-08-13 22:48:38,722 - INFO - step: 2741,loss: 0.065 accuracy: 0.984\n",
      "2017-08-13 22:48:38,785 - INFO - step: 2742,loss: 0.071 accuracy: 0.969\n",
      "2017-08-13 22:48:38,849 - INFO - step: 2743,loss: 0.064 accuracy: 0.984\n",
      "2017-08-13 22:48:38,920 - INFO - step: 2744,loss: 0.025 accuracy: 1.000\n",
      "2017-08-13 22:48:38,984 - INFO - step: 2745,loss: 0.055 accuracy: 0.984\n",
      "2017-08-13 22:48:39,048 - INFO - step: 2746,loss: 0.061 accuracy: 0.984\n",
      "2017-08-13 22:48:39,110 - INFO - step: 2747,loss: 0.074 accuracy: 0.969\n",
      "2017-08-13 22:48:39,174 - INFO - step: 2748,loss: 0.071 accuracy: 0.969\n",
      "2017-08-13 22:48:39,239 - INFO - step: 2749,loss: 0.080 accuracy: 0.969\n",
      "2017-08-13 22:48:39,302 - INFO - step: 2750,loss: 0.127 accuracy: 0.953\n",
      "2017-08-13 22:48:39,365 - INFO - step: 2751,loss: 0.075 accuracy: 0.953\n",
      "2017-08-13 22:48:39,429 - INFO - step: 2752,loss: 0.102 accuracy: 0.953\n",
      "2017-08-13 22:48:39,492 - INFO - step: 2753,loss: 0.056 accuracy: 0.984\n",
      "2017-08-13 22:48:39,556 - INFO - step: 2754,loss: 0.088 accuracy: 0.969\n",
      "2017-08-13 22:48:39,620 - INFO - step: 2755,loss: 0.140 accuracy: 0.938\n",
      "2017-08-13 22:48:39,683 - INFO - step: 2756,loss: 0.086 accuracy: 0.984\n",
      "2017-08-13 22:48:39,747 - INFO - step: 2757,loss: 0.255 accuracy: 0.906\n",
      "2017-08-13 22:48:39,811 - INFO - step: 2758,loss: 0.067 accuracy: 0.969\n",
      "2017-08-13 22:48:39,878 - INFO - step: 2759,loss: 0.046 accuracy: 0.984\n",
      "2017-08-13 22:48:39,941 - INFO - step: 2760,loss: 0.154 accuracy: 0.969\n",
      "2017-08-13 22:48:40,005 - INFO - step: 2761,loss: 0.066 accuracy: 0.969\n",
      "2017-08-13 22:48:40,070 - INFO - step: 2762,loss: 0.035 accuracy: 0.984\n",
      "2017-08-13 22:48:40,132 - INFO - step: 2763,loss: 0.033 accuracy: 1.000\n",
      "2017-08-13 22:48:40,194 - INFO - step: 2764,loss: 0.078 accuracy: 0.969\n",
      "2017-08-13 22:48:40,258 - INFO - step: 2765,loss: 0.160 accuracy: 0.938\n",
      "2017-08-13 22:48:40,320 - INFO - step: 2766,loss: 0.135 accuracy: 0.969\n",
      "2017-08-13 22:48:40,385 - INFO - step: 2767,loss: 0.030 accuracy: 1.000\n",
      "2017-08-13 22:48:40,451 - INFO - step: 2768,loss: 0.174 accuracy: 0.969\n",
      "2017-08-13 22:48:40,514 - INFO - step: 2769,loss: 0.179 accuracy: 0.906\n",
      "2017-08-13 22:48:40,577 - INFO - step: 2770,loss: 0.045 accuracy: 0.984\n",
      "2017-08-13 22:48:40,640 - INFO - step: 2771,loss: 0.023 accuracy: 1.000\n",
      "2017-08-13 22:48:40,663 - INFO - step: 2772,loss: 0.001 accuracy: 1.000\n",
      "2017-08-13 22:48:40,665 - INFO - \n",
      "Evaluation:\n",
      "2017-08-13 22:48:40,748 - INFO - step: 2772,loss: 0.238 accuracy: 0.916\n",
      "2017-08-13 22:48:40,749 - INFO - \train_epoch:\n",
      "2017-08-13 22:48:40,751 - INFO - loss_total: 0.086 accuracy_total: 0.970\n",
      "2017-08-13 22:48:40,833 - INFO - step: 2773,loss: 0.054 accuracy: 0.969\n",
      "2017-08-13 22:48:40,897 - INFO - step: 2774,loss: 0.066 accuracy: 0.969\n",
      "2017-08-13 22:48:40,961 - INFO - step: 2775,loss: 0.137 accuracy: 0.922\n",
      "2017-08-13 22:48:41,023 - INFO - step: 2776,loss: 0.088 accuracy: 0.953\n",
      "2017-08-13 22:48:41,085 - INFO - step: 2777,loss: 0.067 accuracy: 0.984\n",
      "2017-08-13 22:48:41,149 - INFO - step: 2778,loss: 0.099 accuracy: 0.969\n",
      "2017-08-13 22:48:41,213 - INFO - step: 2779,loss: 0.045 accuracy: 0.984\n",
      "2017-08-13 22:48:41,278 - INFO - step: 2780,loss: 0.038 accuracy: 1.000\n",
      "2017-08-13 22:48:41,342 - INFO - step: 2781,loss: 0.066 accuracy: 0.969\n",
      "2017-08-13 22:48:41,405 - INFO - step: 2782,loss: 0.067 accuracy: 0.984\n",
      "2017-08-13 22:48:41,470 - INFO - step: 2783,loss: 0.052 accuracy: 0.984\n",
      "2017-08-13 22:48:41,535 - INFO - step: 2784,loss: 0.089 accuracy: 0.969\n",
      "2017-08-13 22:48:41,599 - INFO - step: 2785,loss: 0.072 accuracy: 0.984\n",
      "2017-08-13 22:48:41,662 - INFO - step: 2786,loss: 0.071 accuracy: 0.969\n",
      "2017-08-13 22:48:41,725 - INFO - step: 2787,loss: 0.098 accuracy: 0.953\n",
      "2017-08-13 22:48:41,787 - INFO - step: 2788,loss: 0.183 accuracy: 0.953\n",
      "2017-08-13 22:48:41,851 - INFO - step: 2789,loss: 0.072 accuracy: 0.953\n",
      "2017-08-13 22:48:41,916 - INFO - step: 2790,loss: 0.093 accuracy: 0.938\n",
      "2017-08-13 22:48:41,980 - INFO - step: 2791,loss: 0.147 accuracy: 0.938\n",
      "2017-08-13 22:48:42,043 - INFO - step: 2792,loss: 0.063 accuracy: 0.984\n",
      "2017-08-13 22:48:42,104 - INFO - step: 2793,loss: 0.167 accuracy: 0.953\n",
      "2017-08-13 22:48:42,167 - INFO - step: 2794,loss: 0.079 accuracy: 0.938\n",
      "2017-08-13 22:48:42,231 - INFO - step: 2795,loss: 0.058 accuracy: 0.984\n",
      "2017-08-13 22:48:42,294 - INFO - step: 2796,loss: 0.136 accuracy: 0.953\n",
      "2017-08-13 22:48:42,357 - INFO - step: 2797,loss: 0.130 accuracy: 0.953\n",
      "2017-08-13 22:48:42,419 - INFO - step: 2798,loss: 0.033 accuracy: 1.000\n",
      "2017-08-13 22:48:42,482 - INFO - step: 2799,loss: 0.078 accuracy: 0.984\n",
      "2017-08-13 22:48:42,547 - INFO - step: 2800,loss: 0.094 accuracy: 0.984\n",
      "2017-08-13 22:48:42,610 - INFO - step: 2801,loss: 0.060 accuracy: 1.000\n",
      "2017-08-13 22:48:42,675 - INFO - step: 2802,loss: 0.127 accuracy: 0.938\n",
      "2017-08-13 22:48:42,739 - INFO - step: 2803,loss: 0.071 accuracy: 0.984\n",
      "2017-08-13 22:48:42,799 - INFO - step: 2804,loss: 0.119 accuracy: 0.922\n",
      "2017-08-13 22:48:42,861 - INFO - step: 2805,loss: 0.201 accuracy: 0.953\n",
      "2017-08-13 22:48:42,923 - INFO - step: 2806,loss: 0.033 accuracy: 1.000\n",
      "2017-08-13 22:48:42,987 - INFO - step: 2807,loss: 0.063 accuracy: 0.969\n",
      "2017-08-13 22:48:43,010 - INFO - step: 2808,loss: 0.015 accuracy: 1.000\n",
      "2017-08-13 22:48:43,012 - INFO - \train_epoch:\n",
      "2017-08-13 22:48:43,014 - INFO - loss_total: 0.087 accuracy_total: 0.968\n",
      "2017-08-13 22:48:43,098 - INFO - step: 2809,loss: 0.057 accuracy: 0.984\n",
      "2017-08-13 22:48:43,162 - INFO - step: 2810,loss: 0.105 accuracy: 0.969\n",
      "2017-08-13 22:48:43,224 - INFO - step: 2811,loss: 0.119 accuracy: 0.953\n",
      "2017-08-13 22:48:43,291 - INFO - step: 2812,loss: 0.081 accuracy: 0.984\n",
      "2017-08-13 22:48:43,352 - INFO - step: 2813,loss: 0.158 accuracy: 0.922\n",
      "2017-08-13 22:48:43,416 - INFO - step: 2814,loss: 0.109 accuracy: 0.953\n",
      "2017-08-13 22:48:43,480 - INFO - step: 2815,loss: 0.038 accuracy: 1.000\n",
      "2017-08-13 22:48:43,544 - INFO - step: 2816,loss: 0.155 accuracy: 0.953\n",
      "2017-08-13 22:48:43,607 - INFO - step: 2817,loss: 0.092 accuracy: 0.953\n",
      "2017-08-13 22:48:43,669 - INFO - step: 2818,loss: 0.107 accuracy: 0.969\n",
      "2017-08-13 22:48:43,733 - INFO - step: 2819,loss: 0.115 accuracy: 0.938\n",
      "2017-08-13 22:48:43,795 - INFO - step: 2820,loss: 0.128 accuracy: 0.922\n",
      "2017-08-13 22:48:43,859 - INFO - step: 2821,loss: 0.135 accuracy: 0.938\n",
      "2017-08-13 22:48:43,931 - INFO - step: 2822,loss: 0.085 accuracy: 0.969\n",
      "2017-08-13 22:48:43,995 - INFO - step: 2823,loss: 0.041 accuracy: 0.984\n",
      "2017-08-13 22:48:44,058 - INFO - step: 2824,loss: 0.087 accuracy: 0.984\n",
      "2017-08-13 22:48:44,120 - INFO - step: 2825,loss: 0.082 accuracy: 0.969\n",
      "2017-08-13 22:48:44,184 - INFO - step: 2826,loss: 0.084 accuracy: 0.969\n",
      "2017-08-13 22:48:44,246 - INFO - step: 2827,loss: 0.051 accuracy: 0.969\n",
      "2017-08-13 22:48:44,309 - INFO - step: 2828,loss: 0.060 accuracy: 0.969\n",
      "2017-08-13 22:48:44,371 - INFO - step: 2829,loss: 0.028 accuracy: 1.000\n",
      "2017-08-13 22:48:44,434 - INFO - step: 2830,loss: 0.063 accuracy: 0.984\n",
      "2017-08-13 22:48:44,497 - INFO - step: 2831,loss: 0.076 accuracy: 0.969\n",
      "2017-08-13 22:48:44,561 - INFO - step: 2832,loss: 0.039 accuracy: 0.984\n",
      "2017-08-13 22:48:44,623 - INFO - step: 2833,loss: 0.048 accuracy: 0.984\n",
      "2017-08-13 22:48:44,688 - INFO - step: 2834,loss: 0.079 accuracy: 0.969\n",
      "2017-08-13 22:48:44,751 - INFO - step: 2835,loss: 0.061 accuracy: 0.969\n",
      "2017-08-13 22:48:44,816 - INFO - step: 2836,loss: 0.049 accuracy: 0.984\n",
      "2017-08-13 22:48:44,883 - INFO - step: 2837,loss: 0.074 accuracy: 0.984\n",
      "2017-08-13 22:48:44,945 - INFO - step: 2838,loss: 0.102 accuracy: 0.953\n",
      "2017-08-13 22:48:45,005 - INFO - step: 2839,loss: 0.090 accuracy: 0.969\n",
      "2017-08-13 22:48:45,068 - INFO - step: 2840,loss: 0.031 accuracy: 1.000\n",
      "2017-08-13 22:48:45,131 - INFO - step: 2841,loss: 0.145 accuracy: 0.906\n",
      "2017-08-13 22:48:45,196 - INFO - step: 2842,loss: 0.040 accuracy: 0.984\n",
      "2017-08-13 22:48:45,258 - INFO - step: 2843,loss: 0.089 accuracy: 0.953\n",
      "2017-08-13 22:48:45,282 - INFO - step: 2844,loss: 0.030 accuracy: 1.000\n",
      "2017-08-13 22:48:45,284 - INFO - \n",
      "Evaluation:\n",
      "2017-08-13 22:48:45,367 - INFO - step: 2844,loss: 0.237 accuracy: 0.916\n",
      "2017-08-13 22:48:45,370 - INFO - \train_epoch:\n",
      "2017-08-13 22:48:45,371 - INFO - loss_total: 0.081 accuracy_total: 0.968\n",
      "2017-08-13 22:48:45,455 - INFO - step: 2845,loss: 0.170 accuracy: 0.922\n",
      "2017-08-13 22:48:45,519 - INFO - step: 2846,loss: 0.014 accuracy: 1.000\n",
      "2017-08-13 22:48:45,581 - INFO - step: 2847,loss: 0.043 accuracy: 0.984\n",
      "2017-08-13 22:48:45,642 - INFO - step: 2848,loss: 0.041 accuracy: 1.000\n",
      "2017-08-13 22:48:45,706 - INFO - step: 2849,loss: 0.074 accuracy: 0.984\n",
      "2017-08-13 22:48:45,770 - INFO - step: 2850,loss: 0.073 accuracy: 0.969\n",
      "2017-08-13 22:48:45,834 - INFO - step: 2851,loss: 0.103 accuracy: 0.953\n",
      "2017-08-13 22:48:45,897 - INFO - step: 2852,loss: 0.148 accuracy: 0.953\n",
      "2017-08-13 22:48:45,960 - INFO - step: 2853,loss: 0.083 accuracy: 0.953\n",
      "2017-08-13 22:48:46,022 - INFO - step: 2854,loss: 0.109 accuracy: 0.969\n",
      "2017-08-13 22:48:46,086 - INFO - step: 2855,loss: 0.064 accuracy: 0.969\n",
      "2017-08-13 22:48:46,163 - INFO - step: 2856,loss: 0.156 accuracy: 0.922\n",
      "2017-08-13 22:48:46,226 - INFO - step: 2857,loss: 0.059 accuracy: 0.969\n",
      "2017-08-13 22:48:46,289 - INFO - step: 2858,loss: 0.140 accuracy: 0.953\n",
      "2017-08-13 22:48:46,351 - INFO - step: 2859,loss: 0.082 accuracy: 0.953\n",
      "2017-08-13 22:48:46,413 - INFO - step: 2860,loss: 0.052 accuracy: 0.984\n",
      "2017-08-13 22:48:46,476 - INFO - step: 2861,loss: 0.034 accuracy: 1.000\n",
      "2017-08-13 22:48:46,540 - INFO - step: 2862,loss: 0.083 accuracy: 0.984\n",
      "2017-08-13 22:48:46,601 - INFO - step: 2863,loss: 0.036 accuracy: 1.000\n",
      "2017-08-13 22:48:46,664 - INFO - step: 2864,loss: 0.063 accuracy: 0.969\n",
      "2017-08-13 22:48:46,728 - INFO - step: 2865,loss: 0.175 accuracy: 0.938\n",
      "2017-08-13 22:48:46,789 - INFO - step: 2866,loss: 0.075 accuracy: 0.969\n",
      "2017-08-13 22:48:46,851 - INFO - step: 2867,loss: 0.107 accuracy: 0.969\n",
      "2017-08-13 22:48:46,915 - INFO - step: 2868,loss: 0.081 accuracy: 0.953\n",
      "2017-08-13 22:48:46,975 - INFO - step: 2869,loss: 0.142 accuracy: 0.922\n",
      "2017-08-13 22:48:47,038 - INFO - step: 2870,loss: 0.076 accuracy: 0.969\n",
      "2017-08-13 22:48:47,101 - INFO - step: 2871,loss: 0.093 accuracy: 0.953\n",
      "2017-08-13 22:48:47,163 - INFO - step: 2872,loss: 0.072 accuracy: 0.969\n",
      "2017-08-13 22:48:47,226 - INFO - step: 2873,loss: 0.130 accuracy: 0.938\n",
      "2017-08-13 22:48:47,290 - INFO - step: 2874,loss: 0.064 accuracy: 0.984\n",
      "2017-08-13 22:48:47,355 - INFO - step: 2875,loss: 0.121 accuracy: 0.922\n",
      "2017-08-13 22:48:47,419 - INFO - step: 2876,loss: 0.088 accuracy: 0.953\n",
      "2017-08-13 22:48:47,480 - INFO - step: 2877,loss: 0.083 accuracy: 0.938\n",
      "2017-08-13 22:48:47,543 - INFO - step: 2878,loss: 0.057 accuracy: 0.984\n",
      "2017-08-13 22:48:47,606 - INFO - step: 2879,loss: 0.044 accuracy: 1.000\n",
      "2017-08-13 22:48:47,628 - INFO - step: 2880,loss: 0.155 accuracy: 0.857\n",
      "2017-08-13 22:48:47,631 - INFO - \train_epoch:\n",
      "2017-08-13 22:48:47,633 - INFO - loss_total: 0.089 accuracy_total: 0.961\n",
      "2017-08-13 22:48:47,705 - INFO - step: 2881,loss: 0.082 accuracy: 0.969\n",
      "2017-08-13 22:48:47,768 - INFO - step: 2882,loss: 0.114 accuracy: 0.938\n",
      "2017-08-13 22:48:47,827 - INFO - step: 2883,loss: 0.058 accuracy: 0.984\n",
      "2017-08-13 22:48:47,891 - INFO - step: 2884,loss: 0.032 accuracy: 1.000\n",
      "2017-08-13 22:48:47,957 - INFO - step: 2885,loss: 0.126 accuracy: 0.938\n",
      "2017-08-13 22:48:48,021 - INFO - step: 2886,loss: 0.110 accuracy: 0.984\n",
      "2017-08-13 22:48:48,083 - INFO - step: 2887,loss: 0.060 accuracy: 0.984\n",
      "2017-08-13 22:48:48,145 - INFO - step: 2888,loss: 0.025 accuracy: 1.000\n",
      "2017-08-13 22:48:48,204 - INFO - step: 2889,loss: 0.098 accuracy: 0.969\n",
      "2017-08-13 22:48:48,268 - INFO - step: 2890,loss: 0.105 accuracy: 0.969\n",
      "2017-08-13 22:48:48,331 - INFO - step: 2891,loss: 0.052 accuracy: 0.984\n",
      "2017-08-13 22:48:48,398 - INFO - step: 2892,loss: 0.232 accuracy: 0.906\n",
      "2017-08-13 22:48:48,459 - INFO - step: 2893,loss: 0.030 accuracy: 0.984\n",
      "2017-08-13 22:48:48,522 - INFO - step: 2894,loss: 0.125 accuracy: 0.938\n",
      "2017-08-13 22:48:48,584 - INFO - step: 2895,loss: 0.044 accuracy: 0.984\n",
      "2017-08-13 22:48:48,647 - INFO - step: 2896,loss: 0.063 accuracy: 0.984\n",
      "2017-08-13 22:48:48,709 - INFO - step: 2897,loss: 0.058 accuracy: 0.969\n",
      "2017-08-13 22:48:48,771 - INFO - step: 2898,loss: 0.071 accuracy: 0.969\n",
      "2017-08-13 22:48:48,836 - INFO - step: 2899,loss: 0.187 accuracy: 0.938\n",
      "2017-08-13 22:48:48,897 - INFO - step: 2900,loss: 0.053 accuracy: 0.984\n",
      "2017-08-13 22:48:48,960 - INFO - step: 2901,loss: 0.050 accuracy: 0.969\n",
      "2017-08-13 22:48:49,019 - INFO - step: 2902,loss: 0.110 accuracy: 0.953\n",
      "2017-08-13 22:48:49,081 - INFO - step: 2903,loss: 0.104 accuracy: 0.953\n",
      "2017-08-13 22:48:49,144 - INFO - step: 2904,loss: 0.049 accuracy: 0.984\n",
      "2017-08-13 22:48:49,205 - INFO - step: 2905,loss: 0.130 accuracy: 0.922\n",
      "2017-08-13 22:48:49,267 - INFO - step: 2906,loss: 0.111 accuracy: 0.953\n",
      "2017-08-13 22:48:49,330 - INFO - step: 2907,loss: 0.069 accuracy: 0.984\n",
      "2017-08-13 22:48:49,393 - INFO - step: 2908,loss: 0.056 accuracy: 0.969\n",
      "2017-08-13 22:48:49,456 - INFO - step: 2909,loss: 0.056 accuracy: 0.984\n",
      "2017-08-13 22:48:49,520 - INFO - step: 2910,loss: 0.117 accuracy: 0.953\n",
      "2017-08-13 22:48:49,583 - INFO - step: 2911,loss: 0.245 accuracy: 0.922\n",
      "2017-08-13 22:48:49,647 - INFO - step: 2912,loss: 0.043 accuracy: 1.000\n",
      "2017-08-13 22:48:49,711 - INFO - step: 2913,loss: 0.161 accuracy: 0.938\n",
      "2017-08-13 22:48:49,774 - INFO - step: 2914,loss: 0.022 accuracy: 1.000\n",
      "2017-08-13 22:48:49,839 - INFO - step: 2915,loss: 0.229 accuracy: 0.906\n",
      "2017-08-13 22:48:49,860 - INFO - step: 2916,loss: 0.047 accuracy: 1.000\n",
      "2017-08-13 22:48:49,862 - INFO - \n",
      "Evaluation:\n",
      "2017-08-13 22:48:49,923 - INFO - step: 2916,loss: 0.238 accuracy: 0.908\n",
      "2017-08-13 22:48:49,925 - INFO - \train_epoch:\n",
      "2017-08-13 22:48:49,926 - INFO - loss_total: 0.092 accuracy_total: 0.966\n",
      "2017-08-13 22:48:49,995 - INFO - step: 2917,loss: 0.084 accuracy: 0.969\n",
      "2017-08-13 22:48:50,060 - INFO - step: 2918,loss: 0.043 accuracy: 1.000\n",
      "2017-08-13 22:48:50,124 - INFO - step: 2919,loss: 0.191 accuracy: 0.938\n",
      "2017-08-13 22:48:50,188 - INFO - step: 2920,loss: 0.139 accuracy: 0.953\n",
      "2017-08-13 22:48:50,248 - INFO - step: 2921,loss: 0.099 accuracy: 0.953\n",
      "2017-08-13 22:48:50,315 - INFO - step: 2922,loss: 0.064 accuracy: 0.984\n",
      "2017-08-13 22:48:50,379 - INFO - step: 2923,loss: 0.074 accuracy: 0.953\n",
      "2017-08-13 22:48:50,441 - INFO - step: 2924,loss: 0.050 accuracy: 0.969\n",
      "2017-08-13 22:48:50,505 - INFO - step: 2925,loss: 0.040 accuracy: 1.000\n",
      "2017-08-13 22:48:50,567 - INFO - step: 2926,loss: 0.021 accuracy: 1.000\n",
      "2017-08-13 22:48:50,630 - INFO - step: 2927,loss: 0.119 accuracy: 0.969\n",
      "2017-08-13 22:48:50,693 - INFO - step: 2928,loss: 0.082 accuracy: 0.953\n",
      "2017-08-13 22:48:50,756 - INFO - step: 2929,loss: 0.160 accuracy: 0.969\n",
      "2017-08-13 22:48:50,819 - INFO - step: 2930,loss: 0.026 accuracy: 0.984\n",
      "2017-08-13 22:48:50,883 - INFO - step: 2931,loss: 0.044 accuracy: 0.984\n",
      "2017-08-13 22:48:50,946 - INFO - step: 2932,loss: 0.036 accuracy: 0.984\n",
      "2017-08-13 22:48:51,007 - INFO - step: 2933,loss: 0.081 accuracy: 0.969\n",
      "2017-08-13 22:48:51,071 - INFO - step: 2934,loss: 0.111 accuracy: 0.953\n",
      "2017-08-13 22:48:51,133 - INFO - step: 2935,loss: 0.055 accuracy: 0.984\n",
      "2017-08-13 22:48:51,205 - INFO - step: 2936,loss: 0.068 accuracy: 0.969\n",
      "2017-08-13 22:48:51,265 - INFO - step: 2937,loss: 0.039 accuracy: 0.984\n",
      "2017-08-13 22:48:51,327 - INFO - step: 2938,loss: 0.054 accuracy: 0.984\n",
      "2017-08-13 22:48:51,391 - INFO - step: 2939,loss: 0.134 accuracy: 0.953\n",
      "2017-08-13 22:48:51,453 - INFO - step: 2940,loss: 0.086 accuracy: 0.969\n",
      "2017-08-13 22:48:51,517 - INFO - step: 2941,loss: 0.104 accuracy: 0.969\n",
      "2017-08-13 22:48:51,580 - INFO - step: 2942,loss: 0.107 accuracy: 0.938\n",
      "2017-08-13 22:48:51,642 - INFO - step: 2943,loss: 0.058 accuracy: 0.984\n",
      "2017-08-13 22:48:51,704 - INFO - step: 2944,loss: 0.053 accuracy: 0.984\n",
      "2017-08-13 22:48:51,768 - INFO - step: 2945,loss: 0.124 accuracy: 0.938\n",
      "2017-08-13 22:48:51,831 - INFO - step: 2946,loss: 0.042 accuracy: 0.984\n",
      "2017-08-13 22:48:51,897 - INFO - step: 2947,loss: 0.133 accuracy: 0.938\n",
      "2017-08-13 22:48:51,960 - INFO - step: 2948,loss: 0.058 accuracy: 0.984\n",
      "2017-08-13 22:48:52,026 - INFO - step: 2949,loss: 0.076 accuracy: 0.969\n",
      "2017-08-13 22:48:52,090 - INFO - step: 2950,loss: 0.038 accuracy: 0.984\n",
      "2017-08-13 22:48:52,153 - INFO - step: 2951,loss: 0.060 accuracy: 1.000\n",
      "2017-08-13 22:48:52,177 - INFO - step: 2952,loss: 0.117 accuracy: 1.000\n",
      "2017-08-13 22:48:52,180 - INFO - \train_epoch:\n",
      "2017-08-13 22:48:52,182 - INFO - loss_total: 0.080 accuracy_total: 0.972\n",
      "2017-08-13 22:48:52,252 - INFO - step: 2953,loss: 0.081 accuracy: 0.984\n",
      "2017-08-13 22:48:52,314 - INFO - step: 2954,loss: 0.099 accuracy: 0.953\n",
      "2017-08-13 22:48:52,378 - INFO - step: 2955,loss: 0.104 accuracy: 0.953\n",
      "2017-08-13 22:48:52,439 - INFO - step: 2956,loss: 0.072 accuracy: 0.969\n",
      "2017-08-13 22:48:52,501 - INFO - step: 2957,loss: 0.053 accuracy: 0.984\n",
      "2017-08-13 22:48:52,563 - INFO - step: 2958,loss: 0.093 accuracy: 0.938\n",
      "2017-08-13 22:48:52,625 - INFO - step: 2959,loss: 0.045 accuracy: 0.984\n",
      "2017-08-13 22:48:52,688 - INFO - step: 2960,loss: 0.041 accuracy: 0.984\n",
      "2017-08-13 22:48:52,750 - INFO - step: 2961,loss: 0.170 accuracy: 0.969\n",
      "2017-08-13 22:48:52,813 - INFO - step: 2962,loss: 0.303 accuracy: 0.953\n",
      "2017-08-13 22:48:52,876 - INFO - step: 2963,loss: 0.099 accuracy: 0.953\n",
      "2017-08-13 22:48:52,939 - INFO - step: 2964,loss: 0.089 accuracy: 0.938\n",
      "2017-08-13 22:48:53,002 - INFO - step: 2965,loss: 0.124 accuracy: 0.984\n",
      "2017-08-13 22:48:53,063 - INFO - step: 2966,loss: 0.080 accuracy: 0.953\n",
      "2017-08-13 22:48:53,126 - INFO - step: 2967,loss: 0.029 accuracy: 1.000\n",
      "2017-08-13 22:48:53,191 - INFO - step: 2968,loss: 0.143 accuracy: 0.922\n",
      "2017-08-13 22:48:53,255 - INFO - step: 2969,loss: 0.045 accuracy: 0.984\n",
      "2017-08-13 22:48:53,318 - INFO - step: 2970,loss: 0.095 accuracy: 0.953\n",
      "2017-08-13 22:48:53,380 - INFO - step: 2971,loss: 0.111 accuracy: 0.953\n",
      "2017-08-13 22:48:53,442 - INFO - step: 2972,loss: 0.077 accuracy: 0.984\n",
      "2017-08-13 22:48:53,505 - INFO - step: 2973,loss: 0.074 accuracy: 0.984\n",
      "2017-08-13 22:48:53,567 - INFO - step: 2974,loss: 0.028 accuracy: 1.000\n",
      "2017-08-13 22:48:53,627 - INFO - step: 2975,loss: 0.064 accuracy: 0.969\n",
      "2017-08-13 22:48:53,689 - INFO - step: 2976,loss: 0.042 accuracy: 0.984\n",
      "2017-08-13 22:48:53,753 - INFO - step: 2977,loss: 0.082 accuracy: 0.969\n",
      "2017-08-13 22:48:53,816 - INFO - step: 2978,loss: 0.084 accuracy: 0.984\n",
      "2017-08-13 22:48:53,879 - INFO - step: 2979,loss: 0.029 accuracy: 0.984\n",
      "2017-08-13 22:48:53,941 - INFO - step: 2980,loss: 0.090 accuracy: 0.969\n",
      "2017-08-13 22:48:54,004 - INFO - step: 2981,loss: 0.095 accuracy: 0.984\n",
      "2017-08-13 22:48:54,065 - INFO - step: 2982,loss: 0.084 accuracy: 0.984\n",
      "2017-08-13 22:48:54,127 - INFO - step: 2983,loss: 0.148 accuracy: 0.938\n",
      "2017-08-13 22:48:54,191 - INFO - step: 2984,loss: 0.044 accuracy: 1.000\n",
      "2017-08-13 22:48:54,252 - INFO - step: 2985,loss: 0.111 accuracy: 0.938\n",
      "2017-08-13 22:48:54,315 - INFO - step: 2986,loss: 0.071 accuracy: 0.969\n",
      "2017-08-13 22:48:54,380 - INFO - step: 2987,loss: 0.043 accuracy: 0.969\n",
      "2017-08-13 22:48:54,402 - INFO - step: 2988,loss: 0.054 accuracy: 1.000\n",
      "2017-08-13 22:48:54,404 - INFO - \n",
      "Evaluation:\n",
      "2017-08-13 22:48:54,466 - INFO - step: 2988,loss: 0.265 accuracy: 0.904\n",
      "2017-08-13 22:48:54,467 - INFO - \train_epoch:\n",
      "2017-08-13 22:48:54,469 - INFO - loss_total: 0.086 accuracy_total: 0.970\n",
      "2017-08-13 22:48:54,552 - INFO - step: 2989,loss: 0.110 accuracy: 0.953\n",
      "2017-08-13 22:48:54,616 - INFO - step: 2990,loss: 0.089 accuracy: 0.953\n",
      "2017-08-13 22:48:54,679 - INFO - step: 2991,loss: 0.051 accuracy: 0.969\n",
      "2017-08-13 22:48:54,742 - INFO - step: 2992,loss: 0.096 accuracy: 0.969\n",
      "2017-08-13 22:48:54,805 - INFO - step: 2993,loss: 0.183 accuracy: 0.953\n",
      "2017-08-13 22:48:54,869 - INFO - step: 2994,loss: 0.051 accuracy: 1.000\n",
      "2017-08-13 22:48:54,932 - INFO - step: 2995,loss: 0.027 accuracy: 1.000\n",
      "2017-08-13 22:48:54,995 - INFO - step: 2996,loss: 0.107 accuracy: 0.984\n",
      "2017-08-13 22:48:55,058 - INFO - step: 2997,loss: 0.047 accuracy: 0.984\n",
      "2017-08-13 22:48:55,122 - INFO - step: 2998,loss: 0.087 accuracy: 0.938\n",
      "2017-08-13 22:48:55,185 - INFO - step: 2999,loss: 0.082 accuracy: 0.969\n",
      "2017-08-13 22:48:55,248 - INFO - step: 3000,loss: 0.098 accuracy: 0.953\n",
      "2017-08-13 22:48:55,311 - INFO - step: 3001,loss: 0.091 accuracy: 0.969\n",
      "2017-08-13 22:48:55,373 - INFO - step: 3002,loss: 0.033 accuracy: 0.984\n",
      "2017-08-13 22:48:55,436 - INFO - step: 3003,loss: 0.078 accuracy: 0.938\n",
      "2017-08-13 22:48:55,499 - INFO - step: 3004,loss: 0.137 accuracy: 0.922\n",
      "2017-08-13 22:48:55,561 - INFO - step: 3005,loss: 0.083 accuracy: 0.969\n",
      "2017-08-13 22:48:55,625 - INFO - step: 3006,loss: 0.037 accuracy: 1.000\n",
      "2017-08-13 22:48:55,687 - INFO - step: 3007,loss: 0.056 accuracy: 0.969\n",
      "2017-08-13 22:48:55,750 - INFO - step: 3008,loss: 0.043 accuracy: 0.984\n",
      "2017-08-13 22:48:55,814 - INFO - step: 3009,loss: 0.015 accuracy: 1.000\n",
      "2017-08-13 22:48:55,877 - INFO - step: 3010,loss: 0.166 accuracy: 0.922\n",
      "2017-08-13 22:48:55,939 - INFO - step: 3011,loss: 0.070 accuracy: 0.969\n",
      "2017-08-13 22:48:56,002 - INFO - step: 3012,loss: 0.039 accuracy: 1.000\n",
      "2017-08-13 22:48:56,066 - INFO - step: 3013,loss: 0.075 accuracy: 0.953\n",
      "2017-08-13 22:48:56,128 - INFO - step: 3014,loss: 0.101 accuracy: 0.953\n",
      "2017-08-13 22:48:56,191 - INFO - step: 3015,loss: 0.062 accuracy: 0.969\n",
      "2017-08-13 22:48:56,262 - INFO - step: 3016,loss: 0.124 accuracy: 0.953\n",
      "2017-08-13 22:48:56,326 - INFO - step: 3017,loss: 0.127 accuracy: 0.969\n",
      "2017-08-13 22:48:56,393 - INFO - step: 3018,loss: 0.176 accuracy: 0.906\n",
      "2017-08-13 22:48:56,457 - INFO - step: 3019,loss: 0.044 accuracy: 1.000\n",
      "2017-08-13 22:48:56,520 - INFO - step: 3020,loss: 0.084 accuracy: 0.953\n",
      "2017-08-13 22:48:56,584 - INFO - step: 3021,loss: 0.041 accuracy: 0.969\n",
      "2017-08-13 22:48:56,647 - INFO - step: 3022,loss: 0.030 accuracy: 1.000\n",
      "2017-08-13 22:48:56,711 - INFO - step: 3023,loss: 0.083 accuracy: 0.953\n",
      "2017-08-13 22:48:56,734 - INFO - step: 3024,loss: 0.022 accuracy: 1.000\n",
      "2017-08-13 22:48:56,736 - INFO - \train_epoch:\n",
      "2017-08-13 22:48:56,738 - INFO - loss_total: 0.079 accuracy_total: 0.967\n",
      "2017-08-13 22:48:56,808 - INFO - step: 3025,loss: 0.076 accuracy: 0.969\n",
      "2017-08-13 22:48:56,871 - INFO - step: 3026,loss: 0.132 accuracy: 0.922\n",
      "2017-08-13 22:48:56,935 - INFO - step: 3027,loss: 0.039 accuracy: 0.984\n",
      "2017-08-13 22:48:57,000 - INFO - step: 3028,loss: 0.113 accuracy: 0.984\n",
      "2017-08-13 22:48:57,064 - INFO - step: 3029,loss: 0.037 accuracy: 1.000\n",
      "2017-08-13 22:48:57,127 - INFO - step: 3030,loss: 0.031 accuracy: 0.984\n",
      "2017-08-13 22:48:57,191 - INFO - step: 3031,loss: 0.141 accuracy: 0.922\n",
      "2017-08-13 22:48:57,256 - INFO - step: 3032,loss: 0.069 accuracy: 0.984\n",
      "2017-08-13 22:48:57,318 - INFO - step: 3033,loss: 0.064 accuracy: 0.984\n",
      "2017-08-13 22:48:57,381 - INFO - step: 3034,loss: 0.065 accuracy: 0.984\n",
      "2017-08-13 22:48:57,443 - INFO - step: 3035,loss: 0.066 accuracy: 1.000\n",
      "2017-08-13 22:48:57,505 - INFO - step: 3036,loss: 0.084 accuracy: 0.953\n",
      "2017-08-13 22:48:57,569 - INFO - step: 3037,loss: 0.101 accuracy: 0.953\n",
      "2017-08-13 22:48:57,631 - INFO - step: 3038,loss: 0.085 accuracy: 0.969\n",
      "2017-08-13 22:48:57,694 - INFO - step: 3039,loss: 0.033 accuracy: 0.984\n",
      "2017-08-13 22:48:57,757 - INFO - step: 3040,loss: 0.055 accuracy: 0.984\n",
      "2017-08-13 22:48:57,818 - INFO - step: 3041,loss: 0.118 accuracy: 0.969\n",
      "2017-08-13 22:48:57,880 - INFO - step: 3042,loss: 0.097 accuracy: 0.984\n",
      "2017-08-13 22:48:57,942 - INFO - step: 3043,loss: 0.058 accuracy: 0.984\n",
      "2017-08-13 22:48:58,004 - INFO - step: 3044,loss: 0.083 accuracy: 0.953\n",
      "2017-08-13 22:48:58,067 - INFO - step: 3045,loss: 0.133 accuracy: 0.938\n",
      "2017-08-13 22:48:58,136 - INFO - step: 3046,loss: 0.143 accuracy: 0.922\n",
      "2017-08-13 22:48:58,199 - INFO - step: 3047,loss: 0.124 accuracy: 0.969\n",
      "2017-08-13 22:48:58,262 - INFO - step: 3048,loss: 0.084 accuracy: 0.969\n",
      "2017-08-13 22:48:58,325 - INFO - step: 3049,loss: 0.111 accuracy: 0.984\n",
      "2017-08-13 22:48:58,388 - INFO - step: 3050,loss: 0.081 accuracy: 0.984\n",
      "2017-08-13 22:48:58,453 - INFO - step: 3051,loss: 0.057 accuracy: 0.984\n",
      "2017-08-13 22:48:58,517 - INFO - step: 3052,loss: 0.115 accuracy: 0.953\n",
      "2017-08-13 22:48:58,581 - INFO - step: 3053,loss: 0.082 accuracy: 0.953\n",
      "2017-08-13 22:48:58,643 - INFO - step: 3054,loss: 0.173 accuracy: 0.953\n",
      "2017-08-13 22:48:58,707 - INFO - step: 3055,loss: 0.103 accuracy: 0.953\n",
      "2017-08-13 22:48:58,770 - INFO - step: 3056,loss: 0.101 accuracy: 0.969\n",
      "2017-08-13 22:48:58,832 - INFO - step: 3057,loss: 0.037 accuracy: 0.984\n",
      "2017-08-13 22:48:58,896 - INFO - step: 3058,loss: 0.072 accuracy: 0.969\n",
      "2017-08-13 22:48:58,957 - INFO - step: 3059,loss: 0.084 accuracy: 0.953\n",
      "2017-08-13 22:48:58,982 - INFO - step: 3060,loss: 0.008 accuracy: 1.000\n",
      "2017-08-13 22:48:58,984 - INFO - \n",
      "Evaluation:\n",
      "2017-08-13 22:48:59,067 - INFO - step: 3060,loss: 0.252 accuracy: 0.912\n",
      "2017-08-13 22:48:59,069 - INFO - \train_epoch:\n",
      "2017-08-13 22:48:59,071 - INFO - loss_total: 0.085 accuracy_total: 0.969\n",
      "2017-08-13 22:48:59,154 - INFO - step: 3061,loss: 0.066 accuracy: 0.984\n",
      "2017-08-13 22:48:59,218 - INFO - step: 3062,loss: 0.083 accuracy: 0.953\n",
      "2017-08-13 22:48:59,279 - INFO - step: 3063,loss: 0.091 accuracy: 0.953\n",
      "2017-08-13 22:48:59,343 - INFO - step: 3064,loss: 0.296 accuracy: 0.875\n",
      "2017-08-13 22:48:59,406 - INFO - step: 3065,loss: 0.084 accuracy: 0.969\n",
      "2017-08-13 22:48:59,469 - INFO - step: 3066,loss: 0.054 accuracy: 1.000\n",
      "2017-08-13 22:48:59,532 - INFO - step: 3067,loss: 0.106 accuracy: 0.938\n",
      "2017-08-13 22:48:59,595 - INFO - step: 3068,loss: 0.125 accuracy: 0.984\n",
      "2017-08-13 22:48:59,659 - INFO - step: 3069,loss: 0.075 accuracy: 0.969\n",
      "2017-08-13 22:48:59,722 - INFO - step: 3070,loss: 0.073 accuracy: 0.969\n",
      "2017-08-13 22:48:59,786 - INFO - step: 3071,loss: 0.148 accuracy: 0.922\n",
      "2017-08-13 22:48:59,850 - INFO - step: 3072,loss: 0.210 accuracy: 0.906\n",
      "2017-08-13 22:48:59,912 - INFO - step: 3073,loss: 0.159 accuracy: 0.938\n",
      "2017-08-13 22:48:59,974 - INFO - step: 3074,loss: 0.073 accuracy: 0.953\n",
      "2017-08-13 22:49:00,037 - INFO - step: 3075,loss: 0.058 accuracy: 1.000\n",
      "2017-08-13 22:49:00,100 - INFO - step: 3076,loss: 0.055 accuracy: 0.984\n",
      "2017-08-13 22:49:00,163 - INFO - step: 3077,loss: 0.050 accuracy: 0.984\n",
      "2017-08-13 22:49:00,231 - INFO - step: 3078,loss: 0.057 accuracy: 0.969\n",
      "2017-08-13 22:49:00,294 - INFO - step: 3079,loss: 0.058 accuracy: 0.984\n",
      "2017-08-13 22:49:00,357 - INFO - step: 3080,loss: 0.089 accuracy: 0.953\n",
      "2017-08-13 22:49:00,420 - INFO - step: 3081,loss: 0.117 accuracy: 0.969\n",
      "2017-08-13 22:49:00,482 - INFO - step: 3082,loss: 0.088 accuracy: 0.984\n",
      "2017-08-13 22:49:00,544 - INFO - step: 3083,loss: 0.077 accuracy: 0.953\n",
      "2017-08-13 22:49:00,608 - INFO - step: 3084,loss: 0.074 accuracy: 0.969\n",
      "2017-08-13 22:49:00,673 - INFO - step: 3085,loss: 0.031 accuracy: 1.000\n",
      "2017-08-13 22:49:00,736 - INFO - step: 3086,loss: 0.031 accuracy: 1.000\n",
      "2017-08-13 22:49:00,799 - INFO - step: 3087,loss: 0.035 accuracy: 1.000\n",
      "2017-08-13 22:49:00,861 - INFO - step: 3088,loss: 0.033 accuracy: 1.000\n",
      "2017-08-13 22:49:00,925 - INFO - step: 3089,loss: 0.102 accuracy: 0.984\n",
      "2017-08-13 22:49:00,988 - INFO - step: 3090,loss: 0.055 accuracy: 0.969\n",
      "2017-08-13 22:49:01,052 - INFO - step: 3091,loss: 0.110 accuracy: 0.938\n",
      "2017-08-13 22:49:01,126 - INFO - step: 3092,loss: 0.064 accuracy: 0.984\n",
      "2017-08-13 22:49:01,190 - INFO - step: 3093,loss: 0.069 accuracy: 0.953\n",
      "2017-08-13 22:49:01,253 - INFO - step: 3094,loss: 0.070 accuracy: 0.969\n",
      "2017-08-13 22:49:01,326 - INFO - step: 3095,loss: 0.097 accuracy: 0.953\n",
      "2017-08-13 22:49:01,349 - INFO - step: 3096,loss: 0.035 accuracy: 1.000\n",
      "2017-08-13 22:49:01,351 - INFO - \train_epoch:\n",
      "2017-08-13 22:49:01,353 - INFO - loss_total: 0.086 accuracy_total: 0.967\n",
      "2017-08-13 22:49:01,436 - INFO - step: 3097,loss: 0.139 accuracy: 0.953\n",
      "2017-08-13 22:49:01,500 - INFO - step: 3098,loss: 0.119 accuracy: 0.938\n",
      "2017-08-13 22:49:01,562 - INFO - step: 3099,loss: 0.047 accuracy: 0.984\n",
      "2017-08-13 22:49:01,627 - INFO - step: 3100,loss: 0.034 accuracy: 0.984\n",
      "2017-08-13 22:49:01,691 - INFO - step: 3101,loss: 0.158 accuracy: 0.938\n",
      "2017-08-13 22:49:01,755 - INFO - step: 3102,loss: 0.087 accuracy: 0.953\n",
      "2017-08-13 22:49:01,818 - INFO - step: 3103,loss: 0.105 accuracy: 0.969\n",
      "2017-08-13 22:49:01,881 - INFO - step: 3104,loss: 0.045 accuracy: 1.000\n",
      "2017-08-13 22:49:01,944 - INFO - step: 3105,loss: 0.094 accuracy: 0.953\n",
      "2017-08-13 22:49:02,007 - INFO - step: 3106,loss: 0.067 accuracy: 0.969\n",
      "2017-08-13 22:49:02,071 - INFO - step: 3107,loss: 0.176 accuracy: 0.984\n",
      "2017-08-13 22:49:02,133 - INFO - step: 3108,loss: 0.092 accuracy: 0.984\n",
      "2017-08-13 22:49:02,194 - INFO - step: 3109,loss: 0.085 accuracy: 0.953\n",
      "2017-08-13 22:49:02,257 - INFO - step: 3110,loss: 0.041 accuracy: 1.000\n",
      "2017-08-13 22:49:02,319 - INFO - step: 3111,loss: 0.097 accuracy: 0.969\n",
      "2017-08-13 22:49:02,382 - INFO - step: 3112,loss: 0.025 accuracy: 1.000\n",
      "2017-08-13 22:49:02,449 - INFO - step: 3113,loss: 0.182 accuracy: 0.938\n",
      "2017-08-13 22:49:02,513 - INFO - step: 3114,loss: 0.103 accuracy: 0.969\n",
      "2017-08-13 22:49:02,576 - INFO - step: 3115,loss: 0.111 accuracy: 0.969\n",
      "2017-08-13 22:49:02,638 - INFO - step: 3116,loss: 0.080 accuracy: 0.969\n",
      "2017-08-13 22:49:02,700 - INFO - step: 3117,loss: 0.090 accuracy: 0.969\n",
      "2017-08-13 22:49:02,764 - INFO - step: 3118,loss: 0.098 accuracy: 0.969\n",
      "2017-08-13 22:49:02,828 - INFO - step: 3119,loss: 0.039 accuracy: 1.000\n",
      "2017-08-13 22:49:02,893 - INFO - step: 3120,loss: 0.089 accuracy: 0.938\n",
      "2017-08-13 22:49:02,956 - INFO - step: 3121,loss: 0.099 accuracy: 0.969\n",
      "2017-08-13 22:49:03,020 - INFO - step: 3122,loss: 0.142 accuracy: 0.953\n",
      "2017-08-13 22:49:03,084 - INFO - step: 3123,loss: 0.201 accuracy: 0.891\n",
      "2017-08-13 22:49:03,145 - INFO - step: 3124,loss: 0.086 accuracy: 0.953\n",
      "2017-08-13 22:49:03,209 - INFO - step: 3125,loss: 0.035 accuracy: 1.000\n",
      "2017-08-13 22:49:03,272 - INFO - step: 3126,loss: 0.071 accuracy: 0.969\n",
      "2017-08-13 22:49:03,335 - INFO - step: 3127,loss: 0.103 accuracy: 0.969\n",
      "2017-08-13 22:49:03,398 - INFO - step: 3128,loss: 0.118 accuracy: 0.969\n",
      "2017-08-13 22:49:03,461 - INFO - step: 3129,loss: 0.171 accuracy: 0.953\n",
      "2017-08-13 22:49:03,525 - INFO - step: 3130,loss: 0.056 accuracy: 0.984\n",
      "2017-08-13 22:49:03,589 - INFO - step: 3131,loss: 0.068 accuracy: 0.969\n",
      "2017-08-13 22:49:03,613 - INFO - step: 3132,loss: 0.002 accuracy: 1.000\n",
      "2017-08-13 22:49:03,615 - INFO - \n",
      "Evaluation:\n",
      "2017-08-13 22:49:03,693 - INFO - step: 3132,loss: 0.256 accuracy: 0.908\n",
      "2017-08-13 22:49:03,695 - INFO - \train_epoch:\n",
      "2017-08-13 22:49:03,697 - INFO - loss_total: 0.093 accuracy_total: 0.967\n",
      "2017-08-13 22:49:03,769 - INFO - step: 3133,loss: 0.045 accuracy: 0.984\n",
      "2017-08-13 22:49:03,831 - INFO - step: 3134,loss: 0.054 accuracy: 0.984\n",
      "2017-08-13 22:49:03,895 - INFO - step: 3135,loss: 0.087 accuracy: 0.938\n",
      "2017-08-13 22:49:03,957 - INFO - step: 3136,loss: 0.088 accuracy: 0.953\n",
      "2017-08-13 22:49:04,021 - INFO - step: 3137,loss: 0.040 accuracy: 0.984\n",
      "2017-08-13 22:49:04,085 - INFO - step: 3138,loss: 0.075 accuracy: 0.953\n",
      "2017-08-13 22:49:04,147 - INFO - step: 3139,loss: 0.069 accuracy: 0.969\n",
      "2017-08-13 22:49:04,210 - INFO - step: 3140,loss: 0.122 accuracy: 0.953\n",
      "2017-08-13 22:49:04,272 - INFO - step: 3141,loss: 0.107 accuracy: 0.953\n",
      "2017-08-13 22:49:04,335 - INFO - step: 3142,loss: 0.072 accuracy: 0.969\n",
      "2017-08-13 22:49:04,398 - INFO - step: 3143,loss: 0.070 accuracy: 0.984\n",
      "2017-08-13 22:49:04,461 - INFO - step: 3144,loss: 0.051 accuracy: 0.984\n",
      "2017-08-13 22:49:04,523 - INFO - step: 3145,loss: 0.083 accuracy: 0.969\n",
      "2017-08-13 22:49:04,590 - INFO - step: 3146,loss: 0.063 accuracy: 0.969\n",
      "2017-08-13 22:49:04,653 - INFO - step: 3147,loss: 0.075 accuracy: 0.969\n",
      "2017-08-13 22:49:04,717 - INFO - step: 3148,loss: 0.141 accuracy: 0.969\n",
      "2017-08-13 22:49:04,781 - INFO - step: 3149,loss: 0.086 accuracy: 0.984\n",
      "2017-08-13 22:49:04,845 - INFO - step: 3150,loss: 0.031 accuracy: 1.000\n",
      "2017-08-13 22:49:04,908 - INFO - step: 3151,loss: 0.047 accuracy: 0.984\n",
      "2017-08-13 22:49:04,972 - INFO - step: 3152,loss: 0.156 accuracy: 0.938\n",
      "2017-08-13 22:49:05,034 - INFO - step: 3153,loss: 0.105 accuracy: 0.938\n",
      "2017-08-13 22:49:05,099 - INFO - step: 3154,loss: 0.076 accuracy: 0.953\n",
      "2017-08-13 22:49:05,161 - INFO - step: 3155,loss: 0.120 accuracy: 0.938\n",
      "2017-08-13 22:49:05,230 - INFO - step: 3156,loss: 0.071 accuracy: 0.984\n",
      "2017-08-13 22:49:05,294 - INFO - step: 3157,loss: 0.054 accuracy: 0.984\n",
      "2017-08-13 22:49:05,358 - INFO - step: 3158,loss: 0.052 accuracy: 0.984\n",
      "2017-08-13 22:49:05,421 - INFO - step: 3159,loss: 0.083 accuracy: 0.953\n",
      "2017-08-13 22:49:05,486 - INFO - step: 3160,loss: 0.082 accuracy: 0.953\n",
      "2017-08-13 22:49:05,548 - INFO - step: 3161,loss: 0.038 accuracy: 0.984\n",
      "2017-08-13 22:49:05,612 - INFO - step: 3162,loss: 0.060 accuracy: 0.984\n",
      "2017-08-13 22:49:05,674 - INFO - step: 3163,loss: 0.074 accuracy: 0.969\n",
      "2017-08-13 22:49:05,738 - INFO - step: 3164,loss: 0.049 accuracy: 0.984\n",
      "2017-08-13 22:49:05,801 - INFO - step: 3165,loss: 0.125 accuracy: 0.984\n",
      "2017-08-13 22:49:05,865 - INFO - step: 3166,loss: 0.071 accuracy: 0.953\n",
      "2017-08-13 22:49:05,927 - INFO - step: 3167,loss: 0.012 accuracy: 1.000\n",
      "2017-08-13 22:49:05,949 - INFO - step: 3168,loss: 0.332 accuracy: 0.857\n",
      "2017-08-13 22:49:05,952 - INFO - \train_epoch:\n",
      "2017-08-13 22:49:05,954 - INFO - loss_total: 0.082 accuracy_total: 0.967\n",
      "2017-08-13 22:49:06,033 - INFO - step: 3169,loss: 0.036 accuracy: 1.000\n",
      "2017-08-13 22:49:06,097 - INFO - step: 3170,loss: 0.019 accuracy: 1.000\n",
      "2017-08-13 22:49:06,161 - INFO - step: 3171,loss: 0.138 accuracy: 0.938\n",
      "2017-08-13 22:49:06,227 - INFO - step: 3172,loss: 0.074 accuracy: 0.969\n",
      "2017-08-13 22:49:06,290 - INFO - step: 3173,loss: 0.092 accuracy: 0.953\n",
      "2017-08-13 22:49:06,360 - INFO - step: 3174,loss: 0.080 accuracy: 0.953\n",
      "2017-08-13 22:49:06,423 - INFO - step: 3175,loss: 0.059 accuracy: 0.984\n",
      "2017-08-13 22:49:06,487 - INFO - step: 3176,loss: 0.066 accuracy: 0.984\n",
      "2017-08-13 22:49:06,548 - INFO - step: 3177,loss: 0.128 accuracy: 0.922\n",
      "2017-08-13 22:49:06,611 - INFO - step: 3178,loss: 0.068 accuracy: 0.984\n",
      "2017-08-13 22:49:06,674 - INFO - step: 3179,loss: 0.087 accuracy: 0.953\n",
      "2017-08-13 22:49:06,738 - INFO - step: 3180,loss: 0.037 accuracy: 1.000\n",
      "2017-08-13 22:49:06,802 - INFO - step: 3181,loss: 0.081 accuracy: 0.969\n",
      "2017-08-13 22:49:06,864 - INFO - step: 3182,loss: 0.131 accuracy: 0.953\n",
      "2017-08-13 22:49:06,927 - INFO - step: 3183,loss: 0.095 accuracy: 0.969\n",
      "2017-08-13 22:49:06,990 - INFO - step: 3184,loss: 0.114 accuracy: 0.953\n",
      "2017-08-13 22:49:07,054 - INFO - step: 3185,loss: 0.050 accuracy: 0.984\n",
      "2017-08-13 22:49:07,119 - INFO - step: 3186,loss: 0.121 accuracy: 0.953\n",
      "2017-08-13 22:49:07,182 - INFO - step: 3187,loss: 0.072 accuracy: 0.953\n",
      "2017-08-13 22:49:07,245 - INFO - step: 3188,loss: 0.033 accuracy: 1.000\n",
      "2017-08-13 22:49:07,309 - INFO - step: 3189,loss: 0.127 accuracy: 0.906\n",
      "2017-08-13 22:49:07,371 - INFO - step: 3190,loss: 0.074 accuracy: 0.984\n",
      "2017-08-13 22:49:07,438 - INFO - step: 3191,loss: 0.046 accuracy: 0.984\n",
      "2017-08-13 22:49:07,503 - INFO - step: 3192,loss: 0.062 accuracy: 0.984\n",
      "2017-08-13 22:49:07,584 - INFO - step: 3193,loss: 0.053 accuracy: 0.969\n",
      "2017-08-13 22:49:07,646 - INFO - step: 3194,loss: 0.116 accuracy: 0.969\n",
      "2017-08-13 22:49:07,710 - INFO - step: 3195,loss: 0.094 accuracy: 0.953\n",
      "2017-08-13 22:49:07,774 - INFO - step: 3196,loss: 0.182 accuracy: 0.922\n",
      "2017-08-13 22:49:07,837 - INFO - step: 3197,loss: 0.093 accuracy: 0.953\n",
      "2017-08-13 22:49:07,899 - INFO - step: 3198,loss: 0.107 accuracy: 0.969\n",
      "2017-08-13 22:49:07,963 - INFO - step: 3199,loss: 0.060 accuracy: 0.969\n",
      "2017-08-13 22:49:08,027 - INFO - step: 3200,loss: 0.046 accuracy: 0.969\n",
      "2017-08-13 22:49:08,091 - INFO - step: 3201,loss: 0.048 accuracy: 1.000\n",
      "2017-08-13 22:49:08,156 - INFO - step: 3202,loss: 0.037 accuracy: 0.984\n",
      "2017-08-13 22:49:08,218 - INFO - step: 3203,loss: 0.105 accuracy: 0.953\n",
      "2017-08-13 22:49:08,241 - INFO - step: 3204,loss: 0.006 accuracy: 1.000\n",
      "2017-08-13 22:49:08,244 - INFO - \n",
      "Evaluation:\n",
      "2017-08-13 22:49:08,385 - INFO - step: 3204,loss: 0.260 accuracy: 0.908\n",
      "2017-08-13 22:49:08,387 - INFO - \train_epoch:\n",
      "2017-08-13 22:49:08,388 - INFO - loss_total: 0.079 accuracy_total: 0.968\n",
      "2017-08-13 22:49:08,473 - INFO - step: 3205,loss: 0.034 accuracy: 1.000\n",
      "2017-08-13 22:49:08,535 - INFO - step: 3206,loss: 0.044 accuracy: 0.984\n",
      "2017-08-13 22:49:08,598 - INFO - step: 3207,loss: 0.069 accuracy: 0.984\n",
      "2017-08-13 22:49:08,662 - INFO - step: 3208,loss: 0.021 accuracy: 1.000\n",
      "2017-08-13 22:49:08,725 - INFO - step: 3209,loss: 0.089 accuracy: 0.984\n",
      "2017-08-13 22:49:08,789 - INFO - step: 3210,loss: 0.079 accuracy: 0.969\n",
      "2017-08-13 22:49:08,853 - INFO - step: 3211,loss: 0.160 accuracy: 0.938\n",
      "2017-08-13 22:49:08,915 - INFO - step: 3212,loss: 0.220 accuracy: 0.953\n",
      "2017-08-13 22:49:08,978 - INFO - step: 3213,loss: 0.052 accuracy: 0.984\n",
      "2017-08-13 22:49:09,041 - INFO - step: 3214,loss: 0.066 accuracy: 0.969\n",
      "2017-08-13 22:49:09,104 - INFO - step: 3215,loss: 0.151 accuracy: 0.906\n",
      "2017-08-13 22:49:09,173 - INFO - step: 3216,loss: 0.045 accuracy: 0.984\n",
      "2017-08-13 22:49:09,234 - INFO - step: 3217,loss: 0.070 accuracy: 0.969\n",
      "2017-08-13 22:49:09,299 - INFO - step: 3218,loss: 0.036 accuracy: 1.000\n",
      "2017-08-13 22:49:09,360 - INFO - step: 3219,loss: 0.163 accuracy: 0.953\n",
      "2017-08-13 22:49:09,423 - INFO - step: 3220,loss: 0.046 accuracy: 0.984\n",
      "2017-08-13 22:49:09,486 - INFO - step: 3221,loss: 0.117 accuracy: 0.922\n",
      "2017-08-13 22:49:09,547 - INFO - step: 3222,loss: 0.078 accuracy: 0.969\n",
      "2017-08-13 22:49:09,609 - INFO - step: 3223,loss: 0.060 accuracy: 0.969\n",
      "2017-08-13 22:49:09,672 - INFO - step: 3224,loss: 0.051 accuracy: 0.984\n",
      "2017-08-13 22:49:09,733 - INFO - step: 3225,loss: 0.093 accuracy: 0.953\n",
      "2017-08-13 22:49:09,797 - INFO - step: 3226,loss: 0.070 accuracy: 0.969\n",
      "2017-08-13 22:49:09,859 - INFO - step: 3227,loss: 0.056 accuracy: 0.984\n",
      "2017-08-13 22:49:09,921 - INFO - step: 3228,loss: 0.144 accuracy: 0.938\n",
      "2017-08-13 22:49:09,983 - INFO - step: 3229,loss: 0.060 accuracy: 0.984\n",
      "2017-08-13 22:49:10,045 - INFO - step: 3230,loss: 0.060 accuracy: 0.969\n",
      "2017-08-13 22:49:10,107 - INFO - step: 3231,loss: 0.053 accuracy: 0.969\n",
      "2017-08-13 22:49:10,169 - INFO - step: 3232,loss: 0.056 accuracy: 0.984\n",
      "2017-08-13 22:49:10,231 - INFO - step: 3233,loss: 0.187 accuracy: 0.969\n",
      "2017-08-13 22:49:10,295 - INFO - step: 3234,loss: 0.057 accuracy: 0.984\n",
      "2017-08-13 22:49:10,357 - INFO - step: 3235,loss: 0.103 accuracy: 0.953\n",
      "2017-08-13 22:49:10,420 - INFO - step: 3236,loss: 0.034 accuracy: 1.000\n",
      "2017-08-13 22:49:10,485 - INFO - step: 3237,loss: 0.089 accuracy: 0.953\n",
      "2017-08-13 22:49:10,549 - INFO - step: 3238,loss: 0.050 accuracy: 1.000\n",
      "2017-08-13 22:49:10,614 - INFO - step: 3239,loss: 0.018 accuracy: 1.000\n",
      "2017-08-13 22:49:10,636 - INFO - step: 3240,loss: 0.052 accuracy: 1.000\n",
      "2017-08-13 22:49:10,638 - INFO - \train_epoch:\n",
      "2017-08-13 22:49:10,640 - INFO - loss_total: 0.079 accuracy_total: 0.973\n",
      "2017-08-13 22:49:10,726 - INFO - step: 3241,loss: 0.132 accuracy: 0.953\n",
      "2017-08-13 22:49:10,790 - INFO - step: 3242,loss: 0.050 accuracy: 0.984\n",
      "2017-08-13 22:49:10,853 - INFO - step: 3243,loss: 0.113 accuracy: 0.938\n",
      "2017-08-13 22:49:10,916 - INFO - step: 3244,loss: 0.057 accuracy: 0.969\n",
      "2017-08-13 22:49:10,979 - INFO - step: 3245,loss: 0.037 accuracy: 0.984\n",
      "2017-08-13 22:49:11,043 - INFO - step: 3246,loss: 0.021 accuracy: 1.000\n",
      "2017-08-13 22:49:11,105 - INFO - step: 3247,loss: 0.068 accuracy: 0.969\n",
      "2017-08-13 22:49:11,168 - INFO - step: 3248,loss: 0.202 accuracy: 0.938\n",
      "2017-08-13 22:49:11,228 - INFO - step: 3249,loss: 0.106 accuracy: 0.953\n",
      "2017-08-13 22:49:11,289 - INFO - step: 3250,loss: 0.036 accuracy: 1.000\n",
      "2017-08-13 22:49:11,351 - INFO - step: 3251,loss: 0.030 accuracy: 0.984\n",
      "2017-08-13 22:49:11,414 - INFO - step: 3252,loss: 0.076 accuracy: 0.969\n",
      "2017-08-13 22:49:11,477 - INFO - step: 3253,loss: 0.111 accuracy: 0.969\n",
      "2017-08-13 22:49:11,541 - INFO - step: 3254,loss: 0.062 accuracy: 0.984\n",
      "2017-08-13 22:49:11,603 - INFO - step: 3255,loss: 0.049 accuracy: 0.969\n",
      "2017-08-13 22:49:11,669 - INFO - step: 3256,loss: 0.105 accuracy: 0.938\n",
      "2017-08-13 22:49:11,734 - INFO - step: 3257,loss: 0.064 accuracy: 0.984\n",
      "2017-08-13 22:49:11,796 - INFO - step: 3258,loss: 0.112 accuracy: 0.938\n",
      "2017-08-13 22:49:11,858 - INFO - step: 3259,loss: 0.040 accuracy: 1.000\n",
      "2017-08-13 22:49:11,922 - INFO - step: 3260,loss: 0.162 accuracy: 0.906\n",
      "2017-08-13 22:49:11,984 - INFO - step: 3261,loss: 0.046 accuracy: 0.984\n",
      "2017-08-13 22:49:12,046 - INFO - step: 3262,loss: 0.029 accuracy: 0.984\n",
      "2017-08-13 22:49:12,110 - INFO - step: 3263,loss: 0.036 accuracy: 0.984\n",
      "2017-08-13 22:49:12,174 - INFO - step: 3264,loss: 0.087 accuracy: 0.984\n",
      "2017-08-13 22:49:12,240 - INFO - step: 3265,loss: 0.172 accuracy: 0.938\n",
      "2017-08-13 22:49:12,303 - INFO - step: 3266,loss: 0.146 accuracy: 0.922\n",
      "2017-08-13 22:49:12,366 - INFO - step: 3267,loss: 0.019 accuracy: 1.000\n",
      "2017-08-13 22:49:12,430 - INFO - step: 3268,loss: 0.104 accuracy: 0.953\n",
      "2017-08-13 22:49:12,493 - INFO - step: 3269,loss: 0.039 accuracy: 0.984\n",
      "2017-08-13 22:49:12,557 - INFO - step: 3270,loss: 0.056 accuracy: 0.969\n",
      "2017-08-13 22:49:12,620 - INFO - step: 3271,loss: 0.124 accuracy: 0.938\n",
      "2017-08-13 22:49:12,685 - INFO - step: 3272,loss: 0.078 accuracy: 0.969\n",
      "2017-08-13 22:49:12,749 - INFO - step: 3273,loss: 0.186 accuracy: 0.953\n",
      "2017-08-13 22:49:12,812 - INFO - step: 3274,loss: 0.063 accuracy: 1.000\n",
      "2017-08-13 22:49:12,874 - INFO - step: 3275,loss: 0.086 accuracy: 0.984\n",
      "2017-08-13 22:49:12,896 - INFO - step: 3276,loss: 0.004 accuracy: 1.000\n",
      "2017-08-13 22:49:12,899 - INFO - \n",
      "Evaluation:\n",
      "2017-08-13 22:49:13,045 - INFO - step: 3276,loss: 0.266 accuracy: 0.912\n",
      "2017-08-13 22:49:13,047 - INFO - \train_epoch:\n",
      "2017-08-13 22:49:13,048 - INFO - loss_total: 0.081 accuracy_total: 0.969\n",
      "2017-08-13 22:49:13,131 - INFO - step: 3277,loss: 0.080 accuracy: 0.969\n",
      "2017-08-13 22:49:13,193 - INFO - step: 3278,loss: 0.108 accuracy: 0.953\n",
      "2017-08-13 22:49:13,256 - INFO - step: 3279,loss: 0.024 accuracy: 1.000\n",
      "2017-08-13 22:49:13,322 - INFO - step: 3280,loss: 0.027 accuracy: 1.000\n",
      "2017-08-13 22:49:13,385 - INFO - step: 3281,loss: 0.051 accuracy: 1.000\n",
      "2017-08-13 22:49:13,449 - INFO - step: 3282,loss: 0.033 accuracy: 1.000\n",
      "2017-08-13 22:49:13,513 - INFO - step: 3283,loss: 0.179 accuracy: 0.938\n",
      "2017-08-13 22:49:13,576 - INFO - step: 3284,loss: 0.018 accuracy: 1.000\n",
      "2017-08-13 22:49:13,640 - INFO - step: 3285,loss: 0.150 accuracy: 0.953\n",
      "2017-08-13 22:49:13,703 - INFO - step: 3286,loss: 0.114 accuracy: 0.969\n",
      "2017-08-13 22:49:13,766 - INFO - step: 3287,loss: 0.067 accuracy: 0.953\n",
      "2017-08-13 22:49:13,830 - INFO - step: 3288,loss: 0.068 accuracy: 0.984\n",
      "2017-08-13 22:49:13,893 - INFO - step: 3289,loss: 0.040 accuracy: 0.984\n",
      "2017-08-13 22:49:13,957 - INFO - step: 3290,loss: 0.050 accuracy: 0.969\n",
      "2017-08-13 22:49:14,020 - INFO - step: 3291,loss: 0.062 accuracy: 0.969\n",
      "2017-08-13 22:49:14,084 - INFO - step: 3292,loss: 0.058 accuracy: 0.984\n",
      "2017-08-13 22:49:14,148 - INFO - step: 3293,loss: 0.073 accuracy: 0.953\n",
      "2017-08-13 22:49:14,212 - INFO - step: 3294,loss: 0.055 accuracy: 0.984\n",
      "2017-08-13 22:49:14,275 - INFO - step: 3295,loss: 0.059 accuracy: 0.984\n",
      "2017-08-13 22:49:14,338 - INFO - step: 3296,loss: 0.138 accuracy: 0.938\n",
      "2017-08-13 22:49:14,401 - INFO - step: 3297,loss: 0.083 accuracy: 0.969\n",
      "2017-08-13 22:49:14,462 - INFO - step: 3298,loss: 0.166 accuracy: 0.953\n",
      "2017-08-13 22:49:14,525 - INFO - step: 3299,loss: 0.060 accuracy: 0.984\n",
      "2017-08-13 22:49:14,587 - INFO - step: 3300,loss: 0.034 accuracy: 0.984\n",
      "2017-08-13 22:49:14,649 - INFO - step: 3301,loss: 0.100 accuracy: 0.969\n",
      "2017-08-13 22:49:14,713 - INFO - step: 3302,loss: 0.056 accuracy: 0.984\n",
      "2017-08-13 22:49:14,776 - INFO - step: 3303,loss: 0.085 accuracy: 0.953\n",
      "2017-08-13 22:49:14,840 - INFO - step: 3304,loss: 0.112 accuracy: 0.969\n",
      "2017-08-13 22:49:14,905 - INFO - step: 3305,loss: 0.043 accuracy: 0.984\n",
      "2017-08-13 22:49:14,969 - INFO - step: 3306,loss: 0.045 accuracy: 0.984\n",
      "2017-08-13 22:49:15,031 - INFO - step: 3307,loss: 0.082 accuracy: 0.953\n",
      "2017-08-13 22:49:15,093 - INFO - step: 3308,loss: 0.064 accuracy: 0.984\n",
      "2017-08-13 22:49:15,155 - INFO - step: 3309,loss: 0.111 accuracy: 0.953\n",
      "2017-08-13 22:49:15,219 - INFO - step: 3310,loss: 0.084 accuracy: 0.969\n",
      "2017-08-13 22:49:15,283 - INFO - step: 3311,loss: 0.108 accuracy: 0.938\n",
      "2017-08-13 22:49:15,308 - INFO - step: 3312,loss: 0.116 accuracy: 1.000\n",
      "2017-08-13 22:49:15,311 - INFO - \train_epoch:\n",
      "2017-08-13 22:49:15,313 - INFO - loss_total: 0.078 accuracy_total: 0.973\n",
      "2017-08-13 22:49:15,385 - INFO - step: 3313,loss: 0.063 accuracy: 0.969\n",
      "2017-08-13 22:49:15,448 - INFO - step: 3314,loss: 0.054 accuracy: 0.984\n",
      "2017-08-13 22:49:15,509 - INFO - step: 3315,loss: 0.145 accuracy: 0.984\n",
      "2017-08-13 22:49:15,572 - INFO - step: 3316,loss: 0.102 accuracy: 0.953\n",
      "2017-08-13 22:49:15,636 - INFO - step: 3317,loss: 0.044 accuracy: 0.969\n",
      "2017-08-13 22:49:15,700 - INFO - step: 3318,loss: 0.100 accuracy: 0.938\n",
      "2017-08-13 22:49:15,763 - INFO - step: 3319,loss: 0.085 accuracy: 0.953\n",
      "2017-08-13 22:49:15,825 - INFO - step: 3320,loss: 0.080 accuracy: 0.969\n",
      "2017-08-13 22:49:15,886 - INFO - step: 3321,loss: 0.077 accuracy: 0.969\n",
      "2017-08-13 22:49:15,948 - INFO - step: 3322,loss: 0.063 accuracy: 0.984\n",
      "2017-08-13 22:49:16,011 - INFO - step: 3323,loss: 0.124 accuracy: 0.953\n",
      "2017-08-13 22:49:16,076 - INFO - step: 3324,loss: 0.059 accuracy: 0.984\n",
      "2017-08-13 22:49:16,139 - INFO - step: 3325,loss: 0.059 accuracy: 0.953\n",
      "2017-08-13 22:49:16,203 - INFO - step: 3326,loss: 0.065 accuracy: 0.969\n",
      "2017-08-13 22:49:16,264 - INFO - step: 3327,loss: 0.061 accuracy: 1.000\n",
      "2017-08-13 22:49:16,328 - INFO - step: 3328,loss: 0.040 accuracy: 1.000\n",
      "2017-08-13 22:49:16,392 - INFO - step: 3329,loss: 0.119 accuracy: 0.953\n",
      "2017-08-13 22:49:16,459 - INFO - step: 3330,loss: 0.061 accuracy: 0.984\n",
      "2017-08-13 22:49:16,522 - INFO - step: 3331,loss: 0.041 accuracy: 0.984\n",
      "2017-08-13 22:49:16,584 - INFO - step: 3332,loss: 0.018 accuracy: 1.000\n",
      "2017-08-13 22:49:16,648 - INFO - step: 3333,loss: 0.031 accuracy: 1.000\n",
      "2017-08-13 22:49:16,712 - INFO - step: 3334,loss: 0.042 accuracy: 1.000\n",
      "2017-08-13 22:49:16,775 - INFO - step: 3335,loss: 0.035 accuracy: 1.000\n",
      "2017-08-13 22:49:16,839 - INFO - step: 3336,loss: 0.060 accuracy: 0.984\n",
      "2017-08-13 22:49:16,901 - INFO - step: 3337,loss: 0.068 accuracy: 0.984\n",
      "2017-08-13 22:49:16,963 - INFO - step: 3338,loss: 0.074 accuracy: 0.953\n",
      "2017-08-13 22:49:17,026 - INFO - step: 3339,loss: 0.058 accuracy: 0.984\n",
      "2017-08-13 22:49:17,090 - INFO - step: 3340,loss: 0.031 accuracy: 1.000\n",
      "2017-08-13 22:49:17,150 - INFO - step: 3341,loss: 0.149 accuracy: 0.969\n",
      "2017-08-13 22:49:17,213 - INFO - step: 3342,loss: 0.028 accuracy: 1.000\n",
      "2017-08-13 22:49:17,276 - INFO - step: 3343,loss: 0.013 accuracy: 1.000\n",
      "2017-08-13 22:49:17,339 - INFO - step: 3344,loss: 0.045 accuracy: 1.000\n",
      "2017-08-13 22:49:17,402 - INFO - step: 3345,loss: 0.121 accuracy: 0.969\n",
      "2017-08-13 22:49:17,465 - INFO - step: 3346,loss: 0.088 accuracy: 0.953\n",
      "2017-08-13 22:49:17,529 - INFO - step: 3347,loss: 0.030 accuracy: 1.000\n",
      "2017-08-13 22:49:17,553 - INFO - step: 3348,loss: 0.033 accuracy: 1.000\n",
      "2017-08-13 22:49:17,555 - INFO - \n",
      "Evaluation:\n",
      "2017-08-13 22:49:17,697 - INFO - step: 3348,loss: 0.275 accuracy: 0.912\n",
      "2017-08-13 22:49:17,698 - INFO - \train_epoch:\n",
      "2017-08-13 22:49:17,700 - INFO - loss_total: 0.066 accuracy_total: 0.979\n",
      "2017-08-13 22:49:17,782 - INFO - step: 3349,loss: 0.031 accuracy: 0.984\n",
      "2017-08-13 22:49:17,846 - INFO - step: 3350,loss: 0.094 accuracy: 0.953\n",
      "2017-08-13 22:49:17,907 - INFO - step: 3351,loss: 0.021 accuracy: 0.984\n",
      "2017-08-13 22:49:17,972 - INFO - step: 3352,loss: 0.111 accuracy: 0.938\n",
      "2017-08-13 22:49:18,035 - INFO - step: 3353,loss: 0.052 accuracy: 0.984\n",
      "2017-08-13 22:49:18,099 - INFO - step: 3354,loss: 0.052 accuracy: 0.984\n",
      "2017-08-13 22:49:18,159 - INFO - step: 3355,loss: 0.015 accuracy: 1.000\n",
      "2017-08-13 22:49:18,220 - INFO - step: 3356,loss: 0.030 accuracy: 0.984\n",
      "2017-08-13 22:49:18,281 - INFO - step: 3357,loss: 0.131 accuracy: 0.938\n",
      "2017-08-13 22:49:18,345 - INFO - step: 3358,loss: 0.040 accuracy: 0.984\n",
      "2017-08-13 22:49:18,408 - INFO - step: 3359,loss: 0.132 accuracy: 0.953\n",
      "2017-08-13 22:49:18,471 - INFO - step: 3360,loss: 0.055 accuracy: 0.984\n",
      "2017-08-13 22:49:18,534 - INFO - step: 3361,loss: 0.046 accuracy: 1.000\n",
      "2017-08-13 22:49:18,596 - INFO - step: 3362,loss: 0.065 accuracy: 0.984\n",
      "2017-08-13 22:49:18,658 - INFO - step: 3363,loss: 0.077 accuracy: 0.969\n",
      "2017-08-13 22:49:18,722 - INFO - step: 3364,loss: 0.047 accuracy: 0.984\n",
      "2017-08-13 22:49:18,785 - INFO - step: 3365,loss: 0.072 accuracy: 0.969\n",
      "2017-08-13 22:49:18,848 - INFO - step: 3366,loss: 0.122 accuracy: 0.938\n",
      "2017-08-13 22:49:18,913 - INFO - step: 3367,loss: 0.073 accuracy: 0.984\n",
      "2017-08-13 22:49:18,976 - INFO - step: 3368,loss: 0.051 accuracy: 0.984\n",
      "2017-08-13 22:49:19,038 - INFO - step: 3369,loss: 0.063 accuracy: 0.969\n",
      "2017-08-13 22:49:19,100 - INFO - step: 3370,loss: 0.154 accuracy: 0.953\n",
      "2017-08-13 22:49:19,163 - INFO - step: 3371,loss: 0.157 accuracy: 0.922\n",
      "2017-08-13 22:49:19,225 - INFO - step: 3372,loss: 0.041 accuracy: 1.000\n",
      "2017-08-13 22:49:19,288 - INFO - step: 3373,loss: 0.137 accuracy: 0.953\n",
      "2017-08-13 22:49:19,352 - INFO - step: 3374,loss: 0.051 accuracy: 0.984\n",
      "2017-08-13 22:49:19,415 - INFO - step: 3375,loss: 0.067 accuracy: 0.984\n",
      "2017-08-13 22:49:19,479 - INFO - step: 3376,loss: 0.159 accuracy: 0.938\n",
      "2017-08-13 22:49:19,542 - INFO - step: 3377,loss: 0.084 accuracy: 0.953\n",
      "2017-08-13 22:49:19,603 - INFO - step: 3378,loss: 0.128 accuracy: 0.969\n",
      "2017-08-13 22:49:19,664 - INFO - step: 3379,loss: 0.016 accuracy: 1.000\n",
      "2017-08-13 22:49:19,726 - INFO - step: 3380,loss: 0.190 accuracy: 0.922\n",
      "2017-08-13 22:49:19,790 - INFO - step: 3381,loss: 0.116 accuracy: 0.938\n",
      "2017-08-13 22:49:19,853 - INFO - step: 3382,loss: 0.091 accuracy: 0.984\n",
      "2017-08-13 22:49:19,916 - INFO - step: 3383,loss: 0.158 accuracy: 0.953\n",
      "2017-08-13 22:49:19,938 - INFO - step: 3384,loss: 0.010 accuracy: 1.000\n",
      "2017-08-13 22:49:19,940 - INFO - \train_epoch:\n",
      "2017-08-13 22:49:19,942 - INFO - loss_total: 0.082 accuracy_total: 0.970\n",
      "2017-08-13 22:49:20,013 - INFO - step: 3385,loss: 0.061 accuracy: 0.969\n",
      "2017-08-13 22:49:20,076 - INFO - step: 3386,loss: 0.024 accuracy: 1.000\n",
      "2017-08-13 22:49:20,139 - INFO - step: 3387,loss: 0.142 accuracy: 0.953\n",
      "2017-08-13 22:49:20,201 - INFO - step: 3388,loss: 0.040 accuracy: 1.000\n",
      "2017-08-13 22:49:20,264 - INFO - step: 3389,loss: 0.034 accuracy: 0.984\n",
      "2017-08-13 22:49:20,326 - INFO - step: 3390,loss: 0.058 accuracy: 0.984\n",
      "2017-08-13 22:49:20,389 - INFO - step: 3391,loss: 0.046 accuracy: 0.984\n",
      "2017-08-13 22:49:20,454 - INFO - step: 3392,loss: 0.121 accuracy: 0.953\n",
      "2017-08-13 22:49:20,517 - INFO - step: 3393,loss: 0.105 accuracy: 0.922\n",
      "2017-08-13 22:49:20,579 - INFO - step: 3394,loss: 0.034 accuracy: 1.000\n",
      "2017-08-13 22:49:20,642 - INFO - step: 3395,loss: 0.147 accuracy: 0.969\n",
      "2017-08-13 22:49:20,707 - INFO - step: 3396,loss: 0.098 accuracy: 0.984\n",
      "2017-08-13 22:49:20,770 - INFO - step: 3397,loss: 0.105 accuracy: 0.969\n",
      "2017-08-13 22:49:20,835 - INFO - step: 3398,loss: 0.012 accuracy: 1.000\n",
      "2017-08-13 22:49:20,898 - INFO - step: 3399,loss: 0.009 accuracy: 1.000\n",
      "2017-08-13 22:49:20,962 - INFO - step: 3400,loss: 0.080 accuracy: 0.969\n",
      "2017-08-13 22:49:21,025 - INFO - step: 3401,loss: 0.105 accuracy: 0.953\n",
      "2017-08-13 22:49:21,089 - INFO - step: 3402,loss: 0.037 accuracy: 0.984\n",
      "2017-08-13 22:49:21,151 - INFO - step: 3403,loss: 0.058 accuracy: 0.969\n",
      "2017-08-13 22:49:21,213 - INFO - step: 3404,loss: 0.149 accuracy: 0.922\n",
      "2017-08-13 22:49:21,276 - INFO - step: 3405,loss: 0.066 accuracy: 0.984\n",
      "2017-08-13 22:49:21,340 - INFO - step: 3406,loss: 0.062 accuracy: 1.000\n",
      "2017-08-13 22:49:21,402 - INFO - step: 3407,loss: 0.083 accuracy: 0.938\n",
      "2017-08-13 22:49:21,464 - INFO - step: 3408,loss: 0.027 accuracy: 1.000\n",
      "2017-08-13 22:49:21,531 - INFO - step: 3409,loss: 0.161 accuracy: 0.969\n",
      "2017-08-13 22:49:21,595 - INFO - step: 3410,loss: 0.076 accuracy: 0.969\n",
      "2017-08-13 22:49:21,659 - INFO - step: 3411,loss: 0.075 accuracy: 0.953\n",
      "2017-08-13 22:49:21,721 - INFO - step: 3412,loss: 0.082 accuracy: 0.969\n",
      "2017-08-13 22:49:21,783 - INFO - step: 3413,loss: 0.105 accuracy: 0.969\n",
      "2017-08-13 22:49:21,848 - INFO - step: 3414,loss: 0.094 accuracy: 0.953\n",
      "2017-08-13 22:49:21,910 - INFO - step: 3415,loss: 0.113 accuracy: 0.969\n",
      "2017-08-13 22:49:21,974 - INFO - step: 3416,loss: 0.072 accuracy: 0.984\n",
      "2017-08-13 22:49:22,037 - INFO - step: 3417,loss: 0.057 accuracy: 0.969\n",
      "2017-08-13 22:49:22,100 - INFO - step: 3418,loss: 0.079 accuracy: 0.953\n",
      "2017-08-13 22:49:22,164 - INFO - step: 3419,loss: 0.139 accuracy: 0.953\n",
      "2017-08-13 22:49:22,187 - INFO - step: 3420,loss: 0.148 accuracy: 0.857\n",
      "2017-08-13 22:49:22,190 - INFO - \n",
      "Evaluation:\n",
      "2017-08-13 22:49:22,331 - INFO - step: 3420,loss: 0.278 accuracy: 0.904\n",
      "2017-08-13 22:49:22,332 - INFO - \train_epoch:\n",
      "2017-08-13 22:49:22,334 - INFO - loss_total: 0.081 accuracy_total: 0.968\n",
      "2017-08-13 22:49:22,404 - INFO - step: 3421,loss: 0.057 accuracy: 0.984\n",
      "2017-08-13 22:49:22,468 - INFO - step: 3422,loss: 0.039 accuracy: 1.000\n",
      "2017-08-13 22:49:22,529 - INFO - step: 3423,loss: 0.140 accuracy: 0.906\n",
      "2017-08-13 22:49:22,593 - INFO - step: 3424,loss: 0.079 accuracy: 0.969\n",
      "2017-08-13 22:49:22,655 - INFO - step: 3425,loss: 0.030 accuracy: 0.984\n",
      "2017-08-13 22:49:22,718 - INFO - step: 3426,loss: 0.088 accuracy: 0.969\n",
      "2017-08-13 22:49:22,781 - INFO - step: 3427,loss: 0.084 accuracy: 0.938\n",
      "2017-08-13 22:49:22,845 - INFO - step: 3428,loss: 0.070 accuracy: 0.984\n",
      "2017-08-13 22:49:22,908 - INFO - step: 3429,loss: 0.064 accuracy: 0.969\n",
      "2017-08-13 22:49:22,971 - INFO - step: 3430,loss: 0.097 accuracy: 0.984\n",
      "2017-08-13 22:49:23,038 - INFO - step: 3431,loss: 0.099 accuracy: 0.969\n",
      "2017-08-13 22:49:23,101 - INFO - step: 3432,loss: 0.083 accuracy: 0.938\n",
      "2017-08-13 22:49:23,164 - INFO - step: 3433,loss: 0.044 accuracy: 1.000\n",
      "2017-08-13 22:49:23,227 - INFO - step: 3434,loss: 0.074 accuracy: 0.969\n",
      "2017-08-13 22:49:23,292 - INFO - step: 3435,loss: 0.201 accuracy: 0.953\n",
      "2017-08-13 22:49:23,356 - INFO - step: 3436,loss: 0.054 accuracy: 0.984\n",
      "2017-08-13 22:49:23,420 - INFO - step: 3437,loss: 0.046 accuracy: 0.984\n",
      "2017-08-13 22:49:23,483 - INFO - step: 3438,loss: 0.108 accuracy: 0.969\n",
      "2017-08-13 22:49:23,545 - INFO - step: 3439,loss: 0.064 accuracy: 0.984\n",
      "2017-08-13 22:49:23,607 - INFO - step: 3440,loss: 0.038 accuracy: 0.984\n",
      "2017-08-13 22:49:23,670 - INFO - step: 3441,loss: 0.080 accuracy: 0.969\n",
      "2017-08-13 22:49:23,733 - INFO - step: 3442,loss: 0.100 accuracy: 0.969\n",
      "2017-08-13 22:49:23,796 - INFO - step: 3443,loss: 0.089 accuracy: 0.953\n",
      "2017-08-13 22:49:23,860 - INFO - step: 3444,loss: 0.104 accuracy: 0.969\n",
      "2017-08-13 22:49:23,923 - INFO - step: 3445,loss: 0.064 accuracy: 0.984\n",
      "2017-08-13 22:49:23,986 - INFO - step: 3446,loss: 0.028 accuracy: 1.000\n",
      "2017-08-13 22:49:24,049 - INFO - step: 3447,loss: 0.084 accuracy: 0.984\n",
      "2017-08-13 22:49:24,112 - INFO - step: 3448,loss: 0.152 accuracy: 0.922\n",
      "2017-08-13 22:49:24,174 - INFO - step: 3449,loss: 0.041 accuracy: 1.000\n",
      "2017-08-13 22:49:24,237 - INFO - step: 3450,loss: 0.067 accuracy: 0.969\n",
      "2017-08-13 22:49:24,301 - INFO - step: 3451,loss: 0.055 accuracy: 0.984\n",
      "2017-08-13 22:49:24,366 - INFO - step: 3452,loss: 0.106 accuracy: 0.969\n",
      "2017-08-13 22:49:24,431 - INFO - step: 3453,loss: 0.024 accuracy: 1.000\n",
      "2017-08-13 22:49:24,496 - INFO - step: 3454,loss: 0.066 accuracy: 0.984\n",
      "2017-08-13 22:49:24,559 - INFO - step: 3455,loss: 0.133 accuracy: 0.953\n",
      "2017-08-13 22:49:24,582 - INFO - step: 3456,loss: 0.110 accuracy: 0.857\n",
      "2017-08-13 22:49:24,585 - INFO - \train_epoch:\n",
      "2017-08-13 22:49:24,586 - INFO - loss_total: 0.080 accuracy_total: 0.969\n",
      "2017-08-13 22:49:24,669 - INFO - step: 3457,loss: 0.138 accuracy: 0.969\n",
      "2017-08-13 22:49:24,732 - INFO - step: 3458,loss: 0.051 accuracy: 0.984\n",
      "2017-08-13 22:49:24,794 - INFO - step: 3459,loss: 0.035 accuracy: 1.000\n",
      "2017-08-13 22:49:24,857 - INFO - step: 3460,loss: 0.084 accuracy: 0.969\n",
      "2017-08-13 22:49:24,919 - INFO - step: 3461,loss: 0.043 accuracy: 0.984\n",
      "2017-08-13 22:49:24,983 - INFO - step: 3462,loss: 0.124 accuracy: 0.953\n",
      "2017-08-13 22:49:25,046 - INFO - step: 3463,loss: 0.091 accuracy: 0.969\n",
      "2017-08-13 22:49:25,111 - INFO - step: 3464,loss: 0.058 accuracy: 0.984\n",
      "2017-08-13 22:49:25,178 - INFO - step: 3465,loss: 0.028 accuracy: 1.000\n",
      "2017-08-13 22:49:25,240 - INFO - step: 3466,loss: 0.054 accuracy: 0.953\n",
      "2017-08-13 22:49:25,304 - INFO - step: 3467,loss: 0.060 accuracy: 0.969\n",
      "2017-08-13 22:49:25,367 - INFO - step: 3468,loss: 0.058 accuracy: 0.984\n",
      "2017-08-13 22:49:25,430 - INFO - step: 3469,loss: 0.040 accuracy: 1.000\n",
      "2017-08-13 22:49:25,494 - INFO - step: 3470,loss: 0.041 accuracy: 0.984\n",
      "2017-08-13 22:49:25,558 - INFO - step: 3471,loss: 0.087 accuracy: 0.969\n",
      "2017-08-13 22:49:25,621 - INFO - step: 3472,loss: 0.085 accuracy: 0.953\n",
      "2017-08-13 22:49:25,685 - INFO - step: 3473,loss: 0.061 accuracy: 0.969\n",
      "2017-08-13 22:49:25,762 - INFO - step: 3474,loss: 0.177 accuracy: 0.953\n",
      "2017-08-13 22:49:25,827 - INFO - step: 3475,loss: 0.056 accuracy: 0.984\n",
      "2017-08-13 22:49:25,890 - INFO - step: 3476,loss: 0.171 accuracy: 0.953\n",
      "2017-08-13 22:49:25,954 - INFO - step: 3477,loss: 0.082 accuracy: 0.938\n",
      "2017-08-13 22:49:26,020 - INFO - step: 3478,loss: 0.027 accuracy: 0.984\n",
      "2017-08-13 22:49:26,086 - INFO - step: 3479,loss: 0.082 accuracy: 0.969\n",
      "2017-08-13 22:49:26,153 - INFO - step: 3480,loss: 0.087 accuracy: 0.953\n",
      "2017-08-13 22:49:26,218 - INFO - step: 3481,loss: 0.092 accuracy: 0.953\n",
      "2017-08-13 22:49:26,281 - INFO - step: 3482,loss: 0.196 accuracy: 0.938\n",
      "2017-08-13 22:49:26,343 - INFO - step: 3483,loss: 0.098 accuracy: 0.938\n",
      "2017-08-13 22:49:26,414 - INFO - step: 3484,loss: 0.112 accuracy: 0.938\n",
      "2017-08-13 22:49:26,478 - INFO - step: 3485,loss: 0.077 accuracy: 0.953\n",
      "2017-08-13 22:49:26,541 - INFO - step: 3486,loss: 0.203 accuracy: 0.953\n",
      "2017-08-13 22:49:26,603 - INFO - step: 3487,loss: 0.072 accuracy: 0.969\n",
      "2017-08-13 22:49:26,667 - INFO - step: 3488,loss: 0.084 accuracy: 0.984\n",
      "2017-08-13 22:49:26,731 - INFO - step: 3489,loss: 0.025 accuracy: 0.984\n",
      "2017-08-13 22:49:26,794 - INFO - step: 3490,loss: 0.056 accuracy: 0.969\n",
      "2017-08-13 22:49:26,858 - INFO - step: 3491,loss: 0.053 accuracy: 0.969\n",
      "2017-08-13 22:49:26,879 - INFO - step: 3492,loss: 0.051 accuracy: 1.000\n",
      "2017-08-13 22:49:26,883 - INFO - \n",
      "Evaluation:\n",
      "2017-08-13 22:49:27,030 - INFO - step: 3492,loss: 0.271 accuracy: 0.904\n",
      "2017-08-13 22:49:27,032 - INFO - \train_epoch:\n",
      "2017-08-13 22:49:27,034 - INFO - loss_total: 0.082 accuracy_total: 0.969\n",
      "2017-08-13 22:49:27,116 - INFO - step: 3493,loss: 0.066 accuracy: 0.953\n",
      "2017-08-13 22:49:27,179 - INFO - step: 3494,loss: 0.068 accuracy: 0.969\n",
      "2017-08-13 22:49:27,241 - INFO - step: 3495,loss: 0.158 accuracy: 0.969\n",
      "2017-08-13 22:49:27,305 - INFO - step: 3496,loss: 0.049 accuracy: 0.969\n",
      "2017-08-13 22:49:27,369 - INFO - step: 3497,loss: 0.032 accuracy: 1.000\n",
      "2017-08-13 22:49:27,430 - INFO - step: 3498,loss: 0.200 accuracy: 0.906\n",
      "2017-08-13 22:49:27,494 - INFO - step: 3499,loss: 0.034 accuracy: 1.000\n",
      "2017-08-13 22:49:27,558 - INFO - step: 3500,loss: 0.097 accuracy: 0.984\n",
      "2017-08-13 22:49:27,621 - INFO - step: 3501,loss: 0.101 accuracy: 0.938\n",
      "2017-08-13 22:49:27,685 - INFO - step: 3502,loss: 0.067 accuracy: 0.969\n",
      "2017-08-13 22:49:27,748 - INFO - step: 3503,loss: 0.116 accuracy: 0.953\n",
      "2017-08-13 22:49:27,816 - INFO - step: 3504,loss: 0.037 accuracy: 0.984\n",
      "2017-08-13 22:49:27,881 - INFO - step: 3505,loss: 0.076 accuracy: 0.969\n",
      "2017-08-13 22:49:27,943 - INFO - step: 3506,loss: 0.092 accuracy: 0.984\n",
      "2017-08-13 22:49:28,011 - INFO - step: 3507,loss: 0.098 accuracy: 0.953\n",
      "2017-08-13 22:49:28,074 - INFO - step: 3508,loss: 0.073 accuracy: 0.969\n",
      "2017-08-13 22:49:28,137 - INFO - step: 3509,loss: 0.158 accuracy: 0.938\n",
      "2017-08-13 22:49:28,200 - INFO - step: 3510,loss: 0.143 accuracy: 0.969\n",
      "2017-08-13 22:49:28,265 - INFO - step: 3511,loss: 0.046 accuracy: 0.984\n",
      "2017-08-13 22:49:28,328 - INFO - step: 3512,loss: 0.036 accuracy: 0.984\n",
      "2017-08-13 22:49:28,391 - INFO - step: 3513,loss: 0.041 accuracy: 0.984\n",
      "2017-08-13 22:49:28,454 - INFO - step: 3514,loss: 0.076 accuracy: 0.953\n",
      "2017-08-13 22:49:28,518 - INFO - step: 3515,loss: 0.060 accuracy: 0.969\n",
      "2017-08-13 22:49:28,581 - INFO - step: 3516,loss: 0.052 accuracy: 0.969\n",
      "2017-08-13 22:49:28,646 - INFO - step: 3517,loss: 0.066 accuracy: 0.984\n",
      "2017-08-13 22:49:28,709 - INFO - step: 3518,loss: 0.043 accuracy: 0.984\n",
      "2017-08-13 22:49:28,772 - INFO - step: 3519,loss: 0.110 accuracy: 0.953\n",
      "2017-08-13 22:49:28,837 - INFO - step: 3520,loss: 0.039 accuracy: 1.000\n",
      "2017-08-13 22:49:28,900 - INFO - step: 3521,loss: 0.040 accuracy: 0.984\n",
      "2017-08-13 22:49:28,965 - INFO - step: 3522,loss: 0.039 accuracy: 1.000\n",
      "2017-08-13 22:49:29,031 - INFO - step: 3523,loss: 0.019 accuracy: 1.000\n",
      "2017-08-13 22:49:29,094 - INFO - step: 3524,loss: 0.051 accuracy: 0.969\n",
      "2017-08-13 22:49:29,158 - INFO - step: 3525,loss: 0.052 accuracy: 0.984\n",
      "2017-08-13 22:49:29,223 - INFO - step: 3526,loss: 0.063 accuracy: 0.969\n",
      "2017-08-13 22:49:29,286 - INFO - step: 3527,loss: 0.078 accuracy: 0.953\n",
      "2017-08-13 22:49:29,309 - INFO - step: 3528,loss: 0.091 accuracy: 1.000\n",
      "2017-08-13 22:49:29,311 - INFO - \train_epoch:\n",
      "2017-08-13 22:49:29,313 - INFO - loss_total: 0.074 accuracy_total: 0.972\n",
      "2017-08-13 22:49:29,399 - INFO - step: 3529,loss: 0.050 accuracy: 0.984\n",
      "2017-08-13 22:49:29,463 - INFO - step: 3530,loss: 0.024 accuracy: 1.000\n",
      "2017-08-13 22:49:29,528 - INFO - step: 3531,loss: 0.054 accuracy: 0.984\n",
      "2017-08-13 22:49:29,593 - INFO - step: 3532,loss: 0.032 accuracy: 1.000\n",
      "2017-08-13 22:49:29,656 - INFO - step: 3533,loss: 0.028 accuracy: 1.000\n",
      "2017-08-13 22:49:29,719 - INFO - step: 3534,loss: 0.036 accuracy: 1.000\n",
      "2017-08-13 22:49:29,782 - INFO - step: 3535,loss: 0.076 accuracy: 0.938\n",
      "2017-08-13 22:49:29,843 - INFO - step: 3536,loss: 0.074 accuracy: 0.984\n",
      "2017-08-13 22:49:29,906 - INFO - step: 3537,loss: 0.098 accuracy: 0.953\n",
      "2017-08-13 22:49:29,966 - INFO - step: 3538,loss: 0.081 accuracy: 0.969\n",
      "2017-08-13 22:49:30,028 - INFO - step: 3539,loss: 0.071 accuracy: 0.969\n",
      "2017-08-13 22:49:30,090 - INFO - step: 3540,loss: 0.049 accuracy: 0.969\n",
      "2017-08-13 22:49:30,151 - INFO - step: 3541,loss: 0.109 accuracy: 0.969\n",
      "2017-08-13 22:49:30,216 - INFO - step: 3542,loss: 0.166 accuracy: 0.906\n",
      "2017-08-13 22:49:30,280 - INFO - step: 3543,loss: 0.035 accuracy: 1.000\n",
      "2017-08-13 22:49:30,341 - INFO - step: 3544,loss: 0.050 accuracy: 0.984\n",
      "2017-08-13 22:49:30,404 - INFO - step: 3545,loss: 0.200 accuracy: 0.938\n",
      "2017-08-13 22:49:30,468 - INFO - step: 3546,loss: 0.058 accuracy: 0.984\n",
      "2017-08-13 22:49:30,532 - INFO - step: 3547,loss: 0.056 accuracy: 0.969\n",
      "2017-08-13 22:49:30,595 - INFO - step: 3548,loss: 0.059 accuracy: 0.984\n",
      "2017-08-13 22:49:30,659 - INFO - step: 3549,loss: 0.064 accuracy: 0.969\n",
      "2017-08-13 22:49:30,722 - INFO - step: 3550,loss: 0.135 accuracy: 0.922\n",
      "2017-08-13 22:49:30,793 - INFO - step: 3551,loss: 0.047 accuracy: 0.953\n",
      "2017-08-13 22:49:30,858 - INFO - step: 3552,loss: 0.055 accuracy: 0.984\n",
      "2017-08-13 22:49:30,919 - INFO - step: 3553,loss: 0.079 accuracy: 0.969\n",
      "2017-08-13 22:49:30,982 - INFO - step: 3554,loss: 0.036 accuracy: 0.984\n",
      "2017-08-13 22:49:31,047 - INFO - step: 3555,loss: 0.010 accuracy: 1.000\n",
      "2017-08-13 22:49:31,110 - INFO - step: 3556,loss: 0.040 accuracy: 0.984\n",
      "2017-08-13 22:49:31,174 - INFO - step: 3557,loss: 0.094 accuracy: 0.969\n",
      "2017-08-13 22:49:31,238 - INFO - step: 3558,loss: 0.035 accuracy: 0.984\n",
      "2017-08-13 22:49:31,300 - INFO - step: 3559,loss: 0.111 accuracy: 0.938\n",
      "2017-08-13 22:49:31,363 - INFO - step: 3560,loss: 0.043 accuracy: 0.984\n",
      "2017-08-13 22:49:31,427 - INFO - step: 3561,loss: 0.048 accuracy: 0.984\n",
      "2017-08-13 22:49:31,490 - INFO - step: 3562,loss: 0.123 accuracy: 0.938\n",
      "2017-08-13 22:49:31,553 - INFO - step: 3563,loss: 0.110 accuracy: 0.938\n",
      "2017-08-13 22:49:31,575 - INFO - step: 3564,loss: 0.114 accuracy: 1.000\n",
      "2017-08-13 22:49:31,578 - INFO - \n",
      "Evaluation:\n",
      "2017-08-13 22:49:31,725 - INFO - step: 3564,loss: 0.268 accuracy: 0.912\n",
      "2017-08-13 22:49:31,726 - INFO - \train_epoch:\n",
      "2017-08-13 22:49:31,728 - INFO - loss_total: 0.071 accuracy_total: 0.972\n",
      "2017-08-13 22:49:31,811 - INFO - step: 3565,loss: 0.018 accuracy: 1.000\n",
      "2017-08-13 22:49:31,873 - INFO - step: 3566,loss: 0.043 accuracy: 1.000\n",
      "2017-08-13 22:49:31,936 - INFO - step: 3567,loss: 0.042 accuracy: 0.984\n",
      "2017-08-13 22:49:31,997 - INFO - step: 3568,loss: 0.037 accuracy: 0.984\n",
      "2017-08-13 22:49:32,060 - INFO - step: 3569,loss: 0.069 accuracy: 0.984\n",
      "2017-08-13 22:49:32,124 - INFO - step: 3570,loss: 0.062 accuracy: 0.969\n",
      "2017-08-13 22:49:32,187 - INFO - step: 3571,loss: 0.041 accuracy: 0.984\n",
      "2017-08-13 22:49:32,251 - INFO - step: 3572,loss: 0.161 accuracy: 0.922\n",
      "2017-08-13 22:49:32,314 - INFO - step: 3573,loss: 0.223 accuracy: 0.891\n",
      "2017-08-13 22:49:32,378 - INFO - step: 3574,loss: 0.061 accuracy: 0.984\n",
      "2017-08-13 22:49:32,444 - INFO - step: 3575,loss: 0.040 accuracy: 0.984\n",
      "2017-08-13 22:49:32,509 - INFO - step: 3576,loss: 0.138 accuracy: 0.906\n",
      "2017-08-13 22:49:32,574 - INFO - step: 3577,loss: 0.147 accuracy: 0.922\n",
      "2017-08-13 22:49:32,638 - INFO - step: 3578,loss: 0.093 accuracy: 0.938\n",
      "2017-08-13 22:49:32,701 - INFO - step: 3579,loss: 0.071 accuracy: 0.969\n",
      "2017-08-13 22:49:32,765 - INFO - step: 3580,loss: 0.064 accuracy: 0.969\n",
      "2017-08-13 22:49:32,826 - INFO - step: 3581,loss: 0.033 accuracy: 1.000\n",
      "2017-08-13 22:49:32,888 - INFO - step: 3582,loss: 0.097 accuracy: 0.984\n",
      "2017-08-13 22:49:32,952 - INFO - step: 3583,loss: 0.041 accuracy: 1.000\n",
      "2017-08-13 22:49:33,015 - INFO - step: 3584,loss: 0.047 accuracy: 0.969\n",
      "2017-08-13 22:49:33,080 - INFO - step: 3585,loss: 0.053 accuracy: 0.984\n",
      "2017-08-13 22:49:33,143 - INFO - step: 3586,loss: 0.228 accuracy: 0.953\n",
      "2017-08-13 22:49:33,206 - INFO - step: 3587,loss: 0.104 accuracy: 0.953\n",
      "2017-08-13 22:49:33,267 - INFO - step: 3588,loss: 0.073 accuracy: 0.969\n",
      "2017-08-13 22:49:33,331 - INFO - step: 3589,loss: 0.042 accuracy: 0.984\n",
      "2017-08-13 22:49:33,396 - INFO - step: 3590,loss: 0.089 accuracy: 0.953\n",
      "2017-08-13 22:49:33,461 - INFO - step: 3591,loss: 0.030 accuracy: 1.000\n",
      "2017-08-13 22:49:33,524 - INFO - step: 3592,loss: 0.071 accuracy: 0.969\n",
      "2017-08-13 22:49:33,585 - INFO - step: 3593,loss: 0.040 accuracy: 0.984\n",
      "2017-08-13 22:49:33,648 - INFO - step: 3594,loss: 0.111 accuracy: 0.953\n",
      "2017-08-13 22:49:33,710 - INFO - step: 3595,loss: 0.084 accuracy: 0.969\n",
      "2017-08-13 22:49:33,773 - INFO - step: 3596,loss: 0.041 accuracy: 0.984\n",
      "2017-08-13 22:49:33,836 - INFO - step: 3597,loss: 0.059 accuracy: 0.984\n",
      "2017-08-13 22:49:33,900 - INFO - step: 3598,loss: 0.030 accuracy: 1.000\n",
      "2017-08-13 22:49:33,963 - INFO - step: 3599,loss: 0.041 accuracy: 1.000\n",
      "2017-08-13 22:49:33,986 - INFO - step: 3600,loss: 0.004 accuracy: 1.000\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zx/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2889: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-5be245e2ed29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/zx/anaconda2/lib/python2.7/site-packages/tensorflow/python/platform/app.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m     46\u001b[0m   \u001b[0;31m# Call the main function, passing through any arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m   \u001b[0;31m# to the final program.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m   \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mflags_passthrough\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSystemExit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
