{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class malware_CNN(object):\n",
    "    def __init__(self,config):\n",
    "        self._config=config\n",
    "        self.input_x=tf.placeholder(tf.int32,[None,self._config.sequence_length],name=\"input_x\")    \n",
    "        self.input_y=tf.placeholder(tf.float32,[None,self._config.num_classes],name=\"input_y\")\n",
    "        self.dropout_keep_prob=tf.placeholder(tf.float32,name=\"dropout_keep_prob\")\n",
    "        l2_loss=tf.constant(0.0)\n",
    "        #embedding,可以尝试用one-hot\n",
    "        with tf.device(\"/cpu:0\"),tf.name_scope(\"embedding\"):\n",
    "            if not self._config.one_hot :\n",
    "                self.embedded=tf.get_variable(name=\"embedded\",shape=[self._config.vocab_size,self._config.embedding_dim],initializer=tf.random_uniform_initializer(-1.0,1.0))\n",
    "                #self.embedded=tf.Variable(tf.random_uniform([self._config.vocab_size,self._config.embedding_dim],-1.0,1.0),name=\"embedded\")\n",
    "                self.embedded_chars=tf.nn.embedding_lookup(self.embedded,self.input_x)\n",
    "                self.embedded_chars_expanded=tf.expand_dims(self.embedded_chars,-1)\n",
    "            else:\n",
    "                self.embedded_chars=tf.contrib.keras.backend.one_hot(self.input_x,vocab_size)\n",
    "                self.embedded_chars_expanded=tf.expand_dims(self.embedded_chars,-1)\n",
    "       # with  tf.variable_scope('conv1') as scope:\n",
    "        self.pooled_outputs = []\n",
    "        for i, filter_size in enumerate(self._config.filter_sizes):\n",
    "            with tf.variable_scope('conv-maxpool-%s' % filter_size):\n",
    "                #convolution layer1\n",
    "                filter_shape=[filter_size,self._config.embedding_dim,1,self._config.num_filter1]\n",
    "                W=tf.get_variable(name=\"W\", shape=filter_shape,initializer=tf.truncated_normal_initializer(stddev = 0.1))\n",
    "                #W=tf.Variable(tf.truncated_normal(filter_shape,stddev=0.1),name=\"W\")\n",
    "                b=tf.get_variable(name=\"b\",shape=[self._config.num_filter1],initializer=tf.constant_initializer(value=0.1, dtype=tf.float32))\n",
    "                conv=tf.nn.conv2d(self.embedded_chars_expanded,W,strides=[1,1,1,1],padding=\"VALID\",name=\"conv1\")\n",
    "                #apply nonlinearity\n",
    "                h=tf.nn.relu(tf.nn.bias_add(conv,b),name=\"relu\")\n",
    "                #Maxpooling\n",
    "                pooled=tf.nn.max_pool(h,ksize=[1,self._config.sequence_length-filter_size+1,1,1],strides=[1,1,1,1],padding=\"VALID\",name=\"pool\")\n",
    "                  # norm1\n",
    "                # norm1 = tf.nn.lrn(pooled, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm1')\n",
    "                self.pooled_outputs.append(pooled)\n",
    "                \n",
    "        # Combine all the pooled features，1，1，1，1 -》1，1，1，3\n",
    "        num_filters_total=self._config.num_filter1*len(self._config.filter_sizes)\n",
    "        self.h_pool = tf.concat(self.pooled_outputs, 3)\n",
    "        self.h_pool_flat=tf.reshape(self.h_pool,[-1,num_filters_total])\n",
    "        # Add dropout\n",
    "        with tf.variable_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat,self.dropout_keep_prob)\n",
    "        # Final (unnormalized) scores and predictions    \n",
    "        with tf.variable_scope(\"output\"):\n",
    "            W = tf.get_variable( \"W\",shape=[num_filters_total, self._config.num_classes],initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b=tf.get_variable(name=\"b\",shape=[self._config.num_classes],initializer=tf.constant_initializer(value=0.1, dtype=tf.float32))\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "        # CalculateMean cross-entropy loss\n",
    "        with tf.variable_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + self._config.l2_reg_lambda * l2_loss\n",
    "        # Accuracy\n",
    "        with tf.variable_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "        \n",
    "        \n",
    "    def batch_iter(x_data,y_data, batch_size, num_epochs, shuffle=True):\n",
    "        x_data=np.array(x_data)\n",
    "        y_data=np.array(y_data)\n",
    "        data_size =len(x_data)\n",
    "        num_batches_per_epoch = int(len(x_data)-1/batch_size) + 1\n",
    "        for epoch in range(num_epochs):\n",
    "            if shuffle:\n",
    "                shuffle_indices=np.random.permutation(data_size)\n",
    "                shuffled_data_x=x_data[shuffle_indices]\n",
    "                shuffled_data_y=y_data[shuffle_indices]\n",
    "                \n",
    "            else:\n",
    "                shuffled_data_x=x_data\n",
    "                shuffled_data_y=y_data\n",
    "            for batch_num in range(num_batches_per_epoch):\n",
    "                start_index=batch_num*batch_size\n",
    "                end_index=min((batch_num+1)*batch_size,data_size)\n",
    "                yield shuffled_data_x[start_index:end_index],shuffled_data_y[start_index:end_index]\n",
    "                \n",
    "        \n",
    "       \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "                \n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
